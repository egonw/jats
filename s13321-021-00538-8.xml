<?xml version="1.0"?>
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="research-article" xml:lang="en">
      <front>
        <journal-meta>
          <journal-id journal-id-type="publisher-id">13321</journal-id>
          <journal-title-group>
            <journal-title>Journal of Cheminformatics</journal-title>
            <abbrev-journal-title abbrev-type="publisher">J Cheminform</abbrev-journal-title>
          </journal-title-group>
          <issn pub-type="epub">1758-2946</issn>
          <publisher>
            <publisher-name>Springer International Publishing</publisher-name>
            <publisher-loc>Cham</publisher-loc>
          </publisher>
        </journal-meta>
        <article-meta>
          <article-id pub-id-type="publisher-id">s13321-021-00538-8</article-id>
          <article-id pub-id-type="manuscript">538</article-id>
          <article-id pub-id-type="doi">10.1186/s13321-021-00538-8</article-id>
          <article-categories>
            <subj-group subj-group-type="heading">
              <subject>Research Article</subject>
            </subj-group>
            <subj-group subj-group-type="article-collection" specific-use="Regular">
              <subject>Citation Typing Ontology (CiTO) Pilot</subject>
            </subj-group>
          </article-categories>
          <title-group>
            <article-title xml:lang="en">DECIMER 1.0: deep learning for chemical image recognition using transformers</article-title>
          </title-group>
          <contrib-group>
            <contrib contrib-type="author" id="Au1">
              <name>
                <surname>Rajan</surname>
                <given-names>Kohulan</given-names>
              </name>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author" id="Au2">
              <name>
                <surname>Zielesny</surname>
                <given-names>Achim</given-names>
              </name>
              <xref ref-type="aff" rid="Aff2">2</xref>
            </contrib>
            <contrib contrib-type="author" corresp="yes" id="Au3">
              <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-6966-0814</contrib-id>
              <name>
                <surname>Steinbeck</surname>
                <given-names>Christoph</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
              <xref ref-type="corresp" rid="IDs13321021005388_cor3">c</xref>
            </contrib>
            <aff id="Aff1">
              <label>1</label>
              <institution-wrap>
                <institution-id institution-id-type="GRID">grid.9613.d</institution-id>
                <institution-id institution-id-type="ISNI">0000 0001 1939 2794</institution-id>
                <institution content-type="org-division">Institute for Inorganic and Analytical Chemistry</institution>
                <institution content-type="org-name">Friedrich-Schiller-University Jena</institution>
              </institution-wrap>
              <addr-line content-type="street">Lessingstr. 8</addr-line>
              <addr-line content-type="postcode">07743</addr-line>
              <addr-line content-type="city">Jena</addr-line>
              <country country="DE">Germany</country>
            </aff>
            <aff id="Aff2">
              <label>2</label>
              <institution-wrap>
                <institution-id institution-id-type="GRID">grid.454254.6</institution-id>
                <institution-id institution-id-type="ISNI">0000 0004 0647 4362</institution-id>
                <institution content-type="org-division">Institute for Bioinformatics and Chemoinformatics</institution>
                <institution content-type="org-name">Westphalian University of Applied Sciences</institution>
              </institution-wrap>
              <addr-line content-type="street">August-Schmidt-Ring 10</addr-line>
              <addr-line content-type="postcode">45665</addr-line>
              <addr-line content-type="city">Recklinghausen</addr-line>
              <country country="DE">Germany</country>
            </aff>
          </contrib-group>
          <author-notes>
            <corresp id="IDs13321021005388_cor3">
              <label>c</label>
              <email>HIDDEN</email>
            </corresp>
          </author-notes>
          <pub-date date-type="pub" publication-format="electronic">
            <day>17</day>
            <month>8</month>
            <year>2021</year>
          </pub-date>
          <pub-date date-type="collection" publication-format="electronic">
            <month>12</month>
            <year>2021</year>
          </pub-date>
          <volume>13</volume>
          <issue seq="61">1</issue>
          <elocation-id>61</elocation-id>
          <history>
            <date date-type="registration">
              <day>26</day>
              <month>7</month>
              <year>2021</year>
            </date>
            <date date-type="received">
              <day>29</day>
              <month>4</month>
              <year>2021</year>
            </date>
            <date date-type="accepted">
              <day>25</day>
              <month>7</month>
              <year>2021</year>
            </date>
            <date date-type="online">
              <day>17</day>
              <month>8</month>
              <year>2021</year>
            </date>
          </history>
          <pub-history>
            <event event-type="Update">
              <event-desc>In the original publication, Ref.19 lead to the incorrect article. The article has been updated and the reference link has been corrected.</event-desc>
              <date>
                <day>24</day>
                <month>11</month>
                <year>2021</year>
              </date>
            </event>
          </pub-history>
          <permissions>
            <copyright-statement content-type="compact">© The Author(s) 2021</copyright-statement>
            <copyright-year>2021</copyright-year>
            <copyright-holder>The Author(s)</copyright-holder>
            <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
              <license-p><bold>Open Access</bold>This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>. The Creative Commons Public Domain Dedication waiver (<ext-link xlink:href="http://creativecommons.org/publicdomain/zero/1.0/" ext-link-type="uri">http://creativecommons.org/publicdomain/zero/1.0/</ext-link>) applies to the data made available in this article, unless otherwise stated in a credit line to the data.</license-p>
            </license>
          </permissions>
          <abstract xml:lang="en" id="Abs1">
            <title>Abstract</title>
            <p id="Par1">The amount of data available on chemical structures and their properties has increased steadily over the past decades. In particular, articles published before the mid-1990 are available only in printed or scanned form. The extraction and storage of data from those articles in a publicly accessible database are desirable, but doing this manually is a slow and error-prone process. In order to extract chemical structure depictions and convert them into a computer-readable format, Optical Chemical Structure Recognition (OCSR) tools were developed where the best performing OCSR tools are mostly rule-based. The DECIMER (Deep lEarning for Chemical ImagE Recognition) project was launched to address the OCSR problem with the latest computational intelligence methods to provide an automated open-source software solution. Various current deep learning approaches were explored to seek a best-fitting solution to the problem. In a preliminary communication, we outlined the prospect of being able to predict SMILES encodings of chemical structure depictions with about 90% accuracy using a dataset of 50–100 million molecules. In this article, the new DECIMER model is presented, a transformer-based network, which can predict SMILES with above 96% accuracy from depictions of chemical structures without stereochemical information and above 89% accuracy for depictions with stereochemical information.</p>
            <p id="Par2">
              <fig id="Figa" position="anchor">
                <graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Figa_HTML.png" position="anchor" id="MO1" />
              </fig>
            </p>
          </abstract>
          <kwd-group xml:lang="en">
            <title>Keywords</title>
            <kwd>Chemical data extraction</kwd>
            <kwd>Deep learning</kwd>
            <kwd>Neural networks</kwd>
            <kwd>Optical chemical structure recognition</kwd>
          </kwd-group>
          <funding-group>
            <award-group>
              <funding-source>
                <institution-wrap>
                  <institution>Carl-Zeiss-Foundation</institution>
                </institution-wrap>
              </funding-source>
            </award-group>
            <award-group>
              <funding-source>
                <institution-wrap>
                  <institution>Friedrich-Schiller-Universität Jena (1010)</institution>
                </institution-wrap>
              </funding-source>
            </award-group>
          </funding-group>
          <custom-meta-group>
            <custom-meta>
              <meta-name>publisher-imprint-name</meta-name>
              <meta-value>Springer</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>volume-issue-count</meta-name>
              <meta-value>1</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-article-count</meta-name>
              <meta-value>89</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-pricelist-year</meta-name>
              <meta-value>2021</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-copyright-holder</meta-name>
              <meta-value>The Author(s)</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-copyright-year</meta-name>
              <meta-value>2021</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-contains-esm</meta-name>
              <meta-value>No</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-numbering-style</meta-name>
              <meta-value>Unnumbered</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-year</meta-name>
              <meta-value>2021</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-month</meta-name>
              <meta-value>7</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-day</meta-name>
              <meta-value>26</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>volume-type</meta-name>
              <meta-value>Regular</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-product</meta-name>
              <meta-value>ArchiveJournal</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>numbering-style</meta-name>
              <meta-value>Unnumbered</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-grants-type</meta-name>
              <meta-value>OpenChoice</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>metadata-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>abstract-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bodypdf-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bodyhtml-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bibliography-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>esm-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>online-first</meta-name>
              <meta-value>false</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>pdf-file-reference</meta-name>
              <meta-value>BodyRef/PDF/13321_2021_Article_538.pdf</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>pdf-type</meta-name>
              <meta-value>Typeset</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>target-type</meta-name>
              <meta-value>OnlinePDF</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-type</meta-name>
              <meta-value>Regular</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-type</meta-name>
              <meta-value>OriginalPaper</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-primary</meta-name>
              <meta-value>Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Computer Applications in Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Documentation and Information in Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Theoretical and Computational Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Computational Biology/Bioinformatics</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-collection</meta-name>
              <meta-value>Chemistry and Materials Science</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>open-access</meta-name>
              <meta-value>true</meta-value>
            </custom-meta>
          </custom-meta-group>
        </article-meta>
      </front>
      <body>
        <sec id="Sec1">
          <title>Introduction</title>
          <p id="Par25">Scientists build on the results of their peers. Knowledge and data arising from previous research is shared through scientific publications and increasingly through the deposition of data in repositories. To enable progress in core areas of chemistry, the availability of open data has a beneficial impact [<xref ref-type="bibr" rid="CR1">1</xref>]. Most of the chemical data is published in the form of text and images in scientific publications [<xref ref-type="bibr" rid="CR2">2</xref>]. Retrieving and storing published information into open-access databases will facilitate the reuse as well as the development of new methods and products [<xref ref-type="bibr" rid="CR3">3</xref>]. But most of the data published is non-machine readable and manual curation is still the standard. This manual work is tedious and error-prone [<xref ref-type="bibr" rid="CR4">4</xref>]. The increase of publications with valuable chemical information [<xref ref-type="bibr" rid="CR5">5</xref>] does encourage the development of tools for automated data retrieval. Information retrieval with corresponding database storage is an ongoing task and multiple projects are working towards this. The CHEMDNER [<xref ref-type="bibr" rid="CR6">6</xref>] challenge is one good example of it.</p>
          <p id="Par26">There has been a significant amount of development in the field of chemical data mining [<xref ref-type="bibr" rid="CR5">5</xref>] with a couple of open source solutions including ChemDataExtracter [<xref ref-type="bibr" rid="CR4">4</xref>] and ChemSchematicResolver (CSR) [<xref ref-type="bibr" rid="CR7">7</xref>], building upon each other. A scanned page of an article, however, cannot be handled by CSR, and not all publications can be processed by CSR. Although most publishers offer documents in markup format, many of the older publications are stored in scanned PDF files. For example, the Journal of Natural Products did publish scientific articles since 1978, one of their issues even dates back to 1949; however, these publications were not formatted in markup format. So retrieving this information is a difficult process.</p>
          <p id="Par27">Image mining methods for chemical structure depictions and their conversion into a machine-readable file format is a comparatively small research area [<xref ref-type="bibr" rid="CR8">8</xref>]. The automatic recognition of chemical structure depictions and their conversion into machine-readable formats such as SMILES [<xref ref-type="bibr" rid="CR9">9</xref>] or InChI [<xref ref-type="bibr" rid="CR10">10</xref>], however, is an important task for creating corresponding databases. The publications include chemical structure depictions along with other information in textual format and contain some information presented as tables, graphs, spectra, etc.</p>
          <p id="Par28">Optical Chemical Structure Recognition (OCSR) software was built to parse chemical structure depictions. However, most of these softwares/tools are unable to handle whole page articles or scanned ones. In order to use these tools, it is necessary to segment the chemical structure depictions into separate images from printed literature and then use these segmented images as inputs. Also, the user should ensure that the image does not contain any other elements or artefacts other than a representation of a chemical structure in a segmented image. All of the available systems vary in their accuracy, OSRA [<xref ref-type="bibr" rid="CR11">11</xref>] and MolVec [<xref ref-type="bibr" rid="CR11">11</xref>, <xref ref-type="bibr" rid="CR12">12</xref>] can resolve a chemical structure with 80–90% accuracy [<xref ref-type="bibr" rid="CR8">8</xref>].</p>
          <p id="Par29">With the advancements in computer vision, a few deep learning-based OCSR tools have been developed, e.g. by Staker et al. [<xref ref-type="bibr" rid="CR13">13</xref>], the first machine learning-based system for segmentation of images and resolution into a computer-readable format. Another deep learning-based work is Chemgrapher [<xref ref-type="bibr" rid="CR14">14</xref>], where multiple neural networks are combined for the recognition of molecules. Recently, there was a new publication called ChemPix [<xref ref-type="bibr" rid="CR15">15</xref>], a deep learning-based method that was developed to recognize hand drawn hydrocarbon chemical structures. Another recent publication describes SMILES generation from images [<xref ref-type="bibr" rid="CR16">16</xref>] where an encoder–decoder method with a pre-trained decoder is used from previous work [<xref ref-type="bibr" rid="CR17">17</xref>]. These contributions demonstrate an increasing interest in this field of research. Even though they all claim to provide enhanced accuracy, none of them is accessible to the general public to date.</p>
          <p id="Par30">The DECIMER (Deep lEarning for Chemical IMagE Recognition) project [<xref ref-type="bibr" rid="CR18">18</xref>] is an end-to-end open-source system that can perform chemical structure segmentation on scanned scientific literature and use the segmented structure depictions to convert them into a computer-readable molecular file format.</p>
          <p id="Par31">In our work on DECIMER-Segmentation [<xref ref-type="bibr" rid="CR19">19</xref>], the segmentation workflow was specifically addressed. Here we now present a transformer-based algorithm that converts the bitmap of a chemical structure depiction into a computer-readable format. The system does not inherit any rules or make any assumptions, thus, it solely relies on the chemical structure depiction to perform its task.</p>
          <p id="Par32">The DECIMER algorithm was primarily inspired by the successful AlphaGo Zero algorithm [<xref ref-type="bibr" rid="CR20">20</xref>] developed by Google’s DeepMind. The success of AlphaGo Zero allowed us to realize that very challenging problems could be adequately tackled by having a sufficient amount of data and using an adequate neural network architecture. With dozens of millions of molecules available in the databases like PubChem [<xref ref-type="bibr" rid="CR21">21</xref>], Zinc20 [<xref ref-type="bibr" rid="CR22">22</xref>], and GDB-17 [<xref ref-type="bibr" rid="CR23">23</xref>], we have shown in our preliminary communication that our goal to have a system that can work with about 90% accuracy, could be achieved by training the network on a dataset of 50–100 million molecules.</p>
        </sec>
        <sec id="Sec2" sec-type="materials|methods">
          <title>Materials and methods</title>
          <p id="Par33">DECIMER is a completely data-driven solution to chemical image recognition. Recent impressive applications of deep learning, such as the AlphaGo Zero example, all relied on the availability of very large to unlimited amounts of training data. In our case, one of the largest chemical databases on the planet, PubChem [<xref ref-type="bibr" rid="CR21">21</xref>], was used.</p>
          <sec id="Sec3">
            <title>Data preparation</title>
            <p id="Par34">The latest version of Pubchem was downloaded from their FTP site. All explicit hydrogens were removed using the CDK [<xref ref-type="bibr" rid="CR24">24</xref>] and isomeric SMILES [<xref ref-type="bibr" rid="CR9">9</xref>] were generated, which inherit the canonicalisation and retain the stereochemistry information. After generating the SMILES, the following set of rules were used to filter the dataset for a balanced dataset. The molecules in both training and test set should,<list list-type="bullet"><list-item><p id="Par35">have a molecular weight of fewer than 1500 Daltons,</p></list-item><list-item><p id="Par36">not possess counter ions,</p></list-item><list-item><p id="Par37">only contain the elements C, H, O, N, P, S, F, Cl, Br, I, Se and B,</p></list-item><list-item><p id="Par38">not contain isotopes of Hydrogens (D, T),</p></list-item><list-item><p id="Par39">have 3–40 bonds,</p></list-item><list-item><p id="Par40">not contain any charged groups including zwitterionic forms,</p></list-item><list-item><p id="Par41">only contain implicit hydrogens, except in functional groups,</p></list-item><list-item><p id="Par42">have less than 40 SMILES characters,</p></list-item><list-item><p id="Par43">no stereochemistry is allowed.</p></list-item></list></p>
            <p id="Par44">The resulting main dataset contains 39 million molecules. The same rule set was used to generate a second dataset, but the molecules with charged groups including zwitterionic forms and stereochemistry were retained. Furthermore, the molecules containing tokens that were rare in the dataset were removed (see “<xref rid="Sec5" ref-type="sec">Tokenization</xref>” section), resulting in a dataset that contains approximately 37 million molecules. Adding extra information caused the SMILES character length to get longer. Later, when the rule that SMILES length should not exceed 40 characters was applied, more molecules were removed. In the end, this resulted in  dataset 2 being smaller in size than  dataset 1.</p>
            <p id="Par45">Molecular bitmap images were generated using the CDK Structure Diagram Generator (SDG). The CDK depiction generator enables the generation of production-quality 2D images. In this work, every molecule was randomly rotated and depicted as 8 Bit PNG images with a 299 × 299 resolution. It was made sure that each image contains only one structure.</p>
            <p id="Par46">Using the set of images from the second dataset and introducing image augmentations the third dataset was generated. The image augmentations were applied using the imgaug [<xref ref-type="bibr" rid="CR25">25</xref>] python package. One of the following augmentations was randomly applied to the images.<list list-type="bullet"><list-item><p id="Par47">Gaussian Blur</p></list-item><list-item><p id="Par48">Average Blur</p></list-item><list-item><p id="Par49">Additive Gaussian Noise</p></list-item><list-item><p id="Par50">Salt and Pepper</p></list-item><list-item><p id="Par51">Salt</p></list-item><list-item><p id="Par52">Pepper</p></list-item><list-item><p id="Par53">Coarse Dropout</p></list-item><list-item><p id="Par54">Gamma Contrast</p></list-item><list-item><p id="Par55">Sharpen</p></list-item><list-item><p id="Par56">Enhance Brightness</p></list-item></list></p>
            <p id="Par57">Often, deep learning in chemistry is using SMILES as a textual representation of structures. Training Neural Networks (NNs) directly with SMILES, however, has pitfalls: In order to generate tokens, a set of rules has to be set up on how and where to split long strings of SMILES into smaller words. After training, invalid SMILES are often encountered in the predictions, which results in overall significantly reduced accuracy. To tackle this problem there are two new text representations named DeepSMILES [<xref ref-type="bibr" rid="CR26">26</xref>] and SELFIES [<xref ref-type="bibr" rid="CR27">27</xref>]. DeepSMILES exhibited better results in comparison to standard SMILES, but again invalid DeepSMILES caused similar problems. In the end, SELFIES were used, since they can be split easily into tokens by splitting the SELFIE at close (“]”) and open brackets (“[”). No further rules had to be applied to split them into a working token set (Fig. <xref rid="Fig1" ref-type="fig">1</xref>). Also, they translate back into a SMILES string without any errors. All SMILES strings in our 3 datasets were converted into SELFIES using Python.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><p>SMILES, DeepSMILES and SELFIES represented as tokens with a space character as a delimiter</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig1_HTML.png" id="MO2" /></fig></p>
            <p id="Par58">To summarize, the datasets used in this work are:<list list-type="order"><list-item><p id="Par59">Dataset 1: PNG images of chemical structure depictions plus corresponding canonical SMILES converted into SELFIES, without stereochemical information and charged groups.</p></list-item><list-item><p id="Par60">Dataset 2: PNG images of chemical structure depictions plus corresponding canonical SMILES converted into SELFIES, with stereochemical information and charged groups.</p></list-item><list-item><p id="Par61">Dataset 3: Augmented PNG images of chemical structure depictions plus corresponding canonical SMILES converted into SELFIES, with stereochemical information and charged groups.</p></list-item></list></p>
            <p id="Par62">Test datasets were selected from 10% of each dataset. To ensure that the chemical diversity of test and training data was similar, 10% of SMILES were selected as Test dataset using the RDKIT MaxMin algorithm. An overview of all the train and test datasets and the naming of subsets can be found in Table <xref rid="Tab1" ref-type="table">1</xref>.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Overview of the datasets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" /><th align="left" colspan="4"><p>Dataset 1</p></th><th align="left" colspan="2"><p>Dataset 2</p></th><th align="left" colspan="2"><p>Dataset 3</p></th></tr><tr><th align="left"><p>Total dataset size</p></th><th align="left" colspan="4"><p>39 million</p></th><th align="left" colspan="2"><p>37 million</p></th><th align="left" colspan="2"><p>37 million</p></th></tr><tr><th align="left" /><th align="left"><p>Subset 1</p></th><th align="left"><p>Subset 2</p></th><th align="left"><p>Subset 3</p></th><th align="left"><p>Subset 4</p></th><th align="left"><p>Subset 5</p></th><th align="left"><p>Subset 6</p></th><th align="left"><p>Non augmented test set</p></th><th align="left"><p>Augmented test set</p></th></tr></thead><tbody><tr><td align="left"><p>Train dataset size</p></td><td char="," align="char"><p>921,600</p></td><td char=", ," align="char"><p>10,240,000</p></td><td char=", ," align="char"><p>15,360,000</p></td><td char=", ," align="char"><p>35,002,240</p></td><td char=", ," align="char"><p>15,360,000</p></td><td char=", ," align="char"><p>33,304,320</p></td><td char=", ," align="char"><p>33,304,320</p></td><td char=", ," align="char"><p>33,304,320</p></td></tr><tr><td align="left"><p>Test dataset size</p></td><td char="," align="char"><p>102,400</p></td><td char=", ," align="char"><p>1,024,000</p></td><td char=", ," align="char"><p>1,536,000</p></td><td char=", ," align="char"><p>3,929,093</p></td><td char=", ," align="char"><p>1,536,000</p></td><td char=", ," align="char"><p>3,700,480</p></td><td char=", ," align="char"><p>2,000,000</p></td><td char=", ," align="char"><p>2,000,000</p></td></tr></tbody></table></table-wrap></p>
          </sec>
          <sec id="Sec4">
            <title>Image feature extraction</title>
            <p id="Par63">A Convolutional Neural Network (CNN) is used to parse the images, where the second last layer retains the features to be extracted for calculations. For training our model, we evaluated InceptionV3 [<xref ref-type="bibr" rid="CR28">28</xref>] and EfficientNet-B3 [<xref ref-type="bibr" rid="CR29">29</xref>], see Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The EfficientNet base model for an image size of 299 × 299 outperforms InceptionV3 in our task at hand [<xref ref-type="bibr" rid="CR29">29</xref>].<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><p>Schema of the encoder–decoder network used in DECIMER for training</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig2_HTML.png" id="MO3" /></fig></p>
            <p id="Par64">Every image was scaled to a resolution of 299 × 299 pixels and the pixel values were normalized to interval − 1 to 1, which corresponds to the format used on InceptionV3 and EfficientNet-B3. Then the image features were extracted into a vector format using the pre-trained weights of ImageNet [<xref ref-type="bibr" rid="CR30">30</xref>] on InceptionV3 and pre-trained weights of Noisy-student [<xref ref-type="bibr" rid="CR31">31</xref>] training on EfficientNet-B3.</p>
            <p id="Par65">For Inception V3 a feature vector size of 8 × 8 × 2048 and for EfficientNet-B3 a feature vector size of 10 × 10 × 1536 was obtained. The 8 × 8 × 2048 and 10 × 10 × 1536 dimensions are simply the shape of the last output layer in Inception-V3 and EfficientNet-B3 networks. Since Inception-V3 and EfficientNet-B3 are networks built for image classification, these features are then used in the final layer of these networks to classify the images.</p>
            <p id="Par66">Next, these extracted feature vectors were saved into NumPy arrays.</p>
          </sec>
          <sec id="Sec5">
            <title>Tokenization</title>
            <p id="Par67">SELFIES were tokenized into a unique set of tokens and padded to fit the maximum length of SELFIES strings. Here the Keras [<xref ref-type="bibr" rid="CR32">32</xref>] tokenizer in Tensorflow 2.3 [<xref ref-type="bibr" rid="CR33">33</xref>] was used. Table <xref rid="Tab2" ref-type="table">2</xref> summarizes the details regarding the tokens present in each dataset.<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>SELFIES tokens and maximum length found on each dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Datasets</p></th><th align="left"><p>Number of SELFIES tokens</p></th><th align="left"><p>The maximum length of the SELFIES strings</p></th></tr></thead><tbody><tr><td align="left"><p>Dataset 1</p></td><td char="." align="char"><p>27</p></td><td char="." align="char"><p>47</p></td></tr><tr><td align="left"><p>Dataset 2</p></td><td char="." align="char"><p>61</p></td><td char="." align="char"><p>47</p></td></tr><tr><td align="left"><p>Dataset 3</p></td><td char="." align="char"><p>61</p></td><td char="." align="char"><p>47</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par68">Tokens in Dataset 1: [C], [=C], [Branch1_1], [N], [Ring1], [O], [Branch1_2], [Expl=Ring1], [=N], [Branch2_1], [Branch1_3], [Ring2], [S], [F], [=O], [Branch2_2], [Cl], [Branch2_3], [#C], [Br], [P], [=S], [I], [=P], [Expl=Ring2], [B], [#N].</p>
            <p id="Par69">Tokens in Dataset 2 &amp; 3: [C], [=C], [Branch1_1], [Branch1_2], [Ring1], [N], [O], [=O], [=N], [Ring2], [Branch2_1], [S], [Branch1_3], [F], [Branch2_2], [Cl], [Branch2_3], [Br], [#C], [/C], [#N], [P], [C@Hexpl], [C@@Hexpl], [=N+expl], [=S], [=N-expl], [I], [O-expl], [N+expl], [\C], [/N], [/O], [C@expl], [B], [C@@expl], [\N], [Expl/Ring1], [\O], [NH+expl], [I-expl], [Expl\Ring1], [P+expl], [NH2+expl], [/Cl], [/S], [NH3+expl], [Cl-expl], [/F], [#N+expl], [C-expl], [\S], [N-expl], [=NH+expl], [=I], [S-expl], [\Cl], [S+expl], [#C-expl], [B-expl], [/Br].</p>
            <p id="Par70">Complete list of rare tokens which were removed: [=B], [=Cl], [=Br], [#I], [=I], [#S], [Expl#Ring1], [#B], [#P], [=Br], [Expl#Ring2].</p>
            <sec id="Sec6">
              <title>Generating TFRecords</title>
              <p id="Par71">Extracted feature vectors and tokenized data must be converted into TFRecords before training the models on Tensor Processing Units (TPU) [<xref ref-type="bibr" rid="CR34">34</xref>]. TFRecords stores the data in binary format which allows training the models faster using GPUs and TPUs. The TPUs are currently available through the Google Cloud Platform. TFRecords are stored in a Google cloud bucket for training. This reduces the training time significantly and reduces the overhead of loading the data and performing the calculations on a TPU.</p>
              <p id="Par72">Using a custom python script all the datasets were converted into 75 MB chunks of TFRecords. Each TFRecord contains 128 Datapoints (128 image vectors + 128 tokenized strings).</p>
              <p id="Par73">After generating the TFRecords locally, they were moved to a Google cloud storage bucket.</p>
            </sec>
          </sec>
          <sec id="Sec7">
            <title>Networks</title>
            <p id="Par74">In this work, two different types of networks were evaluated. Initially, an encoder–decoder model was tested, which is based on the work by Google on their <italic>Show, Attend and Tell</italic> [<xref ref-type="bibr" rid="CR35">35</xref>] publication. The network eventually selected is a transformer-based model based on the <italic>Attention is all you need </italic>[<xref ref-type="bibr" rid="CR36">36</xref>] publication by Google. The models are written using Python and Tensorflow 2.3 as a backend.</p>
          </sec>
          <sec id="Sec8">
            <title>Encoder–decoder network</title>
            <p id="Par75">The encoder–decoder network used is an unaltered implementation by the TensorFlow team [<xref ref-type="bibr" rid="CR37">37</xref>]. The model uses a CNN-based encoder with a ReLU activation function, a soft attention mechanism introduced by Bahdanau et al. [<xref ref-type="bibr" rid="CR38">38</xref>] and the RNN based decoder uses Gated Recurrent Units (GRU) and two fully connected layers. The decoder consists of 1024 units and an embedding size of 512.</p>
            <p id="Par76">The network is trained using an Adam optimizer [<xref ref-type="bibr" rid="CR39">39</xref>] with a learning rate of 0.0005 throughout all learning epochs. The loss is calculated using sparse categorical cross-entropy between real and predicted SELFIES.</p>
            <sec id="Sec9">
              <title>Transformer network</title>
              <p id="Par77">The transformer model (Fig. <xref rid="Fig3" ref-type="fig">3</xref>) used in this work is the model from the 2017 publication <italic>Attention is all you need</italic>. It uses four encoder–decoder layers and eight parallel attention heads. The attention has a dimension size of 512 and the feed-forward networks have a dimension size of 2048. Here the number of rows and columns corresponds to our image features extracted into a vector format, so for the InceptionV3, the feature vector size is 8 × 8 × 2048 and for the EfficientNet-B3 it is 10 × 10 × 1536. A drop out of 0.1 is used to avoid overfitting.<fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><p>Schema of the transformer network used in DECIMER for training</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig3_HTML.png" id="MO4" /></fig></p>
              <p id="Par78">The network is trained using an Adam optimizer with a custom learning rate scheduler according to [<xref ref-type="bibr" rid="CR36">36</xref>]. The loss is calculated using sparse categorical cross-entropy between real and predicted SELFIES.</p>
            </sec>
          </sec>
          <sec id="Sec10">
            <title>Training the models</title>
            <p id="Par79">Initially, all the models were trained using an in-house server equipped with an Nvidia V100 Tesla with 32 GB GPU, 384 GB of RAM and two Intel(R) Xeon(R) Gold 6230 CPUs. The details regarding the scaling and performance were explained in our previous publication [<xref ref-type="bibr" rid="CR18">18</xref>]. For this work, a model with a dataset of 1 million molecules is initially trained using the same GPU equipped server. A batch size of 512 images is used to train the model, resulting in an epoch time of 29 min and 48 s, on average. For a complete convergence of the model, it took about 1 day, 5 h and 48 min on the hardware mentioned above.</p>
            <p id="Par80">On a TPU v3-8 (TPU version 3 with 8 nodes) the same model was trained with a batch size of 1024 which is distributed between 8 nodes, and it took on average 8 min and 41 s per epoch and for a complete convergence of the model, it took 8 h 41 min and 4 s. This is a reduction of 71.9% in computation time and we, therefore, decided to train all models with the TensorFlow distributed training API using the Tensor Processing Units v3-8.</p>
          </sec>
          <sec id="Sec11">
            <title>Testing the models</title>
            <p id="Par81">All the models were trained until their training loss converged, then each model was tested with a test data size of 10% of the training data. Throughout the process of selecting molecules for the test dataset, the RDKit [<xref ref-type="bibr" rid="CR40">40</xref>] MaxMin algorithm is used to select a diverse test dataset covering the same chemical space as the training dataset.</p>
            <p id="Par82">Test dataset evaluations were performed on the GPUs. Predicted SELFIES were decoded back to SMILES and then the Tanimoto Similarity Index was calculated for the original and predicted SMILES using PubChem fingerprints, included in the CDK. The Tanimoto Similarity Index provides useful information because it makes a difference whether a wrong prediction is completely wrong or provides a result very similar to the correct molecule was used because the predictions do not always correspond to the same molecule. The Tanimoto similarity thereby provides a quantitative measure of how well the network is able to “understand” graphical chemical structure representations.</p>
            <p id="Par83">Apart from that for the predictions with the Tanimoto similarity index of 1.0, we additionally generated InChIs using the CDK to perform an isomorphism check and determined whether Tanimoto 1.0 predictions are a good proxy for structure identity.</p>
            <p id="Par84">Models trained with augmentations were tested with augmented images and with images without any augmentation.</p>
          </sec>
        </sec>
        <sec id="Sec12">
          <title>Results and discussion</title>
          <sec id="Sec13">
            <title>Computational considerations</title>
            <p id="Par85">Training large datasets such as the ones used here on deep neural networks take months even on GPUs, let alone regular CPUs. For performance measure, a dataset with 1 million molecules was trained for 50 epochs on an Nvidia Tesla V100 GPU and the same model was also trained on a TPU V3-8 (version 3 TPU with 8 nodes) and TPU V3-32 (version 3 TPU with 32 nodes).</p>
            <p id="Par86">Training a model on a V3-8 TPU helped by increasing training speed up to 4 times compared to a V100 GPU and by using a V3-32 TPU a 16 times faster training speed was achieved, see Fig. <xref rid="Fig4" ref-type="fig">4</xref>. Concerning these results and considering the costs of V3-32 TPUs, it was decided to train all the models on a V3-8 TPU.<fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><p>Training time comparison between a GPU and TPUs (lower is better)</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig4_HTML.png" id="MO5" /></fig></p>
            <p id="Par87">To evaluate if testing accuracy could be improved by increasing the training dataset size, different subsets generated using dataset 1 were trained on TPU V3-8. The maximum length of SELFIES strings stayed the same throughout the training. As shown in Fig. <xref rid="Fig5" ref-type="fig">5</xref>, training time increases with the increase in datasets.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><p>Average training time per epoch with increasing training dataset size</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig5_HTML.png" id="MO6" /></fig></p>
            <p id="Par88">It would take a considerable amount of time to examine the performance of the network using a bigger dataset. For the initial tests, Subset 1: a subset of 1 million was used, which was derived from Dataset 1. We split the dataset into 90% training data (921,600) and 10% test data (102,400) using the RDKIT MaxMin algorithm to ensure that the test data picked are diverse and resemble the training dataset.</p>
          </sec>
          <sec id="Sec14">
            <title>Image feature extraction test</title>
            <p id="Par89">Correct extraction of the image features will result in an improved overall model at the end. In our previous work, the InceptionV3 model was used for image feature extraction. InceptionV3 is a state-of-the-art image classification network. A newer network, called EfficientNet, was created to enable better classification accuracy, and the results of noisy-student training using EfficientNet [<xref ref-type="bibr" rid="CR31">31</xref>] were better than the InceptionV3 network. The EfficientNet-B3 model was then compared to InceptionV3 while still using the same image size (299 × 299) to test whether EfficientNet-based image feature extraction would improve our models’ accuracy.</p>
            <p id="Par90">To compare the InceptionV3 feature extraction with EfficientNet-B3 feature extraction a subset of 1 million molecules was used. Using these models, the features were extracted and then used to train encoder–decoder based networks for 60 epochs until the training loss converged. The training time for the network that uses the features extracted using the InceptionV3 model was found to be shorter than the network which uses the EfficientNet-B3 model.</p>
            <p id="Par91">After training, the models were tested with a test dataset. The predicted SELFIES were retranslated into SMILES strings and the Tanimoto similarity index was calculated between original SMILES and the retranslated SMILES. Here, no errors have occurred in translating SELFIES to SMILES. Table <xref rid="Tab3" ref-type="table">3</xref> summarizes the evaluation.<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>1 million molecules model testing results for comparing InceptionV3 and EfficientNet-B3 feature extraction</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Metrics</p></th><th align="left"><p>InceptionV3</p></th><th align="left"><p>EfficientNet-B3</p></th></tr></thead><tbody><tr><td align="left"><p>Average training time per epoch</p></td><td align="left"><p>7 min 34 s</p></td><td align="left"><p>8 min 57 s</p></td></tr><tr><td align="left"><p>Tanimoto</p></td><td align="left"><p>0.5459</p></td><td align="left"><p>0.6345</p></td></tr><tr><td align="left"><p>Tanimoto 1.0</p></td><td align="left"><p>1.41%</p></td><td align="left"><p>7.03%</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par92">The Tanimoto 1.0 count indicated that the EfficientNet-B3 model led to a remarkable overall performance increase, so it was used for the entire work.</p>
          </sec>
          <sec id="Sec15">
            <title>Encoder–decoder model vs. transformer model</title>
            <p id="Par93">In our previous work [<xref ref-type="bibr" rid="CR18">18</xref>], the encoder–decoder network was extensively explored. Meanwhile, great progress was made in transformer-based networks and the results seemed promising, so we decided to implement a transformer-based network in this work as well.</p>
            <p id="Par94">First, the transformer network was tested with InceptionV3 based image feature extraction, then it was tested using the EfficientNet-B3 based image feature extraction. The extracted image features with tokenized SELFIES were used as inputs for the transformer. For this work, the same 1 million molecules subset was used with a 90:10 split for training and testing.</p>
            <p id="Par95">The models were trained on TPU V3-8 until the training loss converged. The average time for transformer-based models was higher than the other, and the highest average training time was recorded for the EfficientNet-B3 Transformer network. Once the training was completed, the models were tested using the same test set. Table <xref rid="Tab4" ref-type="table">4</xref> summarizes the final evaluation.<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>Comparing the encoder–decoder- and transformer-based approach with a 1 million images test dataset</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"><p>Metrics</p></th><th align="left" colspan="2"><p>Encoder–decoder</p></th><th align="left" colspan="2"><p>Transformer</p></th></tr><tr><th align="left"><p>InceptionV3</p></th><th align="left"><p>EfficientNet-B3</p></th><th align="left"><p>InceptionV3</p></th><th align="left"><p>EfficientNet-B3</p></th></tr></thead><tbody><tr><td align="left"><p>Average training time per epoch</p></td><td align="left"><p>7 min 34 s</p></td><td align="left"><p>8 min 57 s</p></td><td align="left"><p>8 min 33 s</p></td><td align="left"><p>9 min 27 s</p></td></tr><tr><td align="left"><p>Tanimoto</p></td><td align="left"><p>0.5459</p></td><td align="left"><p>0.6345</p></td><td align="left"><p>0.8764</p></td><td align="left"><p>0.9371</p></td></tr><tr><td align="left"><p>Tanimoto 1.0</p></td><td align="left"><p>1.41%</p></td><td align="left"><p>7.03%</p></td><td align="left"><p>55.29%</p></td><td align="left"><p>74.57%</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par96">By comparing the Tanimoto 1.0 count, the transformer-based models clearly outperformed the encoder–decoder based models.</p>
            <p id="Par97">With these results, it was decided to train all the other datasets using transformers with image features extracted using EfficientNet-B3 based image feature extraction.</p>
          </sec>
          <sec id="Sec16">
            <title>Image feature extraction comparison using EfficientNet-B3 and B7</title>
            <p id="Par98">The work described in [<xref ref-type="bibr" rid="CR29">29</xref>] indicated that EfficientNet-B7 outperforms EfficientNet-B3 marginally by 2.7%. We, therefore, implemented EfficientNet-B7 image feature extraction and training on the extracted features. The number of parameters to train using EfficientNet-B7 (66 million parameters) compared to B3 (12 million parameters) is almost 5.5 times larger, however, which makes the network rather big and complex. Furthermore, images had to be rescaled to 600 × 600 for B7, in which the chemical structure depictions had to be magnified twice the normal scale. For B3, it is easy to use the images with a scale of 299 × 299 without any alterations.</p>
            <p id="Par99">To test these two image feature extraction methods and to see how well this helps us to achieve our main goal, a 1 million molecules image subset was used to train the transformer networks and the final models were evaluated using respective Images generated using the same test set. Table <xref rid="Tab5" ref-type="table">5</xref> summarizes the results.<table-wrap id="Tab5"><label>Table 5</label><caption xml:lang="en"><p>Comparison of evaluation of using EfficientNet-B3 and B7 for image feature extraction</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Metrics</p></th><th align="left"><p>EfficientNet-B3</p></th><th align="left"><p>EfficientNet-B7</p></th></tr></thead><tbody><tr><td align="left"><p>Train data size</p></td><td align="left"><p>921,600</p></td><td align="left"><p>921,600</p></td></tr><tr><td align="left"><p>Test data size</p></td><td align="left"><p>102,400</p></td><td align="left"><p>102,400</p></td></tr><tr><td align="left"><p>Train data size</p></td><td align="left"><p>0.46 TB</p></td><td align="left"><p>2.8 TB</p></td></tr><tr><td align="left"><p>Average training time</p></td><td align="left"><p>9 min 27 s</p></td><td align="left"><p>11 min 42 s</p></td></tr><tr><td align="left"><p>Tanimoto</p></td><td align="left"><p>0.9371</p></td><td align="left"><p>0.9669</p></td></tr><tr><td align="left"><p>Tanimoto 1.0</p></td><td align="left"><p>74.57%</p></td><td align="left"><p>84.82%</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par100">It is evident that the Image feature extraction using EfficientNet-B7 outperforms B3. We found, however, that most of the chemical structure depictions found on printed literature can easily fit the scale of 299 × 299, so to use the 600 × 600 scale the images should be upscaled. Upscaling will result in losing information which will be a major downside for this approach since the models majorly rely on the image features.</p>
            <p id="Par101">Chemical structure depictions larger than 299 × 299 square pixels can be downscaled easily to be used in our models without losing any pixel information. Thus the size of the image was decided to be 299 × 299 and the feature extraction was performed using EfficientNet-B3.</p>
            <p id="Par102">It may be possible in the future to use EfficientNet-B7 to extract image features for chemical image depictions with higher resolutions.</p>
          </sec>
          <sec id="Sec17">
            <title>The performance measure with increasing dataset size</title>
            <p id="Par103">To evaluate how the split percentage of training and test data affected the training efficiency, we use a small toy dataset (subset 1). The data was split into different sizes (see Table <xref rid="Tab6" ref-type="table">6</xref>) of train and test sets using the RDKit MaxMin algorithm, and then each model was trained separately and evaluated. Table <xref rid="Tab6" ref-type="table">6</xref> summarizes the results.<table-wrap id="Tab6"><label>Table 6</label><caption xml:lang="en"><p>Results of training the subset 1 with different train and test dataset sizes</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>No.</p></th><th align="left"><p>Train data size</p></th><th align="left"><p>Test data size</p></th><th align="left"><p>Split</p></th><th align="left"><p>Average time per epoch</p></th><th align="left"><p>Average Tanimoto</p></th><th align="left"><p>Tanimoto 1.0 (%)</p></th></tr></thead><tbody><tr><td align="left"><p>1</p></td><td char="." align="char"><p>102,400</p></td><td char="." align="char"><p>921,600</p></td><td align="left"><p>10|90</p></td><td char="." align="char"><p>42.22</p></td><td char="." align="char"><p>0.86</p></td><td char="." align="char"><p>45.05</p></td></tr><tr><td align="left"><p>2</p></td><td char="." align="char"><p>204,800</p></td><td char="." align="char"><p>819,200</p></td><td align="left"><p>20|80</p></td><td char="." align="char"><p>69.95</p></td><td char="." align="char"><p>0.91</p></td><td char="." align="char"><p>63.59</p></td></tr><tr><td align="left"><p>3</p></td><td char="." align="char"><p>307,200</p></td><td char="." align="char"><p>716,800</p></td><td align="left"><p>30|70</p></td><td char="." align="char"><p>199.52</p></td><td char="." align="char"><p>0.93</p></td><td char="." align="char"><p>71.63</p></td></tr><tr><td align="left"><p>4</p></td><td char="." align="char"><p>409,600</p></td><td char="." align="char"><p>614,400</p></td><td align="left"><p>40|60</p></td><td char="." align="char"><p>276.09</p></td><td char="." align="char"><p>0.94</p></td><td char="." align="char"><p>73.93</p></td></tr><tr><td align="left"><p>5</p></td><td char="." align="char"><p>512,000</p></td><td char="." align="char"><p>512,000</p></td><td align="left"><p>50|50</p></td><td char="." align="char"><p>320.25</p></td><td char="." align="char"><p>0.95</p></td><td char="." align="char"><p>77.37</p></td></tr><tr><td align="left"><p>6</p></td><td char="." align="char"><p>614,400</p></td><td char="." align="char"><p>409,600</p></td><td align="left"><p>60|40</p></td><td char="." align="char"><p>392.51</p></td><td char="." align="char"><p>0.96</p></td><td char="." align="char"><p>84.50</p></td></tr><tr><td align="left"><p>7</p></td><td char="." align="char"><p>716,800</p></td><td char="." align="char"><p>307,200</p></td><td align="left"><p>70|30</p></td><td char="." align="char"><p>448.91</p></td><td char="." align="char"><p>0.97</p></td><td char="." align="char"><p>85.38</p></td></tr><tr><td align="left"><p>8</p></td><td char="." align="char"><p>819,200</p></td><td char="." align="char"><p>204,800</p></td><td align="left"><p>80|20</p></td><td char="." align="char"><p>535.57</p></td><td char="." align="char"><p>0.96</p></td><td char="." align="char"><p>82.89</p></td></tr><tr><td align="left"><p>9</p></td><td char="." align="char"><p>921,600</p></td><td char="." align="char"><p>102,400</p></td><td align="left"><p>90|10</p></td><td char="." align="char"><p>560.47</p></td><td char="." align="char"><p>0.94</p></td><td char="." align="char"><p>75.06</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par104">Figure <xref rid="Fig6" ref-type="fig">6</xref> shows that model performance increases with training dataset size. The test data performance increases up to a 70:30 split and then drops slightly, for which we have no explanation. Since we assumed that for our much larger final training data (30 Mio) it would be beneficial for the network to see as much training data as possible, we used a 90:10 split for our final training. To see how well the transformer performs with an increased number of data another subset of 10 million molecules images which was derived from the Dataset 1 (Subset 2) was utilized. The image features were extracted using the InceptionV3 based network and the EfficientNet-B3 based network. Every dataset was converted into TFRecords and moved to the Google cloud. Two different models based on these two different image feature extractions were trained. After the model completed the training they were tested using a test dataset size of 1 million molecule images of chemical structure depictions. Table <xref rid="Tab7" ref-type="table">7</xref> summarizes the results.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><p>Average Tanimoto similarity indices and Tanimoto similarity 1.0 count with dataset number</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig6_HTML.png" id="MO7" /></fig><table-wrap id="Tab7"><label>Table 7</label><caption xml:lang="en"><p>Testing results of the models trained on 10 million molecule images of chemical structure depictions</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Metrics</p></th><th align="left"><p>InceptionV3</p></th><th align="left"><p>EfficientNet-B3</p></th></tr></thead><tbody><tr><td align="left"><p>Train data size</p></td><td align="left"><p>10,240,000</p></td><td align="left"><p>10,240,000</p></td></tr><tr><td align="left"><p>Test data size</p></td><td align="left"><p>1,024,000</p></td><td align="left"><p>1,024,000</p></td></tr><tr><td align="left"><p>Tanimoto</p></td><td align="left"><p>0.9310</p></td><td align="left"><p>0.9695</p></td></tr><tr><td align="left"><p>Tanimoto 1.0</p></td><td align="left"><p>74.52%</p></td><td align="left"><p>87.85%</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par105">Looking at the Tanimoto similarity average and the Tanimoto 1.0 count one can see that the dataset trained with the EfficientNet-B3 based image feature extraction method outperforms the InceptionV3 based method. This also was evident in the previous training with 1 million molecule images. With these results, the next set of training included only the EfficientNet-B3 based image feature extraction.</p>
            <p id="Par106">A total of four subsets were now extracted from Dataset 1, the train and test datasets were created using the RDKit MaxMin algorithm. All four datasets included the same number of tokens. All four datasets were converted into TFRecords and stored on Google Cloud Storage Buckets and used to train the models. Table <xref rid="Tab8" ref-type="table">8</xref> summarizes the overall results for different subsets.<table-wrap id="Tab8"><label>Table 8</label><caption xml:lang="en"><p>Test data results for subsets</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Metrics</p></th><th align="left"><p>Subset 1</p></th><th align="left"><p>Subset 2</p></th><th align="left"><p>Subset 3</p></th><th align="left"><p>Subset 4</p></th></tr></thead><tbody><tr><td align="left"><p>Train data size</p></td><td align="left"><p>921,600</p></td><td align="left"><p>10,240,000</p></td><td align="left"><p>15,360,000</p></td><td align="left"><p>35,002,240</p></td></tr><tr><td align="left"><p>Test data size</p></td><td align="left"><p>102,400</p></td><td align="left"><p>1,024,000</p></td><td align="left"><p>1,536,000</p></td><td align="left"><p>3,929,093</p></td></tr><tr><td align="left"><p>Tanimoto</p></td><td align="left"><p>0.9371</p></td><td align="left"><p>0.9691</p></td><td align="left"><p>0.9779</p></td><td align="left"><p>0.9923</p></td></tr><tr><td align="left"><p>Tanimoto 1.0</p></td><td align="left"><p>74.57%</p></td><td align="left"><p>87.88%</p></td><td align="left"><p>91.02%</p></td><td align="left"><p>96.47%</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par107">These results demonstrate an increasing trend of accurate predictions due to increasing data in the training datasets. In addition, with 35 million molecules training, we reached an average Tanimoto similarity of 0.99, along with a 96.47% Tanimoto 1.0 count. Because of using SELFIES as the input textual data, all of the predictions were successfully retranslated into valid molecules. An isomorphism check using InChIs was carried out in order to find out how many molecules in Tanimoto 1.0 are fully isomorphic.</p>
            <p id="Par108">InChI strings were generated using the CDK for all the predictions with a Tanimoto similarity index of 1.0 and then checked whether they are isomorphic or not by string matching.</p>
            <p id="Par109">Table <xref rid="Tab9" ref-type="table">9</xref> shows that 99% of all predictions which have Tanimoto 1.0 are structurally identical to the depicted molecule. Also with the increasing Training dataset size, the isomorphic structure count kept increasing slightly.<table-wrap id="Tab9"><label>Table 9</label><caption xml:lang="en"><p>Results of isomorphism calculations for the subsets of dataset 1</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Metrics</p></th><th align="left"><p>Subset 1</p></th><th align="left"><p>Subset 2</p></th><th align="left"><p>Subset 3</p></th><th align="left"><p>Subset 4</p></th></tr></thead><tbody><tr><td align="left"><p>Train data size</p></td><td align="left"><p>921,600</p></td><td align="left"><p>10,240,000</p></td><td align="left"><p>15,360,000</p></td><td align="left"><p>35,002,240</p></td></tr><tr><td align="left"><p>Test data size</p></td><td align="left"><p>102,400</p></td><td align="left"><p>1,024,000</p></td><td align="left"><p>1,536,000</p></td><td align="left"><p>3,929,093</p></td></tr><tr><td align="left"><p>Predictions with Tanimoto 1.0</p></td><td align="left"><p>74,176</p></td><td align="left"><p>899,941</p></td><td align="left"><p>1,398,028</p></td><td align="left"><p>3,790,273</p></td></tr><tr><td align="left"><p>Isomorphic predictions</p></td><td align="left"><p>98.63%</p></td><td align="left"><p>99.45%</p></td><td align="left"><p>99.59%</p></td><td align="left"><p>99.75%</p></td></tr><tr><td align="left"><p>Non-isomorphic predictions</p></td><td align="left"><p>1.37%</p></td><td align="left"><p>0.55%</p></td><td align="left"><p>0.41%</p></td><td align="left"><p>0.25%</p></td></tr></tbody></table></table-wrap></p>
          </sec>
          <sec id="Sec18">
            <title>Analysis of the predictions with low Tanimoto similarity indices</title>
            <p id="Par110">The model trained with subset 4 was able to extract machine-readable representations of molecules depicted in the test dataset with near 100% accuracy. In order to understand why predictions with low Tanimoto scores were not predicted correctly, the following analysis was performed (Table <xref rid="Tab10" ref-type="table">10</xref>; Fig. <xref rid="Fig7" ref-type="fig">7</xref>).<table-wrap id="Tab10"><label>Table 10</label><caption xml:lang="en"><p>Predicted SMILES with lower Tanimoto similarity indices compared with the original SMILES</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>No.</p></th><th align="left"><p>Original SMILES</p></th><th align="left"><p>Predicted SMILES</p></th><th align="left"><p>Tanimoto similarity index</p></th></tr></thead><tbody><tr><td align="left"><p>1</p></td><td align="left"><p>P#CP=PP=PP=PP=PP=PP=PP=PP=PP=PP=PP=P</p></td><td align="left"><p>N#CC=NSSSSSSSSSSSSSSSSC=N</p></td><td align="left"><p>0</p></td></tr><tr><td align="left"><p>2</p></td><td align="left"><p>N1=NOO1</p></td><td align="left"><p>C=1=NOC1</p></td><td align="left"><p>0.14</p></td></tr><tr><td align="left"><p>3</p></td><td align="left"><p>OC1OC(C=2C(F)=C(F)C(F)=C(F)C21)C)C</p></td><td align="left"><p>O=C(OCOCCC(F)=C(F)C(F)=C(F)C)NC</p></td><td align="left"><p>0.35</p></td></tr><tr><td align="left"><p>4</p></td><td align="left"><p>OCC(C)(CO)C12CCC(C1)C3SSSC32</p></td><td align="left"><p>OCC(C)(CO)C1C=2SSSC2CCC1C</p></td><td align="left"><p>0.59</p></td></tr><tr><td align="left"><p>5</p></td><td align="left"><p>O=C1N=CC2=CC(=O)C=CC2=N1</p></td><td align="left"><p>O=C1N=CC2=NC(=O)C=CC2=N1</p></td><td align="left"><p>0.81</p></td></tr></tbody></table></table-wrap><fig id="Fig7"><label>Fig. 7</label><caption xml:lang="en"><p>Depictions of chemical structures with lower Tanimoto similarity indices</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig7_HTML.png" id="MO8" /></fig></p>
            <p id="Par111">In most cases the network was able to interpret the skeleton of the chemical structure well. Semantically small errors such as the miss of a ring closure will lead to seemingly large errors in the eyes of a chemist, as can be seen in case 3(Fig <xref rid="Fig7" ref-type="fig">7</xref>).</p>
            <p id="Par112">In the majority of cases, the Tanimoto similarity was low due to the predicted SMILES,<list list-type="bullet"><list-item><p id="Par113">having one or more wrong atoms.</p></list-item><list-item><p id="Par114">missing a bond.</p></list-item><list-item><p id="Par115">having a wrong bond.</p></list-item><list-item><p id="Par116">missing an aromatic ring.</p></list-item></list></p>
            <p id="Par117">A strategy to overcome such issues could be to use multiple depictions of the same chemical structure in the training set with different rotations so that the network sees more examples of the same set of input data. Also implementing different and more image augmentation methods and training the augmented images along with the non augmented images might enable the network to see the chemical structures clearer.</p>
          </sec>
          <sec id="Sec19">
            <title>Performance of the network with training data using stereochemistry information—Dataset 2</title>
            <p id="Par118">To assess the performance of the transformer network on chemical structure depictions with stereochemistry and ions, the same dataset was used but stereochemistry and ion information were included. By including this information the unique number of tokens increased, and the molecules with the least number of tokens were removed after the calculation of the token distribution. A new dataset with 37 Mio molecules was created and split into training and test datasets using the RDKit MaxMin algorithm. This whole dataset is called Dataset-2 from now on.</p>
            <p id="Par119">By adding stereochemical information and ions, the number of unique SELFIES tokens increased from 27 to 61, almost twice the number of the tokens found on Dataset 1. From Table <xref rid="Tab11" ref-type="table">11</xref> one could see the same molecule with and without stereochemistry and how it affects the number of tokens present in the SELFIES and the depicted structure.<table-wrap id="Tab11"><label>Table 11</label><caption xml:lang="en"><p>Analysis of a molecule for with and without stereochemical information</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" /><th align="left" /><th align="left"><p>Molecules with stereochemical information</p></th><th align="left"><p>Molecules without stereochemical information</p></th></tr></thead><tbody><tr><td align="left" rowspan="4"><p>01</p></td><td align="left"><p>SMILES (canonical/isomric)</p></td><td align="left"><p>C1=CC2=C(C=C1C=O)C(C(O2)Br)Br</p></td><td align="left"><p>C1=CC2=C(C=C1C=O)[C@@H]([C@H](O2)Br)Br</p></td></tr><tr><td align="left"><p>SELFIES</p></td><td align="left"><p>[C][=C][C][=C][Branch1_1][Branch1_3][C][=C][Ring1][Branch1_2][C][=O][C][Branch1_1][Branch2_1][C][Branch1_1][Ring2][O][Ring1][Branch2_2][Br][Br]</p></td><td align="left"><p>[C][=C][C][=C][Branch1_1][Branch1_3][C][=C][Ring1][Branch1_2][C][=O][C@@Hexpl][Branch1_1][Branch2_1][C@Hexpl][Branch1_1][Ring2][O][Ring1][Branch2_2][Br][Br]</p></td></tr><tr><td align="left"><p>Number of unique SELFIES tokens</p></td><td align="left"><p>12</p></td><td align="left"><p>14</p></td></tr><tr><td align="left"><p>Depicted structure</p></td><td align="left"><p><inline-graphic xlink:href="MediaObjects/13321_2021_538_Figb_HTML.gif" /></p></td><td align="left"><p><inline-graphic xlink:href="MediaObjects/13321_2021_538_Figc_HTML.gif" /></p></td></tr><tr><td align="left"><p>02</p></td><td align="left"><p>SMILES (canonical/isomeric)</p></td><td align="left"><p>CC1C(=C(N(N1)C)OC2CCC=CC2)C=NO</p></td><td align="left"><p>CC1C(=C(N(N1)C)OC2CCC=CC2)/C=N/O</p></td></tr><tr><td align="left" /><td align="left"><p>SELFIES</p></td><td align="left"><p>[C][C][C][Branch2_2][Ring1][Ring2][=C][Branch1_1][Branch2_1][N][Branch1_1][Ring2][N][Ring1][Branch1_1][C][O][C][C][C][C][=C][C][Ring1][Branch1_2][C][=N][O]</p></td><td align="left"><p>[C][C][C][Branch2_2][Ring1][Ring2][=C][Branch1_1][Branch2_1][N][Branch1_1][Ring2][N][Ring1][Branch1_1][C][O][C][C][C][C][=C][C][Ring1][Branch1_2][/C][=N][/O]</p></td></tr><tr><td align="left" /><td align="left"><p>Number of unique SELFIES tokens</p></td><td align="left"><p>11</p></td><td align="left"><p>13</p></td></tr><tr><td align="left" /><td align="left"><p>Depicted Structure</p></td><td align="left"><p><inline-graphic xlink:href="MediaObjects/13321_2021_538_Figd_HTML.gif" /></p></td><td align="left"><p><inline-graphic xlink:href="MediaObjects/13321_2021_538_Fige_HTML.gif" /></p></td></tr></tbody></table></table-wrap></p>
            <p id="Par120">Inclusion of stereochemistry increased the amount of tokens, but also introduced new artifacts in chemical structure depictions such as wedged and dashed bonds. Including the cis/trans information reduced the amount of curly bonds in the new dataset.</p>
            <p id="Par121">Including the information about the ions also increased the number of tokens, also this introduced new artifacts to the chemical structure depictions such as the “+, −” signs and arrows, see Fig. <xref rid="Fig8" ref-type="fig">8</xref>.<fig id="Fig8"><label>Fig. 8</label><caption xml:lang="en"><p>Chemical structure depictions with ions</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig8_HTML.png" id="MO9" /></fig></p>
            <p id="Par122">Two subsets of Data Set 2 were generated, one with the 15 million training molecules plus 1.5 million test molecules and another with 33 million training molecules plus 3.7 million test molecules. TFRecords were generated from the chemical structure depictions using these datasets and moved into Google cloud storage buckets. Finally, two models were trained using these two datasets. Table <xref rid="Tab12" ref-type="table">12</xref> summarizes the results.<table-wrap id="Tab12"><label>Table 12</label><caption xml:lang="en"><p>Results on the subsets of dataset 2</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Metrics</p></th><th align="left"><p>Subset 5</p></th><th align="left"><p>Subset 6</p></th></tr></thead><tbody><tr><td align="left"><p>Train data size</p></td><td align="left"><p>15,360,000</p></td><td align="left"><p>33,304,320</p></td></tr><tr><td align="left"><p>Test data size</p></td><td align="left"><p>1,536,000</p></td><td align="left"><p>3,700,480</p></td></tr><tr><td align="left"><p>Tanimoto</p></td><td align="left"><p>0.9372</p></td><td align="left"><p>0.9761</p></td></tr><tr><td align="left"><p>Tanimoto 1.0</p></td><td align="left"><p>75.23%</p></td><td align="left"><p>89.87%</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par123">It can be seen from the results shown in Table <xref rid="Tab12" ref-type="table">12</xref> that the average Tanimoto is lower compared to dataset 1 that was seen in Table <xref rid="Tab8" ref-type="table">8</xref>. The Tanimoto 1.0 count is also lower. This is mainly due to the new artefacts included in the new dataset and now the number of tokens in use also doubled. Increasing the data for the newly introduced tokens can improve the results significantly. To check how many of the predicted structures are isomorphic the InChIs were generated for the original and predicted structures and a string matching was performed as explained before, see Table <xref rid="Tab13" ref-type="table">13</xref>.<table-wrap id="Tab13"><label>Table 13</label><caption xml:lang="en"><p>Results of isomorphism calculations for the subsets of dataset 2</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Metrics</p></th><th align="left"><p>Subset 5</p></th><th align="left"><p>Subset 6</p></th></tr></thead><tbody><tr><td align="left"><p>Train data size</p></td><td align="left"><p>15,360,000</p></td><td align="left"><p>33,304,320</p></td></tr><tr><td align="left"><p>Test data size</p></td><td align="left"><p>1,536,000</p></td><td align="left"><p>3,700,480</p></td></tr><tr><td align="left"><p>Predictions with Tanimoto 1.0</p></td><td align="left"><p>1,155,483</p></td><td align="left"><p>3,325,656</p></td></tr><tr><td align="left"><p>Isomorphic predictions</p></td><td align="left"><p>96.42%</p></td><td align="left"><p>98.50%</p></td></tr><tr><td align="left"><p>Non isomorphic predictions</p></td><td align="left"><p>3.58%</p></td><td align="left"><p>1.50%</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par124">Table <xref rid="Tab13" ref-type="table">13</xref> shows that more than 96% of the predicted SMILES are isomorphic for a training data set of 15 Mio compounds. By approximately doubling the training dataset size, the number of isomorphic structures increased to over 98%, which is similar to the results for dataset 1. We also analysed the non-isomorphic predictions to see why they had a Tanimoto similarity of 1.0. Figure <xref rid="Fig9" ref-type="fig">9</xref> demonstrates that the non-isomorphic structures are identical to the original structures, except for an incorrectly predicted stereochemistry. This can very likely be improved by training the network with more molecules with stereochemistry.<fig id="Fig9"><label>Fig. 9</label><caption xml:lang="en"><p>Non-isomorphic structures with Tanimoto similarity index of 1.0. The constitution is the same but predicted stereo-chemistry differs</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig9_HTML.png" id="MO10" /></fig></p>
            <p id="Par125">Increasing the training data points will likely increase isomorphic structure predictions in general. Due to the applied ruleset, only a limited amount of data is available to work with. Therefore, the next step will be to train these models on augmented images to assess whether or not they improve overall accuracy.</p>
          </sec>
          <sec id="Sec20">
            <title>Performance of the network with training data using stereo-chemistry and image augmentation—Dataset 3</title>
            <p id="Par126">By applying image augmentation to dataset 2 we generated dataset 3. The resulting images look similar to Fig. <xref rid="Fig10" ref-type="fig">10</xref>.<fig id="Fig10"><label>Fig. 10</label><caption xml:lang="en"><p>Images augmented with parameters within a given range</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2021_538_Fig10_HTML.png" id="MO11" /></fig></p>
            <p id="Par127">The parameters were restricted to reflect the real world images, not to add extreme augmentations. The parameter is shown in Table <xref rid="Tab14" ref-type="table">14</xref> during augmentations. Here the list of parameters provided is the ones that were implemented to augment the images, for more details about the parameters and how they are implemented, we refer our readers to the imgaug documentation [<xref ref-type="bibr" rid="CR41">41</xref>].<table-wrap id="Tab14"><label>Table 14</label><caption xml:lang="en"><p>Image augmentations and their parameters</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Image augmentations</p></th><th align="left"><p>Parameters (imgaug)</p></th></tr></thead><tbody><tr><td align="left"><p>Gaussian blur</p></td><td align="left"><p>0–1.8</p></td></tr><tr><td align="left"><p>Average blur</p></td><td align="left"><p>k = 0–3</p></td></tr><tr><td align="left"><p>Additive Gaussian noise</p></td><td align="left"><p>Scale = (0, 0.1 * 255)</p></td></tr><tr><td align="left"><p>Salt and pepper</p></td><td align="left"><p>0–0.05</p></td></tr><tr><td align="left"><p>Salt</p></td><td align="left"><p>0–0.05</p></td></tr><tr><td align="left"><p>Pepper</p></td><td align="left"><p>0–0.05</p></td></tr><tr><td align="left"><p>Coarse dropout</p></td><td align="left"><p>0–0.01, size percent = 0.9</p></td></tr><tr><td align="left"><p>Gamma contrast</p></td><td align="left"><p>0.5–2.0</p></td></tr><tr><td align="left"><p>Sharpen</p></td><td align="left"><p>Alpha = (0.0, 1.0), lightness = 1.0</p></td></tr><tr><td align="left"><p>Enhance brightness</p></td><td align="left"><p>Factor = (0.95, 1.5)</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par128">The generated dataset was then used to train two models. One model was trained from scratch using augmented images. Another model previously trained on Dataset 2 was used as the pre-trained model and then refitted with the augmented images (see Table <xref rid="Tab15" ref-type="table">15</xref>). Both of them were tested on a dataset size of 4 million images, which includes 2 million images with augmentations and 2 million images without any augmentations. Table <xref rid="Tab15" ref-type="table">15</xref> summarizes the results.<table-wrap id="Tab15"><label>Table 15</label><caption xml:lang="en"><p>Results on dataset 3 and dataset 2 + 3</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2"><p>Metrics</p></th><th align="left" colspan="2"><p>Augmented dataset (3)</p></th><th align="left" colspan="2"><p>Pre-trained model + augmented dataset (2 + 3)</p></th></tr><tr><th align="left"><p>Non augmented test set</p></th><th align="left"><p>Augmented test set</p></th><th align="left"><p>Non augmented test set</p></th><th align="left"><p>Augmented test set</p></th></tr></thead><tbody><tr><td align="left"><p>Train data size</p></td><td align="left"><p>33,304,320</p></td><td align="left"><p>33,304,320</p></td><td align="left"><p>33,304,320</p></td><td align="left"><p>33,304,320</p></td></tr><tr><td align="left"><p>Test data size</p></td><td align="left"><p>2,000,000</p></td><td align="left"><p>2,000,000</p></td><td align="left"><p>2,000,000</p></td><td align="left"><p>2,000,000</p></td></tr><tr><td align="left"><p>Tanimoto</p></td><td align="left"><p>0.9663</p></td><td align="left"><p>0.9501</p></td><td align="left"><p>0.9708</p></td><td align="left"><p>0.9521</p></td></tr><tr><td align="left"><p>Tanimoto 1.0</p></td><td align="left"><p>86.43%</p></td><td align="left"><p>80.26%</p></td><td align="left"><p>88.04%</p></td><td align="left"><p>80.87%</p></td></tr><tr><td align="left"><p>Isomorphic Predictions</p></td><td align="left"><p>97.89%</p></td><td align="left"><p>97.46%</p></td><td align="left"><p>98.15%</p></td><td align="left"><p>97.61%</p></td></tr><tr><td align="left"><p>Non isomorphic predictions</p></td><td align="left"><p>2.11%</p></td><td align="left"><p>2.54%</p></td><td align="left"><p>1.85%</p></td><td align="left"><p>2.39%</p></td></tr></tbody></table></table-wrap></p>
            <p id="Par129">The first two columns of the table explain the performance of the model trained only on augmented images and tested on augmented and non-augmented images. The last two columns summarize the evaluation of the model which was previously trained on non-augmented images and refitted with dataset 2.</p>
            <p id="Par130">In refitting, we used weights from the best model previously trained on non-augmented images instead of random weights as a starting point for training. This was done to see whether using the weights from a previously trained model would improve the performance of the newly trained model trained using a similar type of data.</p>
            <p id="Par131">The above results clearly show that our models were able to retain the Tanimoto average of above 0.95 and Tanimoto 1.0 of above 80%. Also, the isomorphic results are high in all cases, and this was similar to the earlier results. The overall accuracy of these models could be improved by increasing the number of Augmented and Non-Augmented training images.</p>
            <p id="Par132">Very likely, training with more data will improve the outcome.</p>
          </sec>
        </sec>
        <sec id="Sec21">
          <title>Conclusion and future work</title>
          <p id="Par133">In our preliminary communication [<xref ref-type="bibr" rid="CR18">18</xref>], we claimed that with data around 50–100 million molecule images will help us obtain a model that can predict SMILES with about 90% accuracy. Here, we have now presented a solution based on a transformer network that delivers this promise.</p>
          <p id="Par134">Using the improved EfficientNet-B3 method rather than Inception-V3 for image feature extraction helped in extracting relevant features required for network training. Through the implementation of the new transformer-based models, we've been able to improve the accuracy of our Image-to-SMILES models overall.</p>
          <p id="Par135">We have achieved an accuracy level of about 96% for chemical structure depictions using DECIMER’s new algorithm without stereochemistry training the network using 30–35 million molecules.</p>
          <p id="Par136">When the models were extended to include stereochemical information and ions, a near 90% accuracy was achieved, despite increasing the number of tokens twofold. This can be further improved by increasing the data on stereochemical information and ions. This also applies to the models trained using image augmentations. In order to improve these models, more data should be incorporated into training.</p>
          <p id="Par137">With TPUs, the models could be trained within days, and the largest model took less than 14 days to train. That means even bigger models could be trained within a month using TPUs rather than training on GPUs, which may take several months to complete. It is also cost-effective as well as energy-efficient to implement the TPU solution on the Google cloud platform rather than relying on the local hardware setup.</p>
          <p id="Par138">Our results showed that DECIMER was achieving the intended objective with synthetic data. Further steps in future will include training with more data, refining models using a variety of real-world examples and image datasets with more augmentations. Additionally, training images created by using a variety of tools will contribute to the model's improved accuracy. Ultimately, the DECIMER project aims to provide an open-source tool that is capable of performing optical chemical structure recognition (OCSR) reliably on segmented images from the scanned literature.</p>
          <p id="Par139">The DECIMER software is fully open-source and hosted on GitHub. All data and trained models are openly available.</p>
        </sec>
      </body>
      <back>
        <ack>
          <title>Acknowledgements</title>
          <p>We are grateful for the company Google making free computing time on their TensorFlow Research Cloud infrastructure available to us. We thank Dr. Charles Tapley Hoyt for his valuable advice and help in improving the DECIMER GitHub repository.
</p>
        </ack>
        <sec sec-type="author-contribution">
          <title>Authors’ contributions</title>
          <p>KR developed the software and performed the data analysis. CS and AZ conceived the project and supervised the work. All authors contributed to the manuscript. All authors read and approved the final manuscript.</p>
        </sec>
        <sec>
          <title>Funding</title>
          <p>Open Access funding enabled and organized by Projekt DEAL. The authors acknowledge funding by the Carl-Zeiss-Foundation. Open Access funding enabled and organized by Projekt DEAL.</p>
        </sec>
        <sec sec-type="data-availability">
          <title>Availability of data and materials</title>
          <p>The code for DECIMER and the trained models are available at <ext-link xlink:href="https://github.com/Kohulan/DECIMER-TPU" ext-link-type="uri">https://github.com/Kohulan/DECIMER-TPU</ext-link>, <ext-link xlink:href="10.5281/zenodo.4730515" ext-link-type="doi">https://doi.org/10.5281/zenodo.4730515</ext-link>. The data is available as SMILES at: <ext-link xlink:href="10.5281/zenodo.4766251" ext-link-type="doi">https://doi.org/10.5281/zenodo.4766251</ext-link>.</p>
        </sec>
        <sec sec-type="ethics-statement">
          <title>Declarations</title>
          <sec id="FPar1" sec-type="COI-statement">
            <title>Competing interests</title>
            <p id="Par140">AZ is co-founder of GNWI—Gesellschaft für naturwissenschaftliche Informatik mbH, Dortmund, Germany.</p>
          </sec>
        </sec>
        <ref-list id="Bib1">
          <title>References</title>
          <ref-list>
            <ref id="CR1">
              <label>1.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Gaulton</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Overington</surname>
                    <given-names>JP</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Role of open chemical data in aiding drug discovery and design</article-title>
                <source>Future Med Chem</source>
                <year>2010</year>
                <volume>2</volume>
                <fpage>903</fpage>
                <lpage>907</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3cXpsFaksL0%3D</pub-id>
                <pub-id pub-id-type="doi">10.4155/fmc.10.191</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR2">
              <label>2.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>O’Boyle</surname>
                    <given-names>NM</given-names>
                  </name>
                  <name>
                    <surname>Guha</surname>
                    <given-names>R</given-names>
                  </name>
                  <name>
                    <surname>Willighagen</surname>
                    <given-names>EL</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Open data, open source and open standards in chemistry: the blue obelisk five years on</article-title>
                <source>J Cheminform</source>
                <year>2011</year>
                <volume>3</volume>
                <fpage>1</fpage>
                <lpage>15</lpage>
                <pub-id pub-id-type="doi">10.1186/1758-2946-3-1</pub-id>
                <comment>[cito:cites] [cito:agreesWith]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR3">
              <label>3.</label>
              <mixed-citation publication-type="book">
                <person-group person-group-type="author">
                  <name>
                    <surname>Zhou</surname>
                    <given-names>JZ</given-names>
                  </name>
                </person-group>
                <person-group person-group-type="editor">
                  <name>
                    <surname>Zhou</surname>
                    <given-names>JZ</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Chemoinformatics and library design</article-title>
                <source>Chemical library design</source>
                <year>2011</year>
                <publisher-loc>Totowa</publisher-loc>
                <publisher-name>Humana Press</publisher-name>
                <fpage>27</fpage>
                <lpage>52</lpage>
                <pub-id pub-id-type="doi">10.1007/978-1-60761-931-4_2</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR4">
              <label>4.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Swain</surname>
                    <given-names>MC</given-names>
                  </name>
                  <name>
                    <surname>Cole</surname>
                    <given-names>JM</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">ChemDataExtractor: a toolkit for automated extraction of chemical information from the scientific literature</article-title>
                <source>J Chem Inf Model</source>
                <year>2016</year>
                <volume>56</volume>
                <fpage>1894</fpage>
                <lpage>1904</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC28XhsFKjsr%2FK</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.6b00207</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR5">
              <label>5.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Krallinger</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Rabal</surname>
                    <given-names>O</given-names>
                  </name>
                  <name>
                    <surname>Lourenço</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Oyarzabal</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Valencia</surname>
                    <given-names>A</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Information retrieval and text mining technologies for chemistry</article-title>
                <source>Chem Rev</source>
                <year>2017</year>
                <volume>117</volume>
                <fpage>7673</fpage>
                <lpage>7761</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXntVyru78%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.chemrev.6b00851</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR6">
              <label>6.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Krallinger</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Leitner</surname>
                    <given-names>F</given-names>
                  </name>
                  <name>
                    <surname>Rabal</surname>
                    <given-names>O</given-names>
                  </name>
                  <name>
                    <surname>Vazquez</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Oyarzabal</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Valencia</surname>
                    <given-names>A</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">CHEMDNER: the drugs and chemical names extraction challenge</article-title>
                <source>J Cheminform</source>
                <year>2015</year>
                <volume>7</volume>
                <fpage>S1</fpage>
                <pub-id pub-id-type="doi">10.1186/1758-2946-7-S1-S1</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR7">
              <label>7.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Beard</surname>
                    <given-names>EJ</given-names>
                  </name>
                  <name>
                    <surname>Cole</surname>
                    <given-names>JM</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">ChemSchematicResolver: a toolkit to decode 2D chemical diagrams with labels and R-groups into annotated chemical named entities</article-title>
                <source>J Chem Inf Model</source>
                <year>2020</year>
                <volume>60</volume>
                <fpage>2059</fpage>
                <lpage>2072</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXls12htrY%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.0c00042</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR8">
              <label>8.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Rajan</surname>
                    <given-names>K</given-names>
                  </name>
                  <name>
                    <surname>Brinkhaus</surname>
                    <given-names>HO</given-names>
                  </name>
                  <name>
                    <surname>Zielesny</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Steinbeck</surname>
                    <given-names>C</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">A review of optical chemical structure recognition tools</article-title>
                <source>J Cheminform</source>
                <year>2020</year>
                <volume>12</volume>
                <fpage>60</fpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitlWks7fF</pub-id>
                <pub-id pub-id-type="doi">10.1186/s13321-020-00465-0</pub-id>
                <comment>[cito:cites] [cito:agreesWith] [cito:citesAsAuthority]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR9">
              <label>9.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Weininger</surname>
                    <given-names>D</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">SMILES, a chemical language and information system. 1. Introduction to methodology and encoding rules</article-title>
                <source>J Chem Inf Comput Sci</source>
                <year>1988</year>
                <volume>28</volume>
                <fpage>31</fpage>
                <lpage>36</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaL1cXnsVeqsA%3D%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR10">
              <label>10.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Heller</surname>
                    <given-names>SR</given-names>
                  </name>
                  <name>
                    <surname>McNaught</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Pletnev</surname>
                    <given-names>I</given-names>
                  </name>
                  <name>
                    <surname>Stein</surname>
                    <given-names>S</given-names>
                  </name>
                  <name>
                    <surname>Tchekhovskoi</surname>
                    <given-names>D</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">InChI, the IUPAC international chemical identifier</article-title>
                <source>J Cheminform</source>
                <year>2015</year>
                <volume>7</volume>
                <fpage>23</fpage>
                <pub-id pub-id-type="doi">10.1186/s13321-015-0068-4</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR11">
              <label>11.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Filippov</surname>
                    <given-names>IV</given-names>
                  </name>
                  <name>
                    <surname>Nicklaus</surname>
                    <given-names>MC</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Optical structure recognition software to recover chemical information: OSRA, an open source solution</article-title>
                <source>J Chem Inf Model</source>
                <year>2009</year>
                <volume>49</volume>
                <fpage>740</fpage>
                <lpage>743</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1MXhvFGnt7k%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci800067r</pub-id>
                <comment>[cito:cites] [cito:citesAsAuthority]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR12">
              <label>12.</label>
              <mixed-citation publication-type="other">Peryea T, Katzel D, Zhao T, Southall N, Nguyen D-T (2019) MOLVEC: Open source library for chemical structure recognition. In: Abstracts of papers of the American Chemical Society, vol 258 <bold>[cito:cites] [cito:citesAsAuthority]</bold></mixed-citation>
            </ref>
            <ref id="CR13">
              <label>13.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Staker</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Marshall</surname>
                    <given-names>K</given-names>
                  </name>
                  <name>
                    <surname>Abel</surname>
                    <given-names>R</given-names>
                  </name>
                  <name>
                    <surname>McQuaw</surname>
                    <given-names>CM</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Molecular structure extraction from documents using deep learning</article-title>
                <source>J Chem Inf Model</source>
                <year>2019</year>
                <volume>59</volume>
                <fpage>1017</fpage>
                <lpage>1029</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXivVOgtbY%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00669</pub-id>
                <comment>[cito:cites] [cito:citesAsAuthority]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR14">
              <label>14.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Oldenhof</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Arany</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Moreau</surname>
                    <given-names>Y</given-names>
                  </name>
                  <name>
                    <surname>Simm</surname>
                    <given-names>J</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">ChemGrapher: optical graph recognition of chemical compounds by deep learning</article-title>
                <source>J Chem Inf Model</source>
                <year>2020</year>
                <volume>60</volume>
                <fpage>4506</fpage>
                <lpage>4517</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhvVertLzE</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.0c00459</pub-id>
                <comment>[cito:cites] [cito:citesAsAuthority]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR15">
              <label>15.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Weir</surname>
                    <given-names>H</given-names>
                  </name>
                  <name>
                    <surname>Thompson</surname>
                    <given-names>K</given-names>
                  </name>
                  <name>
                    <surname>Choi</surname>
                    <given-names>B</given-names>
                  </name>
                  <name>
                    <surname>Woodward</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Braun</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Martínez</surname>
                    <given-names>TJ</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">ChemPix: automated recognition of hand-drawn hydrocarbon structures using deep learning</article-title>
                <source>ChemRxiv</source>
                <year>2021</year>
                <pub-id pub-id-type="doi">10.26434/chemrxiv.14156957.v1</pub-id>
                <comment>[cito:cites] [cito:citesAsAuthority]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR16">
              <label>16.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Clevert</surname>
                    <given-names>D-A</given-names>
                  </name>
                  <name>
                    <surname>Le</surname>
                    <given-names>T</given-names>
                  </name>
                  <name>
                    <surname>Winter</surname>
                    <given-names>R</given-names>
                  </name>
                  <name>
                    <surname>Montanari</surname>
                    <given-names>F</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Img2Mol—accurate SMILES recognition from molecular graphical depictions</article-title>
                <source>ChemRxiv</source>
                <year>2021</year>
                <pub-id pub-id-type="doi">10.26434/chemrxiv.14320907.v1</pub-id>
                <comment>[cito:cites] [cito:citesAsAuthority]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR17">
              <label>17.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Le</surname>
                    <given-names>T</given-names>
                  </name>
                  <name>
                    <surname>Winter</surname>
                    <given-names>R</given-names>
                  </name>
                  <name>
                    <surname>Noé</surname>
                    <given-names>F</given-names>
                  </name>
                  <name>
                    <surname>Clevert</surname>
                    <given-names>D-A</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Neuraldecipher—reverse-engineering extended-connectivity fingerprints (ECFPs) to their molecular structures</article-title>
                <source>Chem Sci</source>
                <year>2020</year>
                <volume>11</volume>
                <fpage>10378</fpage>
                <lpage>10389</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXhvVWmurrI</pub-id>
                <pub-id pub-id-type="doi">10.1039/D0SC03115A</pub-id>
                <comment>[cito:cites] [cito:citesAsAuthority]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR18">
              <label>18.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Rajan</surname>
                    <given-names>K</given-names>
                  </name>
                  <name>
                    <surname>Zielesny</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Steinbeck</surname>
                    <given-names>C</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">DECIMER: towards deep learning for chemical image recognition</article-title>
                <source>J Cheminform</source>
                <year>2020</year>
                <volume>12</volume>
                <fpage>65</fpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitlWksbzJ</pub-id>
                <pub-id pub-id-type="doi">10.1186/s13321-020-00469-w</pub-id>
                <comment>[cito:usesMethodIn] [cito:citesAsAuthority] [cito:extends]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR19">
              <label>19.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Rajan</surname>
                    <given-names>K</given-names>
                  </name>
                  <name>
                    <surname>Brinkhaus</surname>
                    <given-names>HO</given-names>
                  </name>
                  <name>
                    <surname>Sorokina</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Zielesny</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Steinbeck</surname>
                    <given-names>C</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">DECIMER-segmentation: automated extraction of chemical structure depictions from scientific literature</article-title>
                <source>J Cheminform</source>
                <year>2021</year>
                <volume>13</volume>
                <fpage>1</fpage>
                <lpage>9</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3MXhs1GktLbI</pub-id>
                <pub-id pub-id-type="doi">10.1186/s13321-021-00496-1</pub-id>
                <comment>[cito:cites] [cito:extends] [cito:citesAsAuthority]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR20">
              <label>20.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Silver</surname>
                    <given-names>D</given-names>
                  </name>
                  <name>
                    <surname>Schrittwieser</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Simonyan</surname>
                    <given-names>K</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Mastering the game of Go without human knowledge</article-title>
                <source>Nature</source>
                <year>2017</year>
                <volume>550</volume>
                <fpage>354</fpage>
                <lpage>359</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhs12ltLvM</pub-id>
                <pub-id pub-id-type="doi">10.1038/nature24270</pub-id>
                <comment>[cito:cites] [cito:agreesWith]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR21">
              <label>21.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Kim</surname>
                    <given-names>S</given-names>
                  </name>
                  <name>
                    <surname>Chen</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Cheng</surname>
                    <given-names>T</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">PubChem 2019 update: improved access to chemical data</article-title>
                <source>Nucleic Acids Res</source>
                <year>2019</year>
                <volume>47</volume>
                <fpage>D1102</fpage>
                <lpage>D1109</lpage>
                <pub-id pub-id-type="doi">10.1093/nar/gky1033</pub-id>
                <comment>[cito:citesAsDataSource]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR22">
              <label>22.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Irwin</surname>
                    <given-names>JJ</given-names>
                  </name>
                  <name>
                    <surname>Tang</surname>
                    <given-names>KG</given-names>
                  </name>
                  <name>
                    <surname>Young</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Dandarchuluun</surname>
                    <given-names>C</given-names>
                  </name>
                  <name>
                    <surname>Wong</surname>
                    <given-names>BR</given-names>
                  </name>
                  <name>
                    <surname>Khurelbaatar</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Moroz</surname>
                    <given-names>YS</given-names>
                  </name>
                  <name>
                    <surname>Mayfield</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Sayle</surname>
                    <given-names>RA</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">ZINC20—a free ultralarge-scale chemical database for ligand discovery</article-title>
                <source>J Chem Inf Model</source>
                <year>2020</year>
                <volume>60</volume>
                <fpage>6065</fpage>
                <lpage>6073</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BB3cXitFygsbrF</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.0c00675</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR23">
              <label>23.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Ruddigkeit</surname>
                    <given-names>L</given-names>
                  </name>
                  <name>
                    <surname>van Deursen</surname>
                    <given-names>R</given-names>
                  </name>
                  <name>
                    <surname>Blum</surname>
                    <given-names>LC</given-names>
                  </name>
                  <name>
                    <surname>Reymond</surname>
                    <given-names>J-L</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17</article-title>
                <source>J Chem Inf Model</source>
                <year>2012</year>
                <volume>52</volume>
                <fpage>2864</fpage>
                <lpage>2875</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XhsFClsL3J</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci300415d</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR24">
              <label>24.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Steinbeck</surname>
                    <given-names>C</given-names>
                  </name>
                  <name>
                    <surname>Han</surname>
                    <given-names>Y</given-names>
                  </name>
                  <name>
                    <surname>Kuhn</surname>
                    <given-names>S</given-names>
                  </name>
                  <name>
                    <surname>Horlacher</surname>
                    <given-names>O</given-names>
                  </name>
                  <name>
                    <surname>Luttmann</surname>
                    <given-names>E</given-names>
                  </name>
                  <name>
                    <surname>Willighagen</surname>
                    <given-names>E</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">The chemistry development kit (CDK): an open-source Java library for Chemo- and bioinformatics</article-title>
                <source>J Chem Inf Comput Sci</source>
                <year>2003</year>
                <volume>43</volume>
                <fpage>493</fpage>
                <lpage>500</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD3sXhtVaktbg%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci025584y</pub-id>
                <comment>[cito:usesMethodIn]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR25">
              <label>25.</label>
              <mixed-citation publication-type="other">Jung AB, Wada K, Crall J et al (2020) Imgaug. GitHub: San Francisco, CA, USA <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR26">
              <label>26.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>O’Boyle</surname>
                    <given-names>N</given-names>
                  </name>
                  <name>
                    <surname>Dalke</surname>
                    <given-names>A</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">DeepSMILES: an adaptation of SMILES for use in machine-learning of chemical structures</article-title>
                <source>ChemRxiv</source>
                <year>2018</year>
                <pub-id pub-id-type="doi">10.26434/chemrxiv.7097960.v1</pub-id>
                <comment>[cito:usesMethodIn]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR27">
              <label>27.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Krenn</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Häse</surname>
                    <given-names>F</given-names>
                  </name>
                  <name>
                    <surname>Nigam</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Friederich</surname>
                    <given-names>P</given-names>
                  </name>
                  <name>
                    <surname>Aspuru-Guzik</surname>
                    <given-names>A</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Self-referencing embedded strings (SELFIES): a 100% robust molecular string representation</article-title>
                <source>Mach Learn Sci Technol</source>
                <year>2020</year>
                <volume>1</volume>
                <fpage>045024</fpage>
                <pub-id pub-id-type="doi">10.1088/2632-2153/aba947</pub-id>
                <comment>[cito:usesMethodIn]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR28">
              <label>28.</label>
              <mixed-citation publication-type="other">Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z (2016) Rethinking the inception architecture for computer vision. In: Proceedings of the IEEE conference on computer vision and pattern recognition. pp 2818–2826 <bold>[cito:cites]</bold></mixed-citation>
            </ref>
            <ref id="CR29">
              <label>29.</label>
              <mixed-citation publication-type="other">Tan M, Le Q (2019) Efficientnet: rethinking model scaling for convolutional neural networks. In: International conference on machine learning. PMLR, pp 6105–6114 <bold>[cito:cites]</bold></mixed-citation>
            </ref>
            <ref id="CR30">
              <label>30.</label>
              <mixed-citation publication-type="other">Deng J, Dong W, Socher R, Li L, Kai Li, Li Fei-Fei (2009) ImageNet: a large-scale hierarchical image database. In: 2009 IEEE conference on computer vision and pattern recognition. pp 248–255 <bold>[cito:cites]</bold></mixed-citation>
            </ref>
            <ref id="CR31">
              <label>31.</label>
              <mixed-citation publication-type="other">Xie Q, Luong M-T, Hovy E, Le QV (2020) Self-training with noisy student improves imagenet classification. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp 10687–10698 <bold>[cito:cites] [cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR32">
              <label>32.</label>
              <mixed-citation publication-type="other">Chollet F et al (2015) Keras. <ext-link xlink:href="https://keras.io" ext-link-type="uri">https://keras.io</ext-link>. <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR33">
              <label>33.</label>
              <mixed-citation publication-type="other">Abadi M, Agarwal A, Barham P et al (2015) TensorFlow: large-scale machine learning on heterogeneous systems. <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR34">
              <label>34.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Norrie</surname>
                    <given-names>T</given-names>
                  </name>
                  <name>
                    <surname>Patil</surname>
                    <given-names>N</given-names>
                  </name>
                  <name>
                    <surname>Yoon</surname>
                    <given-names>DH</given-names>
                  </name>
                  <name>
                    <surname>Kurian</surname>
                    <given-names>G</given-names>
                  </name>
                  <name>
                    <surname>Li</surname>
                    <given-names>S</given-names>
                  </name>
                  <name>
                    <surname>Laudon</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Young</surname>
                    <given-names>C</given-names>
                  </name>
                  <name>
                    <surname>Jouppi</surname>
                    <given-names>N</given-names>
                  </name>
                  <name>
                    <surname>Patterson</surname>
                    <given-names>D</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">The design process for Google’s training chips: TPUv2 and TPUv3</article-title>
                <source>IEEE Micro</source>
                <year>2021</year>
                <volume>41</volume>
                <fpage>56</fpage>
                <lpage>63</lpage>
                <pub-id pub-id-type="doi">10.1109/MM.2021.3058217</pub-id>
                <comment>[cito:cites]</comment>
              </mixed-citation>
            </ref>
            <ref id="CR35">
              <label>35.</label>
              <mixed-citation publication-type="other">Xu K, Ba J, Kiros R, Cho K, Courville A, Salakhudinov R, Zemel R, Bengio Y (2015) Show, attend and tell: neural image caption generation with visual attention. In: Bach F, Blei D (eds) Proceedings of the 32nd international conference on machine learning. PMLR, Lille, France, pp 2048–2057 <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR36">
              <label>36.</label>
              <mixed-citation publication-type="other">Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser L, Polosukhin I (2017) Attention is all you need. arXiv [cs.CL] <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR37">
              <label>37.</label>
              <mixed-citation publication-type="other">Image captioning with visual attention. <ext-link xlink:href="https://www.tensorflow.org/tutorials/text/image_captioning" ext-link-type="uri">https://www.tensorflow.org/tutorials/text/image_captioning</ext-link>. Accessed 17 Mar 2021 <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR38">
              <label>38.</label>
              <mixed-citation publication-type="other">Bahdanau D, Cho K, Bengio Y (2014) Neural machine translation by jointly learning to align and translate. arXiv [cs.CL] <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR39">
              <label>39.</label>
              <mixed-citation publication-type="other">Kingma DP, Ba J (2014) Adam: a method for stochastic optimization. arXiv [cs.LG] <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR40">
              <label>40.</label>
              <mixed-citation publication-type="other">Landrum G et al (2016) RDKit: open-source cheminformatics software (2016). <ext-link xlink:href="http://www.rdkit.org/" ext-link-type="uri">http://www.rdkit.org/</ext-link>, <ext-link xlink:href="https://github.com/rdkit/rdkit" ext-link-type="uri">https://github.com/rdkit/rdkit</ext-link><bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
            <ref id="CR41">
              <label>41.</label>
              <mixed-citation publication-type="other">dtype support—imgaug 0.4.0 documentation. <ext-link xlink:href="https://imgaug.readthedocs.io/en/latest/source/dtype_support.html" ext-link-type="uri">https://imgaug.readthedocs.io/en/latest/source/dtype_support.html</ext-link>. Accessed 15 Apr 2021 <bold>[cito:usesMethodIn]</bold></mixed-citation>
            </ref>
          </ref-list>
        </ref-list>
        <glossary>
          <title>Abbreviations</title>
          <def-list>
            <def-item>
              <term>API</term>
              <def>
                <p id="Par3">Application Programming Interface</p>
              </def>
            </def-item>
            <def-item>
              <term>CDK</term>
              <def>
                <p id="Par4">Chemistry Development Kit</p>
              </def>
            </def-item>
            <def-item>
              <term>CNN</term>
              <def>
                <p id="Par5">Convolutional Neural Network</p>
              </def>
            </def-item>
            <def-item>
              <term>CSR</term>
              <def>
                <p id="Par6">ChemSchematicResolver</p>
              </def>
            </def-item>
            <def-item>
              <term>FTP</term>
              <def>
                <p id="Par7">File Transfer Protocol</p>
              </def>
            </def-item>
            <def-item>
              <term>GB</term>
              <def>
                <p id="Par8">GigaByte</p>
              </def>
            </def-item>
            <def-item>
              <term>GPU</term>
              <def>
                <p id="Par9">Graphical Processing Unit</p>
              </def>
            </def-item>
            <def-item>
              <term>GRU</term>
              <def>
                <p id="Par10">Gated Recurrent Unit</p>
              </def>
            </def-item>
            <def-item>
              <term>InChI</term>
              <def>
                <p id="Par11">International Chemical Identifier</p>
              </def>
            </def-item>
            <def-item>
              <term>DECIMER</term>
              <def>
                <p id="Par12">Deep lEarning for Chemical ImagE Recognition</p>
              </def>
            </def-item>
            <def-item>
              <term>MB</term>
              <def>
                <p id="Par13">MegaByte</p>
              </def>
            </def-item>
            <def-item>
              <term>NNs</term>
              <def>
                <p id="Par14">Neural Networks</p>
              </def>
            </def-item>
            <def-item>
              <term>OCSR</term>
              <def>
                <p id="Par15">Optical Chemical Structure Recognition</p>
              </def>
            </def-item>
            <def-item>
              <term>OSRA</term>
              <def>
                <p id="Par16">Optical Structure Recognition Application</p>
              </def>
            </def-item>
            <def-item>
              <term>PDF</term>
              <def>
                <p id="Par17">Portable Document Format</p>
              </def>
            </def-item>
            <def-item>
              <term>PNG</term>
              <def>
                <p id="Par18">Portable Network Graphics</p>
              </def>
            </def-item>
            <def-item>
              <term>RNN</term>
              <def>
                <p id="Par19">Recurrent Neural Network</p>
              </def>
            </def-item>
            <def-item>
              <term>SDG</term>
              <def>
                <p id="Par20">Structure Diagram Generator</p>
              </def>
            </def-item>
            <def-item>
              <term>SELFIES</term>
              <def>
                <p id="Par21">Self-referencing embedded strings</p>
              </def>
            </def-item>
            <def-item>
              <term>SMILES</term>
              <def>
                <p id="Par22">Simplified Molecular-Input Line-Entry System</p>
              </def>
            </def-item>
            <def-item>
              <term>TFRecord</term>
              <def>
                <p id="Par23">TensorFlow Record</p>
              </def>
            </def-item>
            <def-item>
              <term>TPU</term>
              <def>
                <p id="Par24">Tensor Processing Unit</p>
              </def>
            </def-item>
          </def-list>
        </glossary>
        <notes notes-type="Misc">
          <title>Publisher's Note</title>
          <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
        </notes>
      </back>
    </article>
