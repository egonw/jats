<?xml version="1.0"?>
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="research-article" xml:lang="en">
      <front>
        <journal-meta>
          <journal-id journal-id-type="publisher-id">13321</journal-id>
          <journal-title-group>
            <journal-title>Journal of Cheminformatics</journal-title>
            <abbrev-journal-title abbrev-type="publisher">J Cheminform</abbrev-journal-title>
          </journal-title-group>
          <issn pub-type="epub">1758-2946</issn>
          <publisher>
            <publisher-name>Springer International Publishing</publisher-name>
            <publisher-loc>Cham</publisher-loc>
          </publisher>
        </journal-meta>
        <article-meta>
          <article-id pub-id-type="publisher-id">BMC1758-2946-3-45</article-id>
          <article-id pub-id-type="manuscript">227</article-id>
          <article-id pub-id-type="doi">10.1186/1758-2946-3-45</article-id>
          <article-categories>
            <subj-group subj-group-type="heading">
              <subject>Research Article</subject>
            </subj-group>
            <subj-group subj-group-type="article-collection" specific-use="Regular">
              <subject>Visions of a semantic molecular future</subject>
            </subj-group>
          </article-categories>
          <title-group>
            <article-title xml:lang="en">Ami - The chemist's amanuensis</article-title>
          </title-group>
          <contrib-group>
            <contrib contrib-type="author">
              <name>
                <surname>Brooks</surname>
                <given-names>Brian J</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author">
              <name>
                <surname>Thorn</surname>
                <given-names>Adam L</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author">
              <name>
                <surname>Smith</surname>
                <given-names>Matthew</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author">
              <name>
                <surname>Matthews</surname>
                <given-names>Peter</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author">
              <name>
                <surname>Chen</surname>
                <given-names>Shaoming</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author">
              <name>
                <surname>O'Steen</surname>
                <given-names>Ben</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author">
              <name>
                <surname>Adams</surname>
                <given-names>Sam E</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author">
              <name>
                <surname>Townsend</surname>
                <given-names>Joe A</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author" corresp="yes">
              <name>
                <surname>Murray-Rust</surname>
                <given-names>Peter</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
              <xref ref-type="corresp" rid="IDBMC17582946345_cor9">j</xref>
            </contrib>
            <aff id="Aff1">
              <label>1</label>
              <institution-wrap>
                <institution-id institution-id-type="GRID">grid.5335.0</institution-id>
                <institution-id institution-id-type="ISNI">0000000121885934</institution-id>
                <institution content-type="org-division">Unilever Centre for Molecular Science Informatics, Department of Chemistry</institution>
                <institution content-type="org-name">University of Cambridge</institution>
              </institution-wrap>
              <addr-line content-type="street">Lensfield Road</addr-line>
              <addr-line content-type="postcode">CB2 1EW</addr-line>
              <addr-line content-type="city">Cambridge</addr-line>
              <country country="GB">UK</country>
            </aff>
          </contrib-group>
          <author-notes>
            <corresp id="IDBMC17582946345_cor9">
              <label>j</label>
              <email>HIDDEN</email>
            </corresp>
          </author-notes>
          <pub-date date-type="pub" publication-format="electronic">
            <day>14</day>
            <month>10</month>
            <year>2011</year>
          </pub-date>
          <pub-date date-type="collection" publication-format="electronic">
            <month>12</month>
            <year>2011</year>
          </pub-date>
          <volume>3</volume>
          <issue seq="44">1</issue>
          <elocation-id>45</elocation-id>
          <history>
            <date date-type="registration">
              <day>13</day>
              <month>6</month>
              <year>2011</year>
            </date>
            <date date-type="received">
              <day>13</day>
              <month>6</month>
              <year>2011</year>
            </date>
            <date date-type="accepted">
              <day>14</day>
              <month>10</month>
              <year>2011</year>
            </date>
            <date date-type="online">
              <day>14</day>
              <month>10</month>
              <year>2011</year>
            </date>
          </history>
          <permissions>
            <copyright-statement content-type="compact">© Brooks et al; licensee Chemistry Central Ltd. 2011</copyright-statement>
            <copyright-statement content-type="comment">This article is published under license to BioMed Central Ltd. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link xlink:href="http://creativecommons.org/licenses/by/2.0" ext-link-type="uri">http://creativecommons.org/licenses/by/2.0</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited.</copyright-statement>
            <copyright-year>2011</copyright-year>
            <copyright-holder>Brooks et al; licensee Chemistry Central Ltd.</copyright-holder>
            <license license-type="open-access" xlink:href="https://creativecommons.org/licenses/by-nc/2.0">
              <license-p><bold>Open Access</bold> This is an open access article distributed under the terms of the Creative Commons Attribution Noncommercial License ( 
    <ext-link xlink:href="https://creativecommons.org/licenses/by-nc/2.0" ext-link-type="uri">https://creativecommons.org/licenses/by-nc/2.0</ext-link> 
    ), which permits any noncommercial use, distribution, and reproduction in any medium, provided the original author(s) and source are credited. 
</license-p>
            </license>
          </permissions>
          <abstract xml:lang="en" id="Abs1">
            <title>Abstract</title>
            <p>The Ami project was a six month Rapid Innovation project sponsored by JISC to explore the Virtual Research Environment space. The project brainstormed with chemists and decided to investigate ways to facilitate monitoring and collection of experimental data.</p>
            <p>A frequently encountered use-case was identified of how the chemist reaches the end of an experiment, but finds an unexpected result. The ability to replay events can significantly help make sense of how things progressed. The project therefore concentrated on collecting a variety of dimensions of ancillary data - data that would not normally be collected due to practicality constraints. There were three main areas of investigation: 1) Development of a monitoring tool using infrared and ultrasonic sensors; 2) Time-lapse motion video capture (for example, videoing 5 seconds in every 60); and 3) Activity-driven video monitoring of the fume cupboard environs.</p>
            <p>The Ami client application was developed to control these separate logging functions. The application builds up a timeline of the events in the experiment and around the fume cupboard. The videos and data logs can then be reviewed after the experiment in order to help the chemist determine the exact timings and conditions used.</p>
            <p>The project experimented with ways in which a Microsoft Kinect could be used in a laboratory setting. Investigations suggest that it would not be an ideal device for controlling a mouse, but it shows promise for usages such as manipulating virtual molecules.</p>
            <fig id="Figa" position="anchor">
              <graphic specific-use="web" mime-subtype="GIF" xlink:href="MediaObjects/13321_2011_Article_227_Figa_HTML.gif" position="anchor" />
            </fig>
          </abstract>
          <kwd-group xml:lang="en">
            <title>Keywords</title>
            <kwd>Speech Recognition</kwd>
            <kwd>Video Capture</kwd>
            <kwd>Fume Cupboard</kwd>
            <kwd>Speech Recognition Package</kwd>
            <kwd>Java Medium Framework</kwd>
          </kwd-group>
          <custom-meta-group>
            <custom-meta>
              <meta-name>publisher-imprint-name</meta-name>
              <meta-value>Chemistry Central</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>volume-issue-count</meta-name>
              <meta-value>1</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-article-count</meta-name>
              <meta-value>54</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-copyright-holder</meta-name>
              <meta-value>The Author(s)</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-copyright-year</meta-name>
              <meta-value>2011</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-contains-esm</meta-name>
              <meta-value>Yes</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-numbering-style</meta-name>
              <meta-value>Unnumbered</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-year</meta-name>
              <meta-value>2011</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-month</meta-name>
              <meta-value>6</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-day</meta-name>
              <meta-value>13</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>volume-type</meta-name>
              <meta-value>Regular</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-product</meta-name>
              <meta-value>ArchiveJournal</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>numbering-style</meta-name>
              <meta-value>Unnumbered</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-grants-type</meta-name>
              <meta-value>OpenChoice</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>metadata-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>abstract-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bodypdf-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bodyhtml-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bibliography-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>esm-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>online-first</meta-name>
              <meta-value>false</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>pdf-file-reference</meta-name>
              <meta-value>BodyRef/PDF/13321_2011_Article_227.pdf</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>pdf-type</meta-name>
              <meta-value>Typeset</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>target-type</meta-name>
              <meta-value>OnlinePDF</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-type</meta-name>
              <meta-value>Regular</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-type</meta-name>
              <meta-value>OriginalPaper</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-primary</meta-name>
              <meta-value>Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Computer Applications in Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Documentation and Information in Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Theoretical and Computational Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Computational Biology/Bioinformatics</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-collection</meta-name>
              <meta-value>Chemistry and Materials Science</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>open-access</meta-name>
              <meta-value>true</meta-value>
            </custom-meta>
          </custom-meta-group>
        </article-meta>
        <notes notes-type="ESMHint">
          <title>Electronic supplementary material</title>
          <p>The online version of this article (doi:<ext-link xlink:href="10.1186/1758-2946-3-45" ext-link-type="doi">10.1186/1758-2946-3-45</ext-link>) contains supplementary material, which is available to authorized users.</p>
        </notes>
      </front>
      <body>
        <sec id="Sec1">
          <title>Background</title>
          <p>
            <italic>Amanuensis: One employed to take dictation, or copy manuscripts; A clerk, secretary or stenographer, or scribe</italic>
            <ext-link xlink:href="http://en.wiktionary.org/wiki/amanuensis" ext-link-type="uri">http://en.wiktionary.org/wiki/amanuensis</ext-link>
          </p>
          <p>The chemistry laboratory is a difficult environment for using a computer. Space is at a premium; benches and fume cupboards are covered with apparatus and typically have chemicals that are detrimental to computers (Figure <xref rid="Fig1" ref-type="fig">1</xref>). The chemist wears protective clothing, and often has gloves on (Figure <xref rid="Fig2" ref-type="fig">2</xref>). Lots of little issues add up to make it a challenge to successfully use computers in the lab.<fig id="Fig1"><label>Figure 1</label><caption xml:lang="en"><p><bold>Chemist working at a typical fume cupboard</bold>. It is common to use the glass front for drawing reactions, jotting notes, etc.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig1_HTML.jpg" /></fig><fig id="Fig2"><label>Figure 2</label><caption xml:lang="en"><p><bold>A chemist happy in his element, and probably with his elements too</bold>. Note the protective clothing. Space in the lab is at a premium; catering for computer is an after-thought.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig2_HTML.jpg" /></fig></p>
          <p>Yet the collection of data has never been more important. Trends in science are to require data in support of experimental results. It is considered that research paid for by public money should have the proceeds visible to anyone who may wish to use them. Recent examples of challenging the conclusions of the scientific community - such as MMR episode [<xref ref-type="bibr" rid="CR1">1</xref>] or the Climate-Gate emails [<xref ref-type="bibr" rid="CR2">2</xref>] - plus various examples of scientific fraud, are all events that could have been ameliorated if their data were open for review.</p>
          <p>In recent years, various projects have highlighted how hardware and software can be used for the collection, management and use of laboratory information. At the University of Cambridge Cavendish Laboratory Baumberg <italic>et al</italic>. [<xref ref-type="bibr" rid="CR3">3</xref>] illustrate how new hardware forms (desktop "Surfaces" and tablets) facilitate the use of visual sketching techniques to enhance the scientific process, in particular within a group. The Frey group at Southampton have shown [<xref ref-type="bibr" rid="CR4">4</xref>, <xref ref-type="bibr" rid="CR5">5</xref>] how semantic tools can be used to link complex information from the whole experiment lifecycle. The OPSIN chemical name-to-structure program [<xref ref-type="bibr" rid="CR6">6</xref>, <xref ref-type="bibr" rid="CR7">7</xref>] developed by Lowe <italic>et al</italic>. here in the Unilever Centre has been extended to show how complex information can be accessed using smartphones [<xref ref-type="bibr" rid="CR8">8</xref>].</p>
          <p>The Ami project was created to find improved ways for chemists to use computers in the lab. The goal was to build a prototype next-generation information assistant "natural user interface" for scientists working at the lab bench. The limitations of paper lab notebooks are well recorded, and the Chemistry Department at Cambridge has recently deployed a commercial electronic lab notebook (ELN), a project in which one of the Ami team members was a significant participant. The Ami project aimed to combine and develop existing hardware and software technologies in novel ways to provide an information rich environment for the scientist at the bench.</p>
          <sec id="Sec2">
            <title>Methodology</title>
            <p>The Ami project used a brainstorming session with chemists from the department plus representatives from the Chemistry Department's computing service to identify the issues that chemists have to deal with and how computers could be used to address them (see Appendix 1 for further details). A common use-case that emerged was the example of how chemists often reach the end of their experiment and find an unexpected result. Often the suspicion is that the unexpected result could be due to mundane reasons; the conditions used for the reaction may have varied unexpectedly, or reaction components were not added, or timing was critical, or the wrong chemicals were used, etc. What was required was some way of going back over events to see what actually did happen. What was needed was some way of collecting ancillary data - data that is not the primary data that is scientifically obvious to collect - that could be consulted after the experiment is finished if circumstances needed to be investigated further.</p>
            <p>The desire to log ancillary data identified three areas to work on. The first was to build some hardware device that could monitor parameters such as the temperature of the reaction vessel, keeping a log over the whole duration of the experiment. The second was a video monitor to provide a close-up visual record of the reaction. The third area was a wide-angle video monitor of the whole fumehood which would log all activity in the vicinity of the reaction.</p>
            <p>Windows 7 was chosen as the development platform for Ami. This was because of the wide availability of software tools and utilities available for Windows, and also because of the experience within the group. Where possible code was developed in Java, using the IntelliJ IDEA development tool.</p>
          </sec>
          <sec id="Sec3">
            <title>Ami Client Application</title>
            <p>The Ami client application - "Ami" - is the central application with which the chemist interacts when in the lab. The chemist logs into Ami using their identity badge which is detected by a Touch-A-Tag RFID reader. A list of experiments is then displayed, plus the option to create a new experiment (Figure <xref rid="Fig3" ref-type="fig">3</xref>).<fig id="Fig3"><label>Figure 3</label><caption xml:lang="en"><p><bold>The Ami experiment selection screen</bold>.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig3_HTML.jpg" /></fig></p>
            <p>Having selected their desired experiment, Ami then displays the main experiment control screen. Tabs at the top are used to switch between the Event Log, Sensor Control, and Experiment Details screens. All tabs and buttons can be controlled using speech, the keyboard, or the mouse.</p>
            <p>The Event Log shows all the events that have occurred in the experiment. The log can be filtered by event type if desired (Figure <xref rid="Fig4" ref-type="fig">4</xref>).<fig id="Fig4"><label>Figure 4</label><caption xml:lang="en"><p><bold>The Ami Event Log screen</bold>. The main body shows events logged. Buttons across the top allow selection of different event types. At bottom left are buttons to log different events. Buttons at bottom right are for saving data and launching the timeline viewer. Tabs at the top switch the display to the Sensor Control and Experiment Details screens.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig4_HTML.jpg" /></fig></p>
            <p>Ami allows all chemicals and pieces of apparatus used in the experiment to be tagged with an RFID tag. These are easily and cheaply available in a variety of forms so that they are easy to stick to chemical bottles and apparatus. During the experiment, the chemist registers all the components with Ami.</p>
            <p>As an experiment proceeds, the chemist logs usage of chemicals and apparatus by simply waving them in front of the RFID reader. The date and time of the event is recorded by Ami, so that a timeline of events in built up showing activity in the experiment. The chemist can also add observations by dictating to the PC's microphone, or simply by using the keyboard.</p>
            <p>A refinement of the RFID tagging was an "intelligent labcoat". This was achieved by using a mini Arduino card with an RFID reader built into the sleeve. The Arduino had a Bluetooth transmitter with it, which was able to transmit readings to the Ami application running on a PC. The RFID detector in the labcoat sleeve automatically registers any tagged chemical or piece of equipment that the chemist's hand goes near. The events logged by the labcoat are then transmitted to the Ami application for including as part of the experiment timeline (Figure <xref rid="Fig5" ref-type="fig">5</xref>).<fig id="Fig5"><label>Figure 5</label><caption xml:lang="en"><p><bold>Sensor control tab</bold>. Alarm limits can be set for upper and lower temperatures. Time interval can be specified for taking temperature, distance (ultrasonic sensor), and motion detection (infrared motion detector). Buttons at bottom left are used for starting motion-timelapse video and environs-monitoring video. Buttons at top right control display of sensor graphs.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig5_HTML.jpg" /></fig></p>
            <p>Each sensor monitored by Ami has its own logfile (Figure <xref rid="Fig6" ref-type="fig">6</xref>). All output files are stored in one directory for each experiment, making it easy to keep track of all data created, and to transfer it to the electronic lab notebook. The output from all the sensors and events can be displayed as a graphical timeline to facilitate review of the experiment activities (Figure <xref rid="Fig7" ref-type="fig">7</xref>).<fig id="Fig6"><label>Figure 6</label><caption xml:lang="en"><p><bold>Example event log file for the creation of a cup of tea, inspired by the Southampton Smart Tea project</bold> [<xref ref-type="bibr" rid="CR30">30</xref>]. New events are appended as they occur.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig6_HTML.jpg" /></fig><fig id="Fig7"><label>Figure 7</label><caption xml:lang="en"><p><bold>Graphical view of experiment timeline</bold>. The Timeline application [<xref ref-type="bibr" rid="CR31">31</xref>] is written in Python and receives an XML event-log file created by Ami. Timeline takes this event log and displays different sorts of events in different colours.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig7_HTML.jpg" /></fig></p>
          </sec>
          <sec id="Sec4">
            <title>Monitoring Device - Arduino</title>
            <p>For close-up monitoring of the reaction, an infrared temperature sensor was used (Figure <xref rid="Fig8" ref-type="fig">8</xref>). This was controlled by an Arduino circuit board (Figure <xref rid="Fig9" ref-type="fig">9</xref>), which was programmed using the open source Arduino software [<xref ref-type="bibr" rid="CR9">9</xref>].<fig id="Fig8"><label>Figure 8</label><caption xml:lang="en"><p><bold>The Ami Experiment Monitoring Tool, here monitoring tea temperature...</bold></p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig8_HTML.jpg" /></fig><fig id="Fig9"><label>Figure 9</label><caption xml:lang="en"><p><bold>The infrared sensor being tested on an Arduino circuit board</bold>.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig9_HTML.jpg" /></fig></p>
            <p>The Ami Experiment Monitoring Tool also has an ultrasonic distance sensor and an infrared PIR motion sensor. Output from the sensors is sent to the Ami client application, which is a Java program running on the PC.</p>
          </sec>
          <sec id="Sec5">
            <title>Close-up video monitor</title>
            <p>The usual way of doing time-lapse photography is to take a still picture at regular intervals, then stitch them together to make a moving picture. We wanted to do something slightly different; instead of a still picture, we wanted to use a few seconds of normal moving video, and then stitch them all together to make a time-lapse video. The advantage of this is that it is then possible to see how a given material is behaving (<italic>e.g</italic>. viscosity) which isn't possible from a still picture.</p>
            <p>Recording time-lapse video turned out to be more difficult than expected. We were unable to find an off-the-shelf application that we could use to provide this functionality. Open source Java routines to monitor video had performance issues, for example very poor frame-rates. The main problem seemed to be that available Java-based open source code was out of date; it was all based on the Java Media Framework (JMF) [<xref ref-type="bibr" rid="CR10">10</xref>], the API of which has not changed since 1999, and the last minor modification was in 2004. The open-source FFmpeg [<xref ref-type="bibr" rid="CR11">11</xref>] utility was also tried, but its video capture functionality is provided through "Video For Windows" (V4W) which is not supported on Windows 7. Eventually we settled on VLC [<xref ref-type="bibr" rid="CR12">12</xref>], which is based on Microsoft's DirectShow framework (better supported and up to date).</p>
            <p>Because VLC is an application, the problem arose as to how to start and stop it from the Ami application. Fortunately VLC can be controlled via a telnet connection, so Ami uses telnet to configure the video capture and to start and stop video capture. There is an additional bonus that this separation of video recorder from controller enables multiple cameras to be used and also to start and stop recording on remote systems, without a physical connection to them.</p>
            <p>Linking the videos together also turned out to be more difficult than expected. Modern compressed video formats such as AVI, MPEG4 work by encoding differences between successive frames. When concatenating video it is therefore necessary to decompress the videos before combining them together and re-encoding them using a given compression algorithm, such as an MPEG4 based codec.</p>
            <p>Fortunately, VLC again has the ability to do this task but due to the nature of video concatenation, this process of stitching together the files is best done at the end of an experiment, rather than repeating the CPU-intensive process for each stage. Compression artefacts are extremely liable to arise from the process of repeatedly decompressing and recompressing as well, degrading the quality of the video. It may be possible to 'pause' a recording using the VLC capture, but if any errors arose or power is lost, the video data would have to be recovered manually.</p>
            <p>Storing video files alongside the experimental data enables the logs to travel with the data, given a repository such as the ELN that can accept arbitrary files as part of a submission.</p>
          </sec>
          <sec id="Sec6">
            <title>Wide-angle video monitor</title>
            <p>A common source of error in doing experiments is simply absent-mindedness, forgetting to do something, or using the wrong chemical. The wide-angle video monitor is triggered by activity at the fume cupboard, and records video until activity stops. This gives the ability to replay events over the course of the experiment, hopefully enabling a full picture of what actually was done to be understood. Mounting the webcam high up at the back or side of the fume cupboard gives the best view of activities.</p>
            <p>Two methods of triggering the recording were identified. The first was to use an infrared movement detector, which was connected to the Arduino. When movement was detected, the event is passed by the Arduino back to the Ami program, which then starts the video monitor. The video simply records for a specified duration after movement is no longer detected. The second method was by monitoring the changes in the image itself, and if a threshold value is reached, to start videoing. Unfortunately time did not permit us to explore this area sufficiently to get a working system going.</p>
          </sec>
          <sec id="Sec7">
            <title>Experiments with the Microsoft Kinect</title>
            <p>Towards the end of the Ami project, Microsoft released the Kinect (Figure <xref rid="Fig10" ref-type="fig">10</xref>). The Kinect is an accessory to Microsoft's Xbox consumer gaming machine, and is an exciting new development in human-computer interactions because it uses purely visual techniques to build a three-dimensional understanding of the space in front of it. The Kinect can recognise when a person is standing in front of it, and automatically determine the positions of the person's head, body, arms, hands, legs and feet. The user does not need such things as a transmitting device or special reflective tags; they just have to stand in front of the Kinect. The Kinect is potentially a disruptive technology and is already showing huge potential in robotics [<xref ref-type="bibr" rid="CR13">13</xref>]. It has enormous potential for Ami; positioning a Kinect in a fume cupboard could give the user new ways to interact with the computer, and help in monitoring the environment.<fig id="Fig10"><label>Figure 10</label><caption xml:lang="en"><p><bold>The Microsoft Kinect</bold>. This connects to a computer via a USB connector, making it a powerful way to communicate with computers. The infrared transmitter is on the left.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig10_HTML.jpg" /></fig></p>
            <p>The Kinect consists of a relatively small box (about 25 × 12 × 3 cm) which has two video cameras built into it [<xref ref-type="bibr" rid="CR14">14</xref>, <xref ref-type="bibr" rid="CR15">15</xref>]. One video camera is used for normal videoing using visible-light. The other is an infrared camera which monitors a pattern of infrared dots that the device shines into the room [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>]. The on-board processing built into the Kinect enables it to understand the 3D location of all the objects in front of it (<italic>i.e</italic>. the spatial analysis is done by the Kinect itself, rather than by the computer that it is attached to). The attached computer receives from the Kinect a video feed plus a stream of data points of the 3D locations of all the objects detected by the Kinect. The Kinect also contains four microphones, but using these was not investigated in this project because at the time that this work was done no code had been released which made the sound output available.</p>
            <p>One slight limitation of the Kinect is that its 3D view of the area in front of it is necessarily only seen from one position [<xref ref-type="bibr" rid="CR18">18</xref>]. This means that it cannot understand a full three dimensional view of an object, because it can only see the side nearest the detector. Anything behind an object, and the back of an object, cannot be seen. This could be improved by using more than one Kinect operating together so that they can pool their individual views, and no doubt the techniques and code necessary to achieve this fuller 3D view will emerge over time [<xref ref-type="bibr" rid="CR19">19</xref>].</p>
            <sec id="Sec8">
              <title>Monitoring 3D space</title>
              <p>The Kinect returns a three dimensional description of what it can see in front of it as well as a conventional 'RGB' view. The resolution of the normal colour image camera is 640 × 480, whereas the three dimensional camera is 320 × 240 [<xref ref-type="bibr" rid="CR20">20</xref>]. Whilst this makes it a poor choice for image recognition, logging and so on, the depth camera delivers data that is fundamentally unavailable from other sources. This makes it incredibly exciting in terms of the types of data and interaction it can enable.</p>
              <p>The working range of the Kinect is suitable for a large living room, as it was designed with that in mind. It was found that in the cramped confines of a fume cupboard the detector was not far enough away for reliable operation. This rather precludes the Kinect from being used for monitoring the 3D environment within the fume cupboard (the size of a typical fume cupboard is about 1.7 m wide × 1.2 m high × 0.7 m deep). So we turned our investigations to using the Kinect for controlling the computer itself; because the Kinect monitors body movements, it might be good for someone who is wearing protective clothing.</p>
            </sec>
            <sec id="Sec9">
              <title>Using the Kinect to control a mouse</title>
              <p>One of the intuitive ideas for using a Kinect is to detect human movement and gestures to control a computer, so called 'natural interaction'. One of the first experiments we tried involved detecting a hand and coupling a mouse pointer to respond to its movements. This was done at first by crudely detecting the closest 'blobs' present in the depth, i.e. the hands, and using the motions of these to control the mouse pointer of a computer. At a later stage, we used a much more sophisticated technique to capture the hand motions, which involved 'skeleton mapping' that understood and interpreted the depth of field and looking for humans (Figure <xref rid="Fig11" ref-type="fig">11</xref>). (Skeleton-mapping, as provided by PrimeSense [<xref ref-type="bibr" rid="CR21">21</xref>], required a stack of software to enable, including the <ext-link xlink:href="http://www.OpenNI.org" ext-link-type="uri">http://www.OpenNI.org</ext-link> framework and the SensorKinect driver [<xref ref-type="bibr" rid="CR22">22</xref>]<ext-link xlink:href="https://github.com/avin2/SensorKinect" ext-link-type="uri">https://github.com/avin2/SensorKinect</ext-link> as well as their free closed-source middleware library.) This proved to be much more accurate and not affected unduly by background movement.<fig id="Fig11"><label>Figure 11</label><caption xml:lang="en"><p><bold>Screenshot showing 3D visualisation of the Kinect's interpretation of the volume it can see</bold>. Only the user's shoulder and hand joints are shown for clarity.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig11_HTML.jpg" /></fig></p>
              <p>However, this mouse-metaphor interface proved to be a poor one in the end. This was not due to technical reasons; simply, the human body is not suited to standing still with an arm held out for periods of time. With a hand resting on a desk, it is easy to have the accuracy needed to click on items. With the arm stretched out, it becomes difficult to hold it in a given position for any amount of time.</p>
              <p>It is necessary to build the interface so that the interactions involve periods of relaxation or the ability to ignore actions made unintentionally. One particularly successful form of interaction is selecting items from a menu, where the hand is raised to select an item from a list and a choice is made by swiping the hand across. Swiping the other hand back across is used to cancel that choice. This has the benefit that in between choosing, the arms can be left to relax without worrying that a mouse-pointer would skitter across the screen and select or highlight something unintentionally.</p>
            </sec>
            <sec id="Sec10">
              <title>Using the Kinect to control molecule visualisation</title>
              <p>Moving away from the fume cupboard for a moment, one potential use-case for the Kinect is to use it as an appliance in a given meeting room or open space for collaboration or interaction. We implemented this by using the Kinect's skeletal mapping functions to track hand positions which were then broadcast via multicast to any computer within the same network. This has the advantage that the client computers need no knowledge of how to read and process Kinect data directly, only software that can make use of the hand-position mappings in the same way it might read the location of a mouse pointer [<xref ref-type="bibr" rid="CR23">23</xref>]. At the end of the project, a symposium was held in honour of Dr Murray-Rust's ideas [<xref ref-type="bibr" rid="CR24">24</xref>]. Immediately preceding the symposium was a hackfest at which our experiences working with the Kinect were used to control the rotation and zooming of a molecule in Jmol (Figure <xref rid="Fig12" ref-type="fig">12</xref>).<fig id="Fig12"><label>Figure 12</label><caption xml:lang="en"><p><bold>Controlling rotation of a molecule using hand gestures</bold>. The Kinect is on the bench, on top of the silver cylinder. Picture taken at PMR Symposium.</p></caption><graphic specific-use="web" mime-subtype="JPEG" xlink:href="MediaObjects/13321_2011_Article_227_Fig12_HTML.jpg" /></fig></p>
            </sec>
          </sec>
          <sec id="Sec11">
            <title>Speech recognition</title>
            <p>For a chemist working in the lab, the ability to use speech to communicate with their computer would be a great advantage. Preparative work before the start of Ami showed that Windows Speech Recognition (WSR - the speech recognition facilities build into Windows 7) could be used to control [<xref ref-type="bibr" rid="CR25">25</xref>] the Chemistry add-in for Microsoft Word, Chem4Word [<xref ref-type="bibr" rid="CR26">26</xref>]. Dragon Naturally Speaking (DNS) is the leading speech recognition package, so this was also evaluated.</p>
            <p>Both WSR and DNS have the ability to define macros for navigating around the screen, clicking on buttons, etc. The ability of these tools to use speech to control an application was tested by trying to use only speech to operate the Chemistry Department's ELN, a Java-based application developed by IDBS [<xref ref-type="bibr" rid="CR27">27</xref>]. However, neither WSR or DNS worked very well with Java applications; WSR in particular is much more functional with Windows-based applications because it has closer ties into the operating system's understanding of what objects are being displayed. Controlling the ELN was best achieved using send-key type instructions; if keyboard shortcuts did not exist for a particular activity, then this limited the possible actions.</p>
            <p>Both WSR and DNS can be used to dictate text into Java applications. The demands of a specialist chemistry vocabulary are pretty stringent, however, so for chemistry dictation the transcription accuracy varies significantly. It is possible to train each package to improve voice recognition, but this is a potentially enormous topic and it was not done to any significant extent in this project. DNS has the ability to digest sample documents that the user gives in order to understand their particular vocabularies, but this was not explored beyond initial configuration.</p>
            <p>For the Ami application WSR was chosen for doing further development, mainly because of licensing costs; DNS is very expensive. WSR has a free extension, Windows Speech Macros, which enables tailoring of the commands used when speech is recognised. This was fairly successful and it is possible to navigate all of the screens and buttons in the Ami application. WSR listens for key phrases, and then uses send-key instructions (most commonly an Alt- &lt; single-key > code) to send key codes to buttons in the Ami application. Additionally, WSR can be used for dictating comments (experiment observations) directly into Ami, though we did not have the time to investigate its accuracy or ways of enhancing it and it was only used by the development team.</p>
          </sec>
        </sec>
        <sec id="Sec12">
          <title>Outcomes &amp; Conclusions</title>
          <p>The main outcome from the project was a demonstrator application that shows how experiments and the environment around them can be monitored using various sensors and video monitors. We had the stretch goal of actually having this used by real chemists for real experiments, but unfortunately time prevented us from polishing the system to a sufficient level to allow this.</p>
          <p>At the launch meeting for the Dial-A-Molecule EPSRC Grand Challenge [<xref ref-type="bibr" rid="CR28">28</xref>], a common theme that emerged was the need to have access to chemical data. Much of the data generated in laboratories does not get collected and made available in a form that other chemists can use. Time pressures mean that very often scientists do not get around to making their data available. The Ami project showed how there is huge potential for computers to help the bench chemist in their activities in the lab, and to make much of this information available for further use. In its six months Ami has investigated many technologies and ideas; an obvious follow-on to the project is to consolidate these ideas into a fully integrated tool that can be used in real laboratories. Additionally, there is much potential for further work on the flow of data from the experiment to electronic lab notebooks to an embargo management tool and thence to open repositories, thus facilitating re-use. Reviewers have pointed out how important this type of data will be for retrospective analysis, especially in cases of unexpected results or experimental reproducibility.</p>
        </sec>
        <sec id="Sec13">
          <title>Appendix 1: Links to documentation, code resources, etc</title>
          <p>Brainstorming session:</p>
          <p>
            <list list-type="bullet">
              <list-item>
                <p>Output from the brainstorming session: <ext-link xlink:href="https://bitbucket.org/jat45/ami/downloads/Notes%20output%20from%20Ami%20brainstorming%20session%207May10.docx" ext-link-type="uri">https://bitbucket.org/jat45/ami/downloads/Notes%20output%20from%20Ami%20brainstorming%20session%207May10.docx</ext-link></p>
              </list-item>
            </list>
          </p>
          <p>Project website &amp; tags:</p>
          <p>
            <list list-type="bullet">
              <list-item>
                <p>Project blog: <ext-link xlink:href="http://amiproject.wordpress.com" ext-link-type="uri">http://amiproject.wordpress.com</ext-link></p>
              </list-item>
              <list-item>
                <p>Project Wiki: <ext-link xlink:href="http://bitbucket.org/bjb45/ami-project" ext-link-type="uri">http://bitbucket.org/bjb45/ami-project</ext-link></p>
              </list-item>
              <list-item>
                <p>Project code: <ext-link xlink:href="http://bitbucket.org/jat45/ami/" ext-link-type="uri">http://bitbucket.org/jat45/ami/</ext-link></p>
              </list-item>
            </list>
          </p>
          <p>Software used:</p>
          <p>
            <list list-type="bullet">
              <list-item>
                <p>Java development - IntelliJIDEA Community Edition: <ext-link xlink:href="http://www.jetbrains.com/idea/download/" ext-link-type="uri">http://www.jetbrains.com/idea/download/</ext-link></p>
              </list-item>
              <list-item>
                <p>Speech Macros - Windows: <ext-link xlink:href="http://code.msdn.microsoft.com/wsrmacros" ext-link-type="uri">http://code.msdn.microsoft.com/wsrmacros</ext-link></p>
              </list-item>
              <list-item>
                <p>JFreeChart Java graph package: <ext-link xlink:href="http://www.jfree.org/jfreechart/" ext-link-type="uri">http://www.jfree.org/jfreechart/</ext-link></p>
              </list-item>
              <list-item>
                <p>Timeline application: <ext-link xlink:href="http://thetimelineproj.sourceforge.net/" ext-link-type="uri">http://thetimelineproj.sourceforge.net/</ext-link></p>
              </list-item>
              <list-item>
                <p>Natty - Java library for processing data/times: <ext-link xlink:href="http://natty.joestelmach.com/" ext-link-type="uri">http://natty.joestelmach.com/</ext-link></p>
              </list-item>
              <list-item>
                <p>Video capture - VLC: <ext-link xlink:href="http://www.videolan.org/vlc/" ext-link-type="uri">http://www.videolan.org/vlc/</ext-link></p>
              </list-item>
            </list>
          </p>
          <p>Project code:</p>
          <p>
            <list list-type="bullet">
              <list-item>
                <p>Data logger - Arduino program: <ext-link xlink:href="https://bitbucket.org/jat45/ami/src/096e6df85d58/arduinoControllerWithoutSD/" ext-link-type="uri">https://bitbucket.org/jat45/ami/src/096e6df85d58/arduinoControllerWithoutSD/</ext-link></p>
              </list-item>
              <list-item>
                <p>Ami application: <ext-link xlink:href="https://bitbucket.org/jat45/ami" ext-link-type="uri">https://bitbucket.org/jat45/ami</ext-link></p>
              </list-item>
              <list-item>
                <p>Experiments with the Kinect: <ext-link xlink:href="https://github.com/benosteen/Kinect-tracking-code" ext-link-type="uri">https://github.com/benosteen/Kinect-tracking-code</ext-link></p>
              </list-item>
            </list>
          </p>
          <p>Other tools used:</p>
          <p>
            <list list-type="bullet">
              <list-item>
                <p>Speech recognition - Dragon Naturally Speaking: <ext-link xlink:href="http://nuance.co.uk/" ext-link-type="uri">http://nuance.co.uk/</ext-link></p>
              </list-item>
              <list-item>
                <p>RFID reader - Touch-A-Tag: <ext-link xlink:href="http://www.touchatag.com/" ext-link-type="uri">http://www.touchatag.com/</ext-link></p>
              </list-item>
            </list>
          </p>
        </sec>
      </body>
      <back>
        <ack>
          <title>Acknowledgements</title>
          <p>Funding from JISC for the Ami project is gratefully acknowledged, as is funding from Unilever for PMR. Ami was a six month project under the "JISC Rapid Innovation Grants 10/09" programme [<xref ref-type="bibr" rid="CR29">29</xref>]. Our thanks to Drs Richard Turner, Nadine Bremeyer and Chris Lowe for their scientific advice. The project team was located in the Unilever Centre in the Chemistry Department at the University of Cambridge.</p>
        </ack>
        <ref-list id="Bib1">
          <title>References</title>
          <ref-list>
            <ref id="CR1">
              <label>1.</label>
              <mixed-citation publication-type="other">MMR vaccine controversy. Accessed 2011-05-19, [<ext-link xlink:href="http://en.wikipedia.org/wiki/MMR_vaccine_controversy" ext-link-type="uri">http://en.wikipedia.org/wiki/MMR_vaccine_controversy</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR2">
              <label>2.</label>
              <mixed-citation publication-type="other">Climatic Research Unit email controversy. Accessed 2011-05-19, [<ext-link xlink:href="http://en.wikipedia.org/wiki/Climatic_Research_Unit_email_controversy" ext-link-type="uri">http://en.wikipedia.org/wiki/Climatic_Research_Unit_email_controversy</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR3">
              <label>3.</label>
              <mixed-citation publication-type="book">
                <person-group person-group-type="author">
                  <name>
                    <surname>Baumberg</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Jetter</surname>
                    <given-names>H-C</given-names>
                  </name>
                  <name>
                    <surname>Milic-Frayling</surname>
                    <given-names>N</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">MetaSurfacing with the Surface</article-title>
                <source>Microsoft External Research Symposium</source>
                <year>2010</year>
                <comment>http://research.microsoft.com/en-us/UM/redmond/events/ERSymposium2010/slides/Baumberg.pdf Accessed 2011-05-19</comment>
              </mixed-citation>
            </ref>
            <ref id="CR4">
              <label>4.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Hughes</surname>
                    <given-names>G</given-names>
                  </name>
                  <name>
                    <surname>Mills</surname>
                    <given-names>H</given-names>
                  </name>
                  <name>
                    <surname>De Roure</surname>
                    <given-names>D</given-names>
                  </name>
                  <name>
                    <surname>Frey</surname>
                    <given-names>JG</given-names>
                  </name>
                  <name>
                    <surname>Moreau</surname>
                    <given-names>L</given-names>
                  </name>
                  <name>
                    <surname>Schraefel</surname>
                    <given-names>MC</given-names>
                  </name>
                  <name>
                    <surname>Smith</surname>
                    <given-names>G</given-names>
                  </name>
                  <name>
                    <surname>Zaluska</surname>
                    <given-names>E</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">The semantic smart laboratory: a system for supporting the chemical eScientist</article-title>
                <source>Org Biomol Chem</source>
                <year>2004</year>
                <volume>2</volume>
                <fpage>3284</fpage>
                <lpage>3293</lpage>
                <pub-id pub-id-type="doi">10.1039/b410075a</pub-id>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2cXpsVOqt7g%3D</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR5">
              <label>5.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Taylor</surname>
                    <given-names>KR</given-names>
                  </name>
                  <name>
                    <surname>Essex</surname>
                    <given-names>JW</given-names>
                  </name>
                  <name>
                    <surname>Frey</surname>
                    <given-names>JG</given-names>
                  </name>
                  <name>
                    <surname>Mills</surname>
                    <given-names>HR</given-names>
                  </name>
                  <name>
                    <surname>Hughes</surname>
                    <given-names>G</given-names>
                  </name>
                  <name>
                    <surname>Zaluska</surname>
                    <given-names>EJ</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">The Semantic Grid and chemistry: Experiences with Comb<italic>e</italic>Chem</article-title>
                <source>Web Semantics: Science, Services and Agents on the World Wide Web</source>
                <year>2006</year>
                <volume>4</volume>
                <fpage>84</fpage>
                <lpage>101</lpage>
                <pub-id pub-id-type="doi">10.1016/j.websem.2006.03.003</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR6">
              <label>6.</label>
              <mixed-citation publication-type="other">OPSIN, Open Parser for Systematic IUPAC Nomenclature. Accessed 2011-05-19, [<ext-link xlink:href="http://opsin.ch.cam.ac.uk/" ext-link-type="uri">http://opsin.ch.cam.ac.uk/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR7">
              <label>7.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Lowe</surname>
                    <given-names>DM</given-names>
                  </name>
                  <name>
                    <surname>Corbett</surname>
                    <given-names>PT</given-names>
                  </name>
                  <name>
                    <surname>Murray-Rust</surname>
                    <given-names>P</given-names>
                  </name>
                  <name>
                    <surname>Glen</surname>
                    <given-names>RC</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Chemical Name to Structure: OPSIN, an Open Source Solution</article-title>
                <source>J Chem Inf Model</source>
                <year>2011</year>
                <volume>51</volume>
                <fpage>739</fpage>
                <lpage>753</lpage>
                <pub-id pub-id-type="doi">10.1021/ci100384d</pub-id>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC3MXivVCht7o%3D</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR8">
              <label>8.</label>
              <mixed-citation publication-type="other">Lowe, D. OPSIN-Android. Accessed 2011-05-19, [<ext-link xlink:href="https://bitbucket.org/dan2097/opsin-android" ext-link-type="uri">https://bitbucket.org/dan2097/opsin-android</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR9">
              <label>9.</label>
              <mixed-citation publication-type="other">Arduino. Accessed 2011-05-19, [<ext-link xlink:href="http://www.arduino.cc/" ext-link-type="uri">http://www.arduino.cc/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR10">
              <label>10.</label>
              <mixed-citation publication-type="other">The Java Media Framework. Accessed 2011-05-19, [<ext-link xlink:href="http://www.oracle.com/technetwork/java/javase/tech/index-jsp-140239.html" ext-link-type="uri">http://www.oracle.com/technetwork/java/javase/tech/index-jsp-140239.html</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR11">
              <label>11.</label>
              <mixed-citation publication-type="other">Ffmpeg. Accessed 2011-05-19, [<ext-link xlink:href="http://www.ffmpeg.org/" ext-link-type="uri">http://www.ffmpeg.org/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR12">
              <label>12.</label>
              <mixed-citation publication-type="other">VideoLAN, VLC Media Player. Accessed 2011-05-19, [<ext-link xlink:href="http://www.videolan.org/vlc/" ext-link-type="uri">http://www.videolan.org/vlc/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR13">
              <label>13.</label>
              <mixed-citation publication-type="other">Robbel, Philipp. Personal Robots Group, MIT: The Kinect Sensor in Mobile Robots - Initial Experiments. Accessed 2011-05-19, [<ext-link xlink:href="http://www.youtube.com/watch?v=dRPEns8MS2o" ext-link-type="uri">http://www.youtube.com/watch?v=dRPEns8MS2o</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR14">
              <label>14.</label>
              <mixed-citation publication-type="other">Kreylos, Oliver. Kinect Hacking. Accessed 2011-05-19, [<ext-link xlink:href="http://idav.ucdavis.edu/~okreylos/ResDev/Kinect/" ext-link-type="uri">http://idav.ucdavis.edu/~okreylos/ResDev/Kinect/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR15">
              <label>15.</label>
              <mixed-citation publication-type="other">Microsoft Kinect Teardown. Accessed 2011-05-19, [<ext-link xlink:href="http://www.ifixit.com/Teardown/Microsoft-Kinect-Teardown/4066/" ext-link-type="uri">http://www.ifixit.com/Teardown/Microsoft-Kinect-Teardown/4066/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR16">
              <label>16.</label>
              <mixed-citation publication-type="other">Andrewe1. Kinect with Nightshot. Accessed 2011-05-19, [<ext-link xlink:href="http://www.youtube.com/watch?v=nvvQJxgykcU" ext-link-type="uri">http://www.youtube.com/watch?v=nvvQJxgykcU</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR17">
              <label>17.</label>
              <mixed-citation publication-type="other">RobbeOfficial. Kinect - sensor IR projection. Accessed 2011-05-19, [<ext-link xlink:href="http://www.youtube.com/watch?v=MlTf0yYQjSg" ext-link-type="uri">http://www.youtube.com/watch?v=MlTf0yYQjSg</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR18">
              <label>18.</label>
              <mixed-citation publication-type="other">Kreylos, Oliver. 3D video capture with Kinect. Accessed 2011-05-19, [<ext-link xlink:href="http://www.youtube.com/watch?v=7QrnwoO1-8A" ext-link-type="uri">http://www.youtube.com/watch?v=7QrnwoO1-8A</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR19">
              <label>19.</label>
              <mixed-citation publication-type="other">Kreylos, Oliver. Two Kinects, One Box. Accessed 2011-05-19, [<ext-link xlink:href="http://www.youtube.com/watch?v=5-w7UXCAUJE" ext-link-type="uri">http://www.youtube.com/watch?v=5-w7UXCAUJE</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR20">
              <label>20.</label>
              <mixed-citation publication-type="other">Coldewey, Devin. Kinect specifications. Accessed 2011-05-19, [<ext-link xlink:href="http://www.crunchgear.com/2010/06/29/kinect-specs-posted-640x480-at-30fps-two-players-maximum/" ext-link-type="uri">http://www.crunchgear.com/2010/06/29/kinect-specs-posted-640x480-at-30fps-two-players-maximum/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR21">
              <label>21.</label>
              <mixed-citation publication-type="other">PrimeSense. Accessed 2011-05-19, [<ext-link xlink:href="http://www.primesense.com/" ext-link-type="uri">http://www.primesense.com/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR22">
              <label>22.</label>
              <mixed-citation publication-type="other">avin2. SensorKinect. Accessed 2011-05-19, [<ext-link xlink:href="https://github.com/avin2/SensorKinect" ext-link-type="uri">https://github.com/avin2/SensorKinect</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR23">
              <label>23.</label>
              <mixed-citation publication-type="other">O'Steen, Ben. Code for tracking using the Kinect. Accessed 2011-05-19, [<ext-link xlink:href="https://github.com/benosteen/Kinect-tracking-code" ext-link-type="uri">https://github.com/benosteen/Kinect-tracking-code</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR24">
              <label>24.</label>
              <mixed-citation publication-type="other">Unilever Centre, Dept of Chemistry, Cambridge. Peter Murray-Rust Symposium: Symposium: Visions of a Semantic Molecular Future. Accessed 2011-05-19, [<ext-link xlink:href="http://www-ucc.ch.cam.ac.uk/news/visions-semantic-molecular-future-symposium-17th-january-2011" ext-link-type="uri">http://www-ucc.ch.cam.ac.uk/news/visions-semantic-molecular-future-symposium-17th-january-2011</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR25">
              <label>25.</label>
              <mixed-citation publication-type="other">Townsend, Joe. Chem4Word: Using speech to control input of chemistry in Microsoft Word. Accessed 2011-05-19, [<ext-link xlink:href="https://bitbucket.org/jat45/ami/downloads/AMI%20video%20v4.mp4" ext-link-type="uri">https://bitbucket.org/jat45/ami/downloads/AMI%20video%20v4.mp4</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR26">
              <label>26.</label>
              <mixed-citation publication-type="other">Chemistry Add-in for Microsoft Word. Accessed 2011-05-19, [<ext-link xlink:href="http://research.microsoft.com/en-us/projects/chem4word/" ext-link-type="uri">http://research.microsoft.com/en-us/projects/chem4word/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR27">
              <label>27.</label>
              <mixed-citation publication-type="other">IDBS. Accessed 2011-05-19, [<ext-link xlink:href="http://www.idbs.com/" ext-link-type="uri">http://www.idbs.com/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR28">
              <label>28.</label>
              <mixed-citation publication-type="other">Dial-A-Molecule. Accessed 2011-05-19, [<ext-link xlink:href="http://dialamolecule.chem.soton.ac.uk/site/" ext-link-type="uri">http://dialamolecule.chem.soton.ac.uk/site/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR29">
              <label>29.</label>
              <mixed-citation publication-type="other">JISC Grant Funding 10/09: Grants for the Virtual Research Environment - Rapid Innovation funding call. [<ext-link xlink:href="http://www.jisc.ac.uk/fundingopportunities/funding_calls/2009/10/vreri.aspx" ext-link-type="uri">http://www.jisc.ac.uk/fundingopportunities/funding_calls/2009/10/vreri.aspx</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR30">
              <label>30.</label>
              <mixed-citation publication-type="other">Smart Tea project. Accessed 2011-08-18, [<ext-link xlink:href="http://eprints.soton.ac.uk/2273/" ext-link-type="uri">http://eprints.soton.ac.uk/2273/</ext-link>]</mixed-citation>
            </ref>
            <ref id="CR31">
              <label>31.</label>
              <mixed-citation publication-type="other">The Timeline project. Accessed 2011-05-19, [<ext-link xlink:href="http://thetimelineproj.sourceforge.net" ext-link-type="uri">http://thetimelineproj.sourceforge.net</ext-link>]</mixed-citation>
            </ref>
          </ref-list>
        </ref-list>
        <app-group>
          <app id="App1">
            <sec id="Sec14">
              <title>Authors’ original submitted files for images</title>
              <p>Below are the links to the authors’ original submitted files for images.<supplementary-material content-type="local-data" id="MOESM1" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="jpeg" xlink:href="MediaObjects/13321_2011_227_MOESM1_ESM.jpeg" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 1</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM2" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="jpeg" xlink:href="MediaObjects/13321_2011_227_MOESM2_ESM.jpeg" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 2</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM3" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="png" xlink:href="MediaObjects/13321_2011_227_MOESM3_ESM.png" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 3</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM4" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="png" xlink:href="MediaObjects/13321_2011_227_MOESM4_ESM.png" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 4</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM5" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="png" xlink:href="MediaObjects/13321_2011_227_MOESM5_ESM.png" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 5</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM6" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="png" xlink:href="MediaObjects/13321_2011_227_MOESM6_ESM.png" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 6</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM7" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="png" xlink:href="MediaObjects/13321_2011_227_MOESM7_ESM.png" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 7</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM8" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="jpeg" xlink:href="MediaObjects/13321_2011_227_MOESM8_ESM.jpeg" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 8</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM9" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="jpeg" xlink:href="MediaObjects/13321_2011_227_MOESM9_ESM.jpeg" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 9</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM10" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="png" xlink:href="MediaObjects/13321_2011_227_MOESM10_ESM.png" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 10</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM11" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="png" xlink:href="MediaObjects/13321_2011_227_MOESM11_ESM.png" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 11</p></caption></media></supplementary-material><supplementary-material content-type="local-data" id="MOESM12" xlink:title="Authors’ original submitted files for images"><media mimetype="image" mime-subtype="jpeg" xlink:href="MediaObjects/13321_2011_227_MOESM12_ESM.jpeg" position="anchor"><caption xml:lang="en"><p>Authors’ original file for figure 12</p></caption></media></supplementary-material></p>
            </sec>
          </app>
        </app-group>
        <notes notes-type="Misc">
          <title>Competing interests</title>
          <p>The authors declare that they have no competing interests.</p>
        </notes>
        <notes notes-type="Misc">
          <title>Authors' contributions</title>
          <p>BJB was the project leader/manager. He did the project's bureaucracy and organisation, guided the project's direction, and did much of the investigations into the speech. He also wrote the paper.</p>
          <p>ALT did most of the programming for the Ami application, and investigated most of the software tools used, including investigations into video capture. He also worked on the Arduino development.</p>
          <p>MS, PM and SC worked on Arduino hardware and software development. They also added detail to the project's use-cases, and integrated the Arduino development into the Ami application. They also did demonstrations of the project.</p>
          <p>BOS worked on building the video capture for time-lapse video motion-snapshot monitoring. He also did the investigations into the use of the Arduino.</p>
          <p>SEA developed software for driving the RFID reader system and advised on application development, testing environments, and troubleshooting. He also helped with the brainstorming session.</p>
          <p>JAT configured the code management systems used by the project, and provided feedback, expertise and advice on the design of the application.</p>
          <p>PMR was the principal investigator on the project giving advice, guidance, encouragement and enthusiasm. He participated in the brainstorming session, reviewed project reports and both contributed-to and edited this paper.</p>
          <p>All authors have seen and approved the final paper.</p>
        </notes>
      </back>
    </article>
