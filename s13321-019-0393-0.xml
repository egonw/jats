<?xml version="1.0"?>
<!ENTITY % article SYSTEM "http://jats.nlm.nih.gov/archiving/1.2/JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="research-article" xml:lang="en">
      <front>
        <journal-meta>
          <journal-id journal-id-type="publisher-id">13321</journal-id>
          <journal-title-group>
            <journal-title>Journal of Cheminformatics</journal-title>
            <abbrev-journal-title abbrev-type="publisher">J Cheminform</abbrev-journal-title>
          </journal-title-group>
          <issn pub-type="epub">1758-2946</issn>
          <publisher>
            <publisher-name>Springer International Publishing</publisher-name>
            <publisher-loc>Cham</publisher-loc>
          </publisher>
        </journal-meta>
        <article-meta>
          <article-id pub-id-type="publisher-id">s13321-019-0393-0</article-id>
          <article-id pub-id-type="manuscript">393</article-id>
          <article-id pub-id-type="doi">10.1186/s13321-019-0393-0</article-id>
          <article-categories>
            <subj-group subj-group-type="heading">
              <subject>Research Article</subject>
            </subj-group>
            <subj-group subj-group-type="article-collection" specific-use="Regular">
              <subject>Big Data in Chemistry</subject>
            </subj-group>
          </article-categories>
          <title-group>
            <article-title xml:lang="en">Randomized SMILES strings improve the quality of molecular generative models</article-title>
          </title-group>
          <contrib-group>
            <contrib contrib-type="author" corresp="yes" id="Au1">
              <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-9860-2944</contrib-id>
              <name>
                <surname>Arús-Pous</surname>
                <given-names>Josep</given-names>
              </name>
              <address>
                <email>HIDDEN</email>
              </address>
              <xref ref-type="aff" rid="Aff1">1</xref>
              <xref ref-type="aff" rid="Aff3">3</xref>
              <xref ref-type="corresp" rid="IDs1332101903930_cor1">a</xref>
            </contrib>
            <contrib contrib-type="author" id="Au2">
              <name>
                <surname>Johansson</surname>
                <given-names>Simon Viet</given-names>
              </name>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author" id="Au3">
              <name>
                <surname>Prykhodko</surname>
                <given-names>Oleksii</given-names>
              </name>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author" id="Au4">
              <name>
                <surname>Bjerrum</surname>
                <given-names>Esben Jannik</given-names>
              </name>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author" id="Au5">
              <name>
                <surname>Tyrchan</surname>
                <given-names>Christian</given-names>
              </name>
              <xref ref-type="aff" rid="Aff2">2</xref>
            </contrib>
            <contrib contrib-type="author" id="Au6">
              <name>
                <surname>Reymond</surname>
                <given-names>Jean-Louis</given-names>
              </name>
              <xref ref-type="aff" rid="Aff3">3</xref>
            </contrib>
            <contrib contrib-type="author" id="Au7">
              <name>
                <surname>Chen</surname>
                <given-names>Hongming</given-names>
              </name>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <contrib contrib-type="author" id="Au8">
              <name>
                <surname>Engkvist</surname>
                <given-names>Ola</given-names>
              </name>
              <xref ref-type="aff" rid="Aff1">1</xref>
            </contrib>
            <aff id="Aff1">
              <label>1</label>
              <institution-wrap>
                <institution content-type="org-division">Hit Discovery, Discovery Sciences</institution>
                <institution content-type="org-name">R&amp;D, AstraZeneca Gothenburg</institution>
              </institution-wrap>
              <addr-line content-type="city">Mölndal</addr-line>
              <country country="SE">Sweden</country>
            </aff>
            <aff id="Aff2">
              <label>2</label>
              <institution-wrap>
                <institution content-type="org-division">Medicinal Chemistry, BioPharmaceuticals Early RIA</institution>
                <institution content-type="org-name">R&amp;D, AstraZeneca Gothenburg</institution>
              </institution-wrap>
              <addr-line content-type="city">Mölndal</addr-line>
              <country country="SE">Sweden</country>
            </aff>
            <aff id="Aff3">
              <label>3</label>
              <institution-wrap>
                <institution-id institution-id-type="ISNI">0000 0001 0726 5157</institution-id>
                <institution-id institution-id-type="GRID">grid.5734.5</institution-id>
                <institution content-type="org-division">Department of Chemistry and Biochemistry</institution>
                <institution content-type="org-name">University of Bern</institution>
              </institution-wrap>
              <addr-line content-type="street">Freiestrasse 3</addr-line>
              <addr-line content-type="postcode">3012</addr-line>
              <addr-line content-type="city">Bern</addr-line>
              <country country="CH">Switzerland</country>
            </aff>
          </contrib-group>
          <author-notes>
            <corresp id="IDs1332101903930_cor1">
              <label>a</label>
              <email>HIDDEN</email>
            </corresp>
          </author-notes>
          <pub-date date-type="pub" publication-format="electronic">
            <day>21</day>
            <month>11</month>
            <year>2019</year>
          </pub-date>
          <pub-date date-type="collection" publication-format="electronic">
            <month>12</month>
            <year>2019</year>
          </pub-date>
          <volume>11</volume>
          <issue seq="71">1</issue>
          <elocation-id>71</elocation-id>
          <history>
            <date date-type="registration">
              <day>9</day>
              <month>11</month>
              <year>2019</year>
            </date>
            <date date-type="received">
              <day>30</day>
              <month>7</month>
              <year>2019</year>
            </date>
            <date date-type="accepted">
              <day>9</day>
              <month>11</month>
              <year>2019</year>
            </date>
            <date date-type="online">
              <day>21</day>
              <month>11</month>
              <year>2019</year>
            </date>
          </history>
          <permissions>
            <copyright-statement content-type="compact">© The Author(s) 2019</copyright-statement>
            <copyright-year>2019</copyright-year>
            <copyright-holder>The Author(s)</copyright-holder>
            <license license-type="open-access" xlink:href="http://creativecommons.org/licenses/by/4.0/">
              <license-p><bold>Open Access</bold>This article is distributed under the terms of the Creative Commons Attribution 4.0 International License (<ext-link xlink:href="http://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">http://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons license, and indicate if changes were made. The Creative Commons Public Domain Dedication waiver (http://creativecommons.org/publicdomain/zero/1.0/) applies to the data made available in this article, unless otherwise stated.</license-p>
            </license>
          </permissions>
          <abstract xml:lang="en" id="Abs1">
            <title>Abstract</title>
            <p id="Par1">Recurrent Neural Networks (RNNs) trained with a set of molecules represented as unique (canonical) SMILES strings, have shown the capacity to create large chemical spaces of valid and meaningful structures. Herein we perform an extensive benchmark on models trained with subsets of GDB-13 of different sizes (1 million, 10,000 and 1000), with different SMILES variants (canonical, randomized and DeepSMILES), with two different recurrent cell types (LSTM and GRU) and with different hyperparameter combinations. To guide the benchmarks new metrics were developed that define how well a model has generalized the training set. The generated chemical space is evaluated with respect to its uniformity, closedness and completeness. Results show that models that use LSTM cells trained with 1 million randomized SMILES, a non-unique molecular string representation, are able to generalize to larger chemical spaces than the other approaches and they represent more accurately the target chemical space. Specifically, a model was trained with randomized SMILES that was able to generate almost all molecules from GDB-13 with a quasi-uniform probability. Models trained with smaller samples show an even bigger improvement when trained with randomized SMILES models. Additionally, models were trained on molecules obtained from ChEMBL and illustrate again that training with randomized SMILES lead to models having a better representation of the drug-like chemical space. Namely, the model trained with randomized SMILES was able to generate at least double the amount of unique molecules with the same distribution of properties comparing to one trained with canonical SMILES.<fig id="Figa" position="anchor"><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2019_393_Figa_HTML.png" position="anchor" id="MO1" /></fig></p>
          </abstract>
          <kwd-group xml:lang="en">
            <title>Keywords</title>
            <kwd>Deep learning</kwd>
            <kwd>Generative models</kwd>
            <kwd>SMILES</kwd>
            <kwd>Randomized SMILES</kwd>
            <kwd>Recurrent Neural Networks</kwd>
            <kwd>Chemical databases</kwd>
          </kwd-group>
          <funding-group>
            <award-group>
              <funding-source>
                <institution-wrap>
                  <institution>H2020 Marie Skłodowska-Curie Actions</institution>
                  <institution-id institution-id-type="doi" vocab="open-funder-registry">http://dx.doi.org/10.13039/100010665</institution-id>
                </institution-wrap>
              </funding-source>
              <award-id award-type="FundRef grant">676434</award-id>
              <principal-award-recipient>
                <name>
                  <surname>Arús-Pous</surname>
                  <given-names>Josep</given-names>
                </name>
              </principal-award-recipient>
            </award-group>
          </funding-group>
          <custom-meta-group>
            <custom-meta>
              <meta-name>publisher-imprint-name</meta-name>
              <meta-value>Springer</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>volume-issue-count</meta-name>
              <meta-value>1</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-article-count</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-pricelist-year</meta-name>
              <meta-value>2019</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-copyright-holder</meta-name>
              <meta-value>The Author(s)</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-copyright-year</meta-name>
              <meta-value>2019</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-contains-esm</meta-name>
              <meta-value>Yes</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-numbering-style</meta-name>
              <meta-value>Unnumbered</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-year</meta-name>
              <meta-value>2019</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-month</meta-name>
              <meta-value>11</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-registration-date-day</meta-name>
              <meta-value>9</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>toc-levels</meta-name>
              <meta-value>0</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>volume-type</meta-name>
              <meta-value>Regular</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-product</meta-name>
              <meta-value>ArchiveJournal</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>numbering-style</meta-name>
              <meta-value>Unnumbered</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-collection-editor</meta-name>
              <meta-value>Igor Tetko,</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-grants-type</meta-name>
              <meta-value>OpenChoice</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>metadata-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>abstract-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bodypdf-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bodyhtml-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>bibliography-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>esm-grant</meta-name>
              <meta-value>OpenAccess</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>online-first</meta-name>
              <meta-value>false</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>pdf-file-reference</meta-name>
              <meta-value>BodyRef/PDF/13321_2019_Article_393.pdf</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>target-type</meta-name>
              <meta-value>OnlinePDF</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>issue-type</meta-name>
              <meta-value>Regular</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>article-type</meta-name>
              <meta-value>OriginalPaper</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-primary</meta-name>
              <meta-value>Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Computer Applications in Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Documentation and Information in Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Theoretical and Computational Chemistry</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-secondary</meta-name>
              <meta-value>Computational Biology/Bioinformatics</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>journal-subject-collection</meta-name>
              <meta-value>Chemistry and Materials Science</meta-value>
            </custom-meta>
            <custom-meta>
              <meta-name>open-access</meta-name>
              <meta-value>true</meta-value>
            </custom-meta>
          </custom-meta-group>
        </article-meta>
      </front>
      <body>
        <sec id="Sec1">
          <title>Introduction</title>
          <p id="Par18">Exploring the unknown chemical space in a meaningful way has always been one of the major objectives in drug discovery. Given the fact that the drug-like chemical space is enormous (the lower estimation is 10<sup>23</sup> molecules) [<xref ref-type="bibr" rid="CR1">1</xref>], it cannot be easily searched. One of the most interesting attempts to understand the chemical space is the GDB project [<xref ref-type="bibr" rid="CR2">2</xref>], which encompasses a set of databases that combinatorially enumerate large parts of the small molecule fragment-like chemical space. Currently there are databases that enumerate most fragment-like molecules with up to 13 (975 million molecules) [<xref ref-type="bibr" rid="CR3">3</xref>] and 17 (166 billion molecules) [<xref ref-type="bibr" rid="CR4">4</xref>] heavy atoms. Another approach, GDB4c [<xref ref-type="bibr" rid="CR5">5</xref>], enumerates ring systems up to four rings both in 2D (circa one million ring systems) and 3D (more than 6 million structures). Although managing billion-sized databases is computationally challenging, the enumerative approach has proven useful to study the entire small drug-like molecular chemical space in an unbiased way [<xref ref-type="bibr" rid="CR6">6</xref>].</p>
          <p id="Par19">In the last 2 years molecular deep generative models have emerged as a powerful method to generate chemical space [<xref ref-type="bibr" rid="CR7">7</xref>] and obtain optimized compounds [<xref ref-type="bibr" rid="CR8">8</xref>]. Given a training set with molecules (generally a database such as ChEMBL [<xref ref-type="bibr" rid="CR9">9</xref>]), these models learn how to create molecules that are similar but not the same as those in the training set, thus spanning a bigger chemical space than that of the training data. Either after or during training, the probability of generating molecules with specific properties can be altered with techniques such as reinforcement [<xref ref-type="bibr" rid="CR8">8</xref>] or transfer learning [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR10">10</xref>]. Multiple architectures have been reported in literature: the first one is Recurrent Neural Networks (RNNs) [<xref ref-type="bibr" rid="CR7">7</xref>], but also others such as Variational AutoEncoders (VAEs) [<xref ref-type="bibr" rid="CR11">11</xref>], Generative Adversarial Networks (GANs) [<xref ref-type="bibr" rid="CR12">12</xref>, <xref ref-type="bibr" rid="CR13">13</xref>], etc. [<xref ref-type="bibr" rid="CR14">14</xref>]. Due to its simplicity, in most published research the format representing molecules is the canonical SMILES notation [<xref ref-type="bibr" rid="CR15">15</xref>], a string representation unique to each molecule. Nevertheless, models that use the molecular graph directly are starting to gain interest [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR17">17</xref>].</p>
          <p id="Par20">Notwithstanding the popularity of RNNs, the idiosyncrasies of the canonical SMILES syntax can lead to training biased models [<xref ref-type="bibr" rid="CR18">18</xref>]. Specifically, models trained with a set of one million molecules from GDB-13 have a higher probability of generating molecules with fewer rings. Additionally, the canonical SMILES representation can generate substantially different strings for molecules that are very similar, thus making some of them more difficult to sample. To prove this, these models were sampled with replacement 2 billion times and at most only 68% of GDB-13 could be obtained from a theoretical maximum of 87%. This maximum would be from sampling with replacement the same number of times from a theoretical ideal model that has a uniform probability of obtaining each molecule from GDB-13, thus obtaining the least possible biased output domain.</p>
          <p id="Par21">We performed an extensive benchmark of RNN models trained with SMILES obtained from GDB-13 whilst exploring an array of architectural changes. First and foremost, models were trained with three different variants of the SMILES notation. One of them is the commonly used canonical SMILES, another one are randomized SMILES (also known as enumerated SMILES), which have been used as a data amplification technique and are shown to generate more diversity in some model architectures [<xref ref-type="bibr" rid="CR19">19</xref>–<xref ref-type="bibr" rid="CR21">21</xref>]. The third one is DeepSMILES [<xref ref-type="bibr" rid="CR22">22</xref>], a recently published modification of the canonical SMILES syntax. Secondly, models were trained with decreasing training set sizes (1,000,000, 10,000 and 1000 molecules) to explore the data amplification capabilities of randomizes SMILES. Thirdly, the two most used recurrent cell architectures were compared: long short-term memory (LSTM) [<xref ref-type="bibr" rid="CR23">23</xref>] and Gated Recurrent Unit (GRU) [<xref ref-type="bibr" rid="CR24">24</xref>]. GRU cells are widely used as a drop-in replacement of LSTM cells with an noticeable speed improvement, but it has been shown that in some tasks they perform worse [<xref ref-type="bibr" rid="CR25">25</xref>]. Fourthly, regularization techniques such as dropout [<xref ref-type="bibr" rid="CR26">26</xref>] in conjunction with different batch sizes were also tested and their impact on the generated chemical space assessed. All of the benchmarks were supported by a set of metrics that evaluate the uniformity, completeness and closedness of the generated chemical space. With this approach, the generated chemical space is treated as a generalization of the training set to the entire GDB-13 and the chemical space exploration capability of the models can be assessed. Finally, to demonstrate how the same methodology can be used to train models that generate real-world drug-like compounds, models were trained with a subset of the ChEMBL [<xref ref-type="bibr" rid="CR9">9</xref>] database.</p>
        </sec>
        <sec id="Sec2" sec-type="methods">
          <title>Methods</title>
          <sec id="Sec3">
            <title>Randomized SMILES strings</title>
            <p id="Par22">To obtain canonical SMILES the atoms in a given molecule have to be uniquely and consistently numbered. In the case of RDKit this is done by using a modified version of the Morgan algorithm [<xref ref-type="bibr" rid="CR27">27</xref>, <xref ref-type="bibr" rid="CR28">28</xref>]. The SMILES generation algorithm is then able to traverse the molecular graph always in the same way (Fig. <xref rid="Fig1" ref-type="fig">1</xref>a). Some atom orderings can lead to overly complicated SMILES strings and that is why RDKit has some built-in fixes that alter atom order on-the-fly. They prevent strange combinations, such as prioritizing traversing sidechains before the ring atoms, and are by default active.<fig id="Fig1"><label>Fig. 1</label><caption xml:lang="en"><p>Traversal of the molecular graph of Aspirin using three methods: <bold>a</bold> the canonical ordering of the molecule; <bold>b</bold> atom order randomization without RDKit restrictions; <bold>c</bold> Atom order randomization with RDKit restrictions of the same atom ordering as <bold>b</bold>. Atom ordering is specified with a number ranking from 1 to 13 for each atom and the arrows show the molecular graph traversal process. Notice that the atom ordering is altered in <bold>c</bold>, prioritizing the sidechains (red arrows) when traversing a ring and preventing SMILES substrings like <monospace>c1cc(c(cc1))</monospace></p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2019_393_Fig1_HTML.png" id="MO2" /></fig></p>
            <p id="Par23">One easy way of obtaining randomized SMILES is by randomizing atom ordering. This does not alter how the algorithm traverses the graph (i.e., depth-first in the case of RDKit), but changes the starting point and in what order the branching paths are selected. With this approach, theoretically, at most <inline-formula id="IEq1"><alternatives><mml:math id="IEq1_Math"><mml:mrow><mml:mi>n</mml:mi><mml:mo>!</mml:mo></mml:mrow></mml:math><tex-math id="IEq1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n!$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq1.gif" /></alternatives></inline-formula> different SMILES can be generated on a molecule with <inline-formula id="IEq2"><alternatives><mml:math id="IEq2_Math"><mml:mi>n</mml:mi></mml:math><tex-math id="IEq2_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq2.gif" /></alternatives></inline-formula> heavy atoms, yet the resulting number of different combinations ends up being much lower. The two different variants of randomized SMILES used here (Fig. <xref rid="Fig1" ref-type="fig">1</xref>b, c) only change on the application of the RDKit fixes. This makes the unrestricted version a superset of the restricted one, which includes the SMILES that are disallowed in the regular restricted version.</p>
          </sec>
          <sec id="Sec4">
            <title>RNNs trained with SMILES</title>
            <sec id="Sec5">
              <title>Pre-processing SMILES strings</title>
              <p id="Par24">SMILES strings of all variants need to be tokenized to be understood by the model. Tokenization was performed on a character basis with the exception of some specific cases. The first are the “<monospace>Cl</monospace>” and “<monospace>Br</monospace>” atoms, which are two-character tokens. Second are atoms with explicit hydrogens or charge, which are between brackets (e.g., “<monospace>[nH]</monospace>” or “<monospace>[O-]</monospace>”). Third, ring tokens can be higher than 9 in which case the SMILES syntax represents the number prepended with the “<monospace>%</monospace>” character (e.g., “<monospace>%10</monospace>”). These rules apply to all SMILES variants used in this research. Lastly, the begin token “<monospace>^</monospace>” was prepended and the end token “<monospace>$</monospace>” appended to all SMILES strings. The tokenization process was performed independently for each database and yielded vocabulary sizes of 26 in GDB-13 and 31 in ChEMBL. When training the DeepSMILES models, the official implementation [<xref ref-type="bibr" rid="CR22">22</xref>] was used to convert the SMILES.</p>
            </sec>
            <sec id="Sec6">
              <title>Architecture</title>
              <p id="Par25">The model architecture used is similar to the one used in [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>, <xref ref-type="bibr" rid="CR18">18</xref>] and is illustrated in Fig. <xref rid="Fig2" ref-type="fig">2</xref>. The training set sequences are pre-processed, and for each training epoch the entire training set is shuffled and subdivided in <inline-formula id="IEq3"><alternatives><mml:math id="IEq3_Math"><mml:mi>b</mml:mi></mml:math><tex-math id="IEq3_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq3.gif" /></alternatives></inline-formula> batches. The encoded SMILES strings of each batch are fed token by token to an embedding layer of <inline-formula id="IEq4"><alternatives><mml:math id="IEq4_Math"><mml:mi>m</mml:mi></mml:math><tex-math id="IEq4_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq4.gif" /></alternatives></inline-formula> dimensions, followed by <inline-formula id="IEq5"><alternatives><mml:math id="IEq5_Math"><mml:mi>l</mml:mi></mml:math><tex-math id="IEq5_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq5.gif" /></alternatives></inline-formula> layers of LSTM [<xref ref-type="bibr" rid="CR23">23</xref>] /GRU [<xref ref-type="bibr" rid="CR24">24</xref>] cell size <inline-formula id="IEq6"><alternatives><mml:math id="IEq6_Math"><mml:mi>w</mml:mi></mml:math><tex-math id="IEq6_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq6.gif" /></alternatives></inline-formula>. To prevent squeezing the encoded input, the embedding dimensions should be <inline-formula id="IEq7"><alternatives><mml:math id="IEq7_Math"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≤</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:math><tex-math id="IEq7_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m \le w$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq7.gif" /></alternatives></inline-formula>. Between the inner RNN layers there can be dropout layers [<xref ref-type="bibr" rid="CR26">26</xref>] with a probability <inline-formula id="IEq8"><alternatives><mml:math id="IEq8_Math"><mml:mi>d</mml:mi></mml:math><tex-math id="IEq8_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq8.gif" /></alternatives></inline-formula>. The output from the cells is squeezed to the vocabulary size <inline-formula id="IEq9"><alternatives><mml:math id="IEq9_Math"><mml:mi>v</mml:mi></mml:math><tex-math id="IEq9_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$v$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq9.gif" /></alternatives></inline-formula> by a linear transformation layer and a softmax is performed to obtain the probabilities of sampling each token in the next position. This is repeated for each token in the entire sequence.<fig id="Fig2"><label>Fig. 2</label><caption xml:lang="en"><p>Architecture of the RNN model used in this study. For every step <inline-formula id="IEq39"><alternatives><mml:math id="IEq39_Math"><mml:mi>i</mml:mi></mml:math><tex-math id="IEq39_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq39.gif" /></alternatives></inline-formula>, input one-hot encoded token <inline-formula id="IEq40"><alternatives><mml:math id="IEq40_Math"><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq40_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{i}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq40.gif" /></alternatives></inline-formula> goes through an embedding layer of size <inline-formula id="IEq41"><alternatives><mml:math id="IEq41_Math"><mml:mrow><mml:mi>m</mml:mi><mml:mo>≤</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:math><tex-math id="IEq41_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$m \le w$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq41.gif" /></alternatives></inline-formula>, followed by <inline-formula id="IEq42"><alternatives><mml:math id="IEq42_Math"><mml:mrow><mml:mi>l</mml:mi><mml:mo>></mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><tex-math id="IEq42_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$l > 0$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq42.gif" /></alternatives></inline-formula> GRU/LSTM layers of size <inline-formula id="IEq43"><alternatives><mml:math id="IEq43_Math"><mml:mi>w</mml:mi></mml:math><tex-math id="IEq43_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq43.gif" /></alternatives></inline-formula> with dropout in-between and then a linear layer that has dimensionality <inline-formula id="IEq44"><alternatives><mml:math id="IEq44_Math"><mml:mi>w</mml:mi></mml:math><tex-math id="IEq44_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq44.gif" /></alternatives></inline-formula> and the size of the vocabulary. Lastly a softmax is used to obtain the token probability distribution <inline-formula id="IEq45"><alternatives><mml:math id="IEq45_Math"><mml:msub><mml:mi>Y</mml:mi><mml:mrow><mml:mi mathvariant="italic">ij</mml:mi></mml:mrow></mml:msub></mml:math><tex-math id="IEq45_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y_{ij}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq45.gif" /></alternatives></inline-formula>. <inline-formula id="IEq46"><alternatives><mml:math id="IEq46_Math"><mml:msub><mml:mi>H</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq46_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H_{i}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq46.gif" /></alternatives></inline-formula> symbolizes the input hidden state matrix at step <inline-formula id="IEq47"><alternatives><mml:math id="IEq47_Math"><mml:mi>i</mml:mi></mml:math><tex-math id="IEq47_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq47.gif" /></alternatives></inline-formula></p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2019_393_Fig2_HTML.png" id="MO3" /></fig></p>
            </sec>
            <sec id="Sec7">
              <title>Training a model</title>
              <p id="Par26">Following [<xref ref-type="bibr" rid="CR18">18</xref>], all models have two sets: a training and a validation set. The validation set holds molecules that are in the target chemical space but are not used for training the model. Depending on the training set different splits can be made. In Table <xref rid="Tab1" ref-type="table">1</xref> is shown the size of the training and validation sets for each of the benchmarks (see Additional file <xref ref-type="supplementary-material" rid="MOESM1">1</xref>: Methods S1 for more information on how the databases were filtered). In the case of models trained with randomized SMILES, a new sample of randomized SMILES of the same molecules are used for the training and validation set for each epoch. These training set files are created beforehand and the model uses a different file for each epoch. For example, a model trained with one million molecules for 300 epochs will have approximately 300 million different randomized SMILES, although the number is generally lower because some SMILES are more commonly sampled than others.<table-wrap id="Tab1"><label>Table 1</label><caption xml:lang="en"><p>Training and validation set sizes for the different benchmarks</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Model</p></th><th align="left"><p>Training set size</p></th><th align="left"><p>Validation set size</p></th></tr></thead><tbody><tr><td align="left"><p>GDB-13 1M</p></td><td char="." align="char"><p>1,000,000</p></td><td char="." align="char"><p>10,000</p></td></tr><tr><td align="left"><p>GDB-13 10K</p></td><td char="." align="char"><p>10,000</p></td><td char="." align="char"><p>1000</p></td></tr><tr><td align="left"><p>GDB-13 1K</p></td><td char="." align="char"><p>1000</p></td><td char="." align="char"><p>1000</p></td></tr><tr><td align="left"><p>ChEMBL</p></td><td char="." align="char"><p>1,483,943</p></td><td char="." align="char"><p>78,102</p></td></tr></tbody></table><table-wrap-foot><p>Notice that depending on the expected size of the target chemical space and the total amount of molecules, different ratios have been used</p></table-wrap-foot></table-wrap></p>
              <p id="Par27">During each epoch the training set is shuffled and minibatches of size <inline-formula id="IEq10"><alternatives><mml:math id="IEq10_Math"><mml:mi>b</mml:mi></mml:math><tex-math id="IEq10_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq10.gif" /></alternatives></inline-formula> are created. These batches are in the form of a matrix with a row for each encoded SMILES string and appended with end tokens as padding. The “teacher’s forcing” approach is used in training, which means that the correct token is always input in the next step, regardless of the prediction from the model [<xref ref-type="bibr" rid="CR29">29</xref>]. The loss function to minimize by the model is the average negative log-likelihood (NLL) of the entire batch of tokenized SMILES strings. Given <inline-formula id="IEq11"><alternatives><mml:math id="IEq11_Math"><mml:msub><mml:mi>X</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq11_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$X_{i}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq11.gif" /></alternatives></inline-formula> and <inline-formula id="IEq12"><alternatives><mml:math id="IEq12_Math"><mml:msub><mml:mi>x</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:math><tex-math id="IEq12_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq12.gif" /></alternatives></inline-formula> as the sampled and expected token at previous step <inline-formula id="IEq13"><alternatives><mml:math id="IEq13_Math"><mml:mrow><mml:mi>i</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><tex-math id="IEq13_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$i \ge 0$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq13.gif" /></alternatives></inline-formula> respectively and the current time step <inline-formula id="IEq14"><alternatives><mml:math id="IEq14_Math"><mml:mrow><mml:mi>T</mml:mi><mml:mo>≥</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><tex-math id="IEq14_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T \ge 0$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq14.gif" /></alternatives></inline-formula>, the partial NLL of a SMILES string is computed as:<disp-formula id="Equa"><alternatives><mml:math display="block" id="Equa_Math"><mml:mrow><mml:mi>J</mml:mi><mml:mfenced close=")" open="("><mml:mi>T</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mfenced close=")" open="("><mml:mi>T</mml:mi></mml:mfenced><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mo>ln</mml:mo><mml:mi>P</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mn>0</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>o</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:munderover><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>t</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mi>T</mml:mi></mml:munderover><mml:mo>ln</mml:mo><mml:mi>P</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mi>t</mml:mi></mml:msub><mml:mrow><mml:mo stretchy="false">|</mml:mo></mml:mrow><mml:msub><mml:mi>X</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mi>X</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>x</mml:mi><mml:mn>1</mml:mn></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="Equa_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$J\left( T \right) = NLL\left( T \right) = - \ln P\left( {X_{0} = x_{o} } \right) - \mathop \sum \limits_{t = 1}^{T} \ln P\left( {X_{t} = x_{t} |X_{t - 1} = x_{t - 1} \ldots X_{1} = x_{1} } \right)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_Equa.gif" /></alternatives></disp-formula></p>
              <p id="Par28">To prevent instability during training, the computed gradients are updated so that the norm is <inline-formula id="IEq15"><alternatives><mml:math id="IEq15_Math"><mml:mrow><mml:mn>1.0</mml:mn></mml:mrow></mml:math><tex-math id="IEq15_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.0$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq15.gif" /></alternatives></inline-formula>. When performing a forward-pass on a batch, the model does not apply any mask to already finished sequences. This makes the model run slightly faster because no masks are computed and, as the padding token is the end of sequence, it does not affect the quality of the training process. All weight matrices are initialized from a uniform random distribution <inline-formula id="IEq16"><alternatives><mml:math id="IEq16_Math"><mml:mrow><mml:mi mathvariant="script">U</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mo>-</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msqrt><mml:mo>,</mml:mo><mml:msqrt><mml:mrow><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mi>w</mml:mi></mml:mrow></mml:msqrt></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="IEq16_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{U}}\left( { - \sqrt {1/w} ,\sqrt {1/w} } \right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq16.gif" /></alternatives></inline-formula>. The learning decay strategy is based on a custom metric calculated at each epoch (UC-JSD) and is discussed in section “Adaptive learning rate decay strategy” of the Additional file <xref ref-type="supplementary-material" rid="MOESM1">1</xref>: Methods S2.</p>
            </sec>
          </sec>
          <sec id="Sec8">
            <title>Benchmark</title>
            <p id="Par29">The models were optimized over the hyperparameter combinations shown in Table <xref rid="Tab2" ref-type="table">2</xref>. The two models with bigger training set sizes were optimized for fewer parameters, as training times were much longer. On the other hand, the two smaller models allowed for more optimizations, as each epoch took few seconds to calculate. After the first benchmark, GRU cells were dropped because of their consistently lower performance.<table-wrap id="Tab2"><label>Table 2</label><caption xml:lang="en"><p>Hyperparameter combinations used in the grid search</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Model</p></th><th align="left"><p>l</p></th><th align="left"><p>w</p></th><th align="left"><p>d</p></th><th align="left"><p>b</p></th><th align="left"><p>RNN</p></th></tr></thead><tbody><tr><td align="left"><p>GDB-13 1M</p></td><td align="left"><p>3</p></td><td align="left"><p>512</p></td><td align="left"><p>0, 25, 50</p></td><td align="left"><p>64, 128, 256, 512</p></td><td align="left"><p>GRU, LSTM</p></td></tr><tr><td align="left"><p>GDB-13 10K</p></td><td align="left"><p>2, 3, 4</p></td><td align="left"><p>256, 384, 512</p></td><td align="left"><p>0, 25, 50</p></td><td align="left"><p>8, 16, 32</p></td><td align="left"><p>LSTM</p></td></tr><tr><td align="left"><p>GDB-13 1K</p></td><td align="left"><p>2, 3, 4</p></td><td align="left"><p>128, 192, 256</p></td><td align="left"><p>0, 25, 50</p></td><td align="left"><p>4, 8, 16</p></td><td align="left"><p>LSTM</p></td></tr><tr><td align="left"><p>ChEMBL</p></td><td align="left"><p>3</p></td><td align="left"><p>512</p></td><td align="left"><p>0, 25, 50</p></td><td align="left"><p>64, 128, 256, 512</p></td><td align="left"><p>LSTM</p></td></tr></tbody></table><table-wrap-foot><p>Number of layers (<italic>l</italic>), dimensions of the RNN layers (<italic>w</italic>), dropout rate % (<italic>d</italic>), batch size (<italic>b</italic>), RNN cell type (<italic>RNN</italic>)</p></table-wrap-foot></table-wrap></p>
            <p id="Par30">After each hyperparameter optimization, the best epoch was chosen as follows. A smoothing window function size 4 was applied to the UC-JSD calculated on each epoch, selecting the epoch with the lowest UC-JSD (see next section) as the best one.</p>
            <sec id="Sec9">
              <title>UC-JSD—a metric for generative models</title>
              <p id="Par31">The metric used for the benchmark is derived from previous research [<xref ref-type="bibr" rid="CR18">18</xref>]. There, it was hypothesized that the best models are those in which the validation, training and sampled set NLL distributions are uniform and equivalent. The Jensen–Shannon Divergence (JSD) measures the divergence between a set of probability distributions [<xref ref-type="bibr" rid="CR30">30</xref>] and is calculated as:<disp-formula id="Equ1"><label>1</label><alternatives><mml:math display="block" id="Equ1_Math"><mml:mrow><mml:mi>J</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo>=</mml:mo><mml:mi>H</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>·</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mrow></mml:mfenced><mml:mo>-</mml:mo><mml:munder><mml:mo movablelimits="false">∑</mml:mo><mml:mrow><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:munder><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mi>H</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub></mml:mfenced></mml:mrow></mml:math><tex-math id="Equ1_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$JSD = H\left( {\mathop \sum \limits_{d \in D} \alpha_{i} \cdot d_{i} } \right) - \mathop \sum \limits_{d \in D} \alpha_{i} H\left( {d_{i} } \right)$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_Equ1.gif" /></alternatives></disp-formula>where <inline-formula id="IEq17"><alternatives><mml:math id="IEq17_Math"><mml:mrow><mml:mi>H</mml:mi><mml:mfenced close=")" open="("><mml:mi>d</mml:mi></mml:mfenced></mml:mrow></mml:math><tex-math id="IEq17_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\left( d \right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq17.gif" /></alternatives></inline-formula> is the Shannon entropy of a given probability distribution and <inline-formula id="IEq18"><alternatives><mml:math id="IEq18_Math"><mml:mrow><mml:mo>∀</mml:mo><mml:mi>d</mml:mi><mml:mo>∈</mml:mo><mml:mi>D</mml:mi><mml:mo>;</mml:mo><mml:mn>0</mml:mn><mml:mo>&lt;</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>&lt;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><tex-math id="IEq18_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\forall d \in D; 0 &lt; \alpha_{d} &lt; 1$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq18.gif" /></alternatives></inline-formula> and <inline-formula id="IEq19"><alternatives><mml:math id="IEq19_Math"><mml:mrow><mml:mo>∑</mml:mo><mml:msub><mml:mi>α</mml:mi><mml:mi>d</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math><tex-math id="IEq19_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sum \alpha_{d} = 1$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq19.gif" /></alternatives></inline-formula> are weights. The <inline-formula id="IEq20"><alternatives><mml:math id="IEq20_Math"><mml:mrow><mml:mi>J</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><tex-math id="IEq20_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$JSD \to 0$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq20.gif" /></alternatives></inline-formula> when <inline-formula id="IEq21"><alternatives><mml:math id="IEq21_Math"><mml:mrow><mml:mo>∀</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>∈</mml:mo><mml:mi mathvariant="script">D</mml:mi><mml:mo>;</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mi>d</mml:mi><mml:mi>j</mml:mi></mml:msub><mml:mo>;</mml:mo><mml:mi>i</mml:mi><mml:mo>≠</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:math><tex-math id="IEq21_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\forall d_{i} \in {\mathcal{D}};d_{i} = d_{j} ;i \ne j$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq21.gif" /></alternatives></inline-formula>, which does not explicitly consider uniformity (i.e., the distributions can be non-uniform but equal).</p>
              <p id="Par32">To solve this issue the Uniformity–Completeness JSD (UC-JSD) was designed. Instead of binning the raw distribution NLLs, each of the NLLs is used as it is. Given the three NLL vectors for the sampled, training and validation sets of the same size <inline-formula id="IEq22"><alternatives><mml:math id="IEq22_Math"><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:mi>L</mml:mi><mml:mi>S</mml:mi><mml:mo>=</mml:mo><mml:mfenced close="}" open="{" separators=""><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">validation</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">training</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi mathvariant="italic">sampled</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:mrow></mml:math><tex-math id="IEq22_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$NLLS = \left\{ {NLL_{validation} ,NLL_{training} ,NLL_{sampled} } \right\}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq22.gif" /></alternatives></inline-formula> and <inline-formula id="IEq23"><alternatives><mml:math id="IEq23_Math"><mml:mrow><mml:msub><mml:mi>α</mml:mi><mml:mi>i</mml:mi></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo stretchy="false">/</mml:mo><mml:mn>3</mml:mn></mml:mrow></mml:math><tex-math id="IEq23_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha_{i} = 1/3$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq23.gif" /></alternatives></inline-formula>, the values in each vector are divided by the total sum, giving a probability distribution with as many values as items in the vector. Then (Eq. <xref rid="Equ1" ref-type="disp-formula">1</xref> is used to calculate the JSD between the three distributions. Notice that, since the model is sampled randomly, the <inline-formula id="IEq24"><alternatives><mml:math id="IEq24_Math"><mml:mrow><mml:mi>U</mml:mi><mml:msub><mml:mi>C</mml:mi><mml:mrow><mml:mi mathvariant="italic">JSD</mml:mi></mml:mrow></mml:msub><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><tex-math id="IEq24_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$UC_{JSD} \to 0$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq24.gif" /></alternatives></inline-formula> either in the highly unlikely case that all the samples have molecules with the same NLL or all three distributions are uniform, and the model is complete.</p>
            </sec>
            <sec id="Sec10">
              <title>Sampling the best epoch of a model</title>
              <p id="Par33">The main objective of sampling a model is to assess the properties of the output domain. Namely, in the case of GDB-13, the uniformity (equal probability of sampling), completeness (sampling all molecules from GDB-13) and closedness (only molecules from GDB-13 are sampled) are to be assessed. To ease the evaluation of the models, three ratios representing the three properties were defined.</p>
              <p id="Par34">Given a sample with replacement size <inline-formula id="IEq25"><alternatives><mml:math id="IEq25_Math"><mml:mi>k</mml:mi></mml:math><tex-math id="IEq25_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq25.gif" /></alternatives></inline-formula>, the <inline-formula id="IEq26"><alternatives><mml:math id="IEq26_Math"><mml:mrow><mml:mi mathvariant="italic">valid</mml:mi></mml:mrow></mml:math><tex-math id="IEq26_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$valid$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq26.gif" /></alternatives></inline-formula> (SMILES parsed correctly with repeats), <inline-formula id="IEq27"><alternatives><mml:math id="IEq27_Math"><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:math><tex-math id="IEq27_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$in$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq27.gif" /></alternatives></inline-formula> (SMILES with repeats in GDB-13), <inline-formula id="IEq28"><alternatives><mml:math id="IEq28_Math"><mml:mrow><mml:mi mathvariant="italic">unique</mml:mi></mml:mrow></mml:math><tex-math id="IEq28_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$unique$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq28.gif" /></alternatives></inline-formula> (sampled unique canonical SMILES in GDB-13) subsets are obtained. Both <inline-formula id="IEq29"><alternatives><mml:math id="IEq29_Math"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi mathvariant="italic">valid</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mi mathvariant="italic">valid</mml:mi></mml:mrow></mml:mfenced><mml:mi>k</mml:mi></mml:mfrac></mml:mrow></mml:math><tex-math id="IEq29_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ratio_{valid} = \frac{{\left| {valid} \right|}}{k}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq29.gif" /></alternatives></inline-formula> and <inline-formula id="IEq30"><alternatives><mml:math id="IEq30_Math"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:mfenced><mml:mi>k</mml:mi></mml:mfrac></mml:mrow></mml:math><tex-math id="IEq30_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ratio_{in} = \frac{{\left| {in} \right|}}{k}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq30.gif" /></alternatives></inline-formula> are relative to the entire sample but <inline-formula id="IEq31"><alternatives><mml:math id="IEq31_Math"><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi mathvariant="italic">unique</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mfrac><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mi mathvariant="italic">unique</mml:mi></mml:mrow></mml:mfenced><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mi>G</mml:mi><mml:mi>D</mml:mi><mml:mi>B</mml:mi><mml:mn>13</mml:mn></mml:mrow></mml:mfenced></mml:mfrac></mml:mrow></mml:math><tex-math id="IEq31_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$ratio_{unique} = \frac{{\left| {unique} \right|}}{{\left| {GDB13} \right|}}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq31.gif" /></alternatives></inline-formula> is relative to <inline-formula id="IEq32"><alternatives><mml:math id="IEq32_Math"><mml:mrow><mml:mi>φ</mml:mi><mml:mfenced close=")" open="("><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:math><tex-math id="IEq32_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi \left( k \right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq32.gif" /></alternatives></inline-formula>, which represents the expected ratio of different molecules obtainable when a sample size <inline-formula id="IEq33"><alternatives><mml:math id="IEq33_Math"><mml:mi>k</mml:mi></mml:math><tex-math id="IEq33_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq33.gif" /></alternatives></inline-formula> with replacement is performed on a model that generates uniformly all molecules from and only from GDB-13 (ideal model) [<xref ref-type="bibr" rid="CR18">18</xref>] (i.e., <inline-formula id="IEq34"><alternatives><mml:math id="IEq34_Math"><mml:mrow><mml:mi>φ</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mn>0.8712</mml:mn></mml:mrow></mml:math><tex-math id="IEq34_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\varphi \left( {2 \cdot 10^{9} } \right) = 0.8712$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq34.gif" /></alternatives></inline-formula>). This allows to define the ratios as:<disp-formula id="Equb"><alternatives><mml:math display="block" id="Equb_Math"><mml:mrow><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi mathvariant="italic">unique</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mfenced close=")" open="("><mml:mi>k</mml:mi></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="Equb_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$completeness = \frac{{ratio_{unique} }}{\varphi \left( k \right)}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_Equb.gif" /></alternatives></disp-formula><disp-formula id="Equc"><alternatives><mml:math display="block" id="Equc_Math"><mml:mrow><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi mathvariant="italic">unique</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mi>φ</mml:mi><mml:mfenced close=")" open="("><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:mfenced></mml:mfenced></mml:mrow></mml:mfrac></mml:mrow></mml:math><tex-math id="Equc_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$uniformity = \frac{{ratio_{unique} }}{{\varphi \left( {\left| {in} \right|} \right)}}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_Equc.gif" /></alternatives></disp-formula><disp-formula id="Equd"><alternatives><mml:math display="block" id="Equd_Math"><mml:mrow><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>=</mml:mo><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:msub><mml:mi>o</mml:mi><mml:mrow><mml:mi mathvariant="italic">in</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:math><tex-math id="Equd_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$closedness = ratio_{in}$$\end{document}</tex-math><graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_Equd.gif" /></alternatives></disp-formula></p>
              <p id="Par35">Also, the <inline-formula id="IEq35"><alternatives><mml:math id="IEq35_Math"><mml:mrow><mml:mi>U</mml:mi><mml:mi>C</mml:mi><mml:mi>C</mml:mi><mml:mo>=</mml:mo><mml:mi>c</mml:mi><mml:mi>o</mml:mi><mml:mi>m</mml:mi><mml:mi>p</mml:mi><mml:mi>l</mml:mi><mml:mi>e</mml:mi><mml:mi>t</mml:mi><mml:mi>e</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi><mml:mo>·</mml:mo><mml:mi>u</mml:mi><mml:mi>n</mml:mi><mml:mi>i</mml:mi><mml:mi>f</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mi>m</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>y</mml:mi><mml:mo>·</mml:mo><mml:mi>c</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>d</mml:mi><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:math><tex-math id="IEq35_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$UCC = completeness \cdot uniformity \cdot closedness$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq35.gif" /></alternatives></inline-formula> was also defined as a unified score that heavily penalizes models that have low scores. See the Additional file <xref ref-type="supplementary-material" rid="MOESM1">1</xref>: Methods S2–4 for further details on how the benchmark was performed.</p>
            </sec>
          </sec>
          <sec id="Sec11">
            <title>Technical notes</title>
            <p id="Par36">All the software was coded in Python 3.6.8. The models were coded using the PyTorch 1.0.1 library [<xref ref-type="bibr" rid="CR31">31</xref>]. Unless specified, the chemistry library used throughout is RDKit 2019_03_01 [<xref ref-type="bibr" rid="CR32">32</xref>] and for all the big data processing Spark 2.4.3 [<xref ref-type="bibr" rid="CR33">33</xref>] was used. All plots were made with matplotlib 3.0.3 [<xref ref-type="bibr" rid="CR34">34</xref>] and seaborn 0.9.0 [<xref ref-type="bibr" rid="CR35">35</xref>]. The GPU hardware used to train and sample the models were Nvidia Tesla V100 (Volta) 16 GB VRAM cards using CUDA 9.1 on stable driver 390.30. The MOSES and FCD benchmarks were calculated using the code provided in (<ext-link xlink:href="https://github.com/molecularsets/moses" ext-link-type="uri">https://github.com/molecularsets/moses</ext-link>).</p>
          </sec>
        </sec>
        <sec id="Sec12" sec-type="results">
          <title>Results</title>
          <sec id="Sec13">
            <title>Optimizing generative models with 1 million SMILES from GDB-13</title>
            <sec id="Sec14">
              <title>Canonical vs. randomized SMILES</title>
              <p id="Par37">Hyperparameter optimizations of the three main SMILES variants (canonical, randomized restricted and randomized unrestricted) were performed on models trained with 1 million molecules randomly sampled from GDB-13 (Table <xref rid="Tab2" ref-type="table">2</xref>). A <inline-formula id="IEq36"><alternatives><mml:math id="IEq36_Math"><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>·</mml:mo><mml:msup><mml:mn>10</mml:mn><mml:mn>9</mml:mn></mml:msup></mml:mrow></mml:math><tex-math id="IEq36_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$k = 2 \cdot 10^{9}$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq36.gif" /></alternatives></inline-formula> SMILES sample was performed on the best epoch for each of the models trained in the benchmark (see Additional file <xref ref-type="supplementary-material" rid="MOESM1">1</xref>: Methods S1). Results show (Table <xref rid="Tab3" ref-type="table">3</xref>, Additional file <xref ref-type="supplementary-material" rid="MOESM2">2</xref>: Figure S4 for the best hyperparameter combinations for each SMILES type and Additional file <xref ref-type="supplementary-material" rid="MOESM3">3</xref>: Table S1 for all results) that the randomized variants greatly outperform canonical SMILES. The best canonical SMILES model was only able to enumerate 72.8% of GDB-13 compared to the 83.0% of the restricted randomized SMILES (Fig. <xref rid="Fig3" ref-type="fig">3</xref>). All three metrics, uniformity, completeness and closedness are much higher and show that the restricted randomized models are theoretically able to generate most of GDB-13 with uniform probability. This can be further seen in Fig. <xref rid="Fig4" ref-type="fig">4</xref>b, where the NLL distribution of a sample of molecules from the GDB-13 randomized SMILES models is centered at <inline-formula id="IEq37"><alternatives><mml:math id="IEq37_Math"><mml:mrow><mml:mi>N</mml:mi><mml:mi>L</mml:mi><mml:msub><mml:mi>L</mml:mi><mml:mrow><mml:mi>G</mml:mi><mml:mi>D</mml:mi><mml:mi>B</mml:mi><mml:mn>13</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>-</mml:mo><mml:mi>l</mml:mi><mml:mi>n</mml:mi><mml:mfenced close=")" open="(" separators=""><mml:mfrac><mml:mn>1</mml:mn><mml:mfenced close="|" open="|" separators=""><mml:mrow><mml:mi>G</mml:mi><mml:mi>D</mml:mi><mml:mi>B</mml:mi><mml:mn>13</mml:mn></mml:mrow></mml:mfenced></mml:mfrac></mml:mfenced><mml:mo>=</mml:mo><mml:mn>20.6</mml:mn></mml:mrow></mml:math><tex-math id="IEq37_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$NLL_{GDB13} = - ln\left( {\frac{1}{{\left| {GDB13} \right|}}} \right) = 20.6$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq37.gif" /></alternatives></inline-formula> and is much narrower than that of the canonical variant model.<table-wrap id="Tab3"><label>Table 3</label><caption xml:lang="en"><p>Best models trained on subsets of GDB-13 after the hyperparameter optimization</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>Set</p></th><th align="left"><p>SMILES</p></th><th align="left"><p>Time</p></th><th align="left"><p>% GDB-13</p></th><th align="left"><p>Valid</p></th><th align="left"><p>Unif</p></th><th align="left"><p>Comp</p></th><th align="left"><p>Closed</p></th><th align="left"><p>UCC</p></th></tr></thead><tbody><tr><td align="left" rowspan="8"><p>1M</p></td><td align="left"><p>Canonical</p></td><td align="left"><p>4:08</p></td><td char="." align="char"><p>72.8</p></td><td char="." align="char"><p>0.994</p></td><td char="." align="char"><p>0.879</p></td><td char="." align="char"><p><italic>0.836</italic></p></td><td char="." align="char"><p>0.861</p></td><td char="." align="char"><p>0.633</p></td></tr><tr><td align="left"><p>Rand. unr.</p></td><td align="left"><p>31:47</p></td><td char="." align="char"><p>80.9</p></td><td char="." align="char"><p>0.995</p></td><td char="." align="char"><p>0.970</p></td><td char="." align="char"><p>0.929</p></td><td char="." align="char"><p>0.876</p></td><td char="." align="char"><p>0.790</p></td></tr><tr><td align="left"><p>Rand. unr. no DA</p></td><td align="left"><p>1:37</p></td><td char="." align="char"><p>77.0</p></td><td char="." align="char"><p>0.987</p></td><td char="." align="char"><p>0.957</p></td><td char="." align="char"><p>0.795</p></td><td char="." align="char"><p>0.883</p></td><td char="." align="char"><p>0.672</p></td></tr><tr><td align="left"><p><italic>Rand. rest.</italic></p></td><td align="left"><p><italic>7:19</italic></p></td><td char="." align="char"><p><italic>83.0</italic></p></td><td char="." align="char"><p><italic>0.999</italic></p></td><td char="." align="char"><p><italic>0.977</italic></p></td><td char="." align="char"><p><italic>0.953</italic></p></td><td char="." align="char"><p><italic>0.925</italic></p></td><td char="." align="char"><p><italic>0.860</italic></p></td></tr><tr><td align="left"><p>Rand. rest. no DA</p></td><td align="left"><p>1:21</p></td><td char="." align="char"><p>78.2</p></td><td char="." align="char"><p>0.992</p></td><td char="." align="char"><p>0.957</p></td><td char="." align="char"><p>0.829</p></td><td char="." align="char"><p>0.898</p></td><td char="." align="char"><p>0.712</p></td></tr><tr><td align="left"><p>DS branch</p></td><td align="left"><p>1:33</p></td><td char="." align="char"><p>72.1</p></td><td char="." align="char"><p>0.987</p></td><td char="." align="char"><p>0.881</p></td><td char="." align="char"><p>0.828</p></td><td char="." align="char"><p>0.834</p></td><td char="." align="char"><p>0.608</p></td></tr><tr><td align="left"><p>DS rings</p></td><td align="left"><p>1:11</p></td><td char="." align="char"><p>68.6</p></td><td char="." align="char"><p>0.979</p></td><td char="." align="char"><p>0.852</p></td><td char="." align="char"><p>0.788</p></td><td char="." align="char"><p>0.798</p></td><td char="." align="char"><p>0.535</p></td></tr><tr><td align="left"><p>DS both</p></td><td align="left"><p>1:05</p></td><td char="." align="char"><p>68.4</p></td><td char="." align="char"><p>0.979</p></td><td char="." align="char"><p>0.851</p></td><td char="." align="char"><p>0.785</p></td><td char="." align="char"><p>0.796</p></td><td char="." align="char"><p>0.532</p></td></tr><tr><td align="left" rowspan="2"><p>10K</p></td><td align="left"><p>Canonical</p></td><td align="left"><p>0:04</p></td><td char="." align="char"><p>38.8</p></td><td char="." align="char"><p>0.905</p></td><td char="." align="char"><p>0.666</p></td><td char="." align="char"><p>0.445</p></td><td char="." align="char"><p>0.426</p></td><td char="." align="char"><p>0.126</p></td></tr><tr><td align="left"><p><italic>Rand. rest.</italic></p></td><td align="left"><p><italic>0:36</italic></p></td><td char="." align="char"><p><italic>62.3</italic></p></td><td char="." align="char"><p><italic>0.974</italic></p></td><td char="." align="char"><p><italic>0.882</italic></p></td><td char="." align="char"><p><italic>0.715</italic></p></td><td char="." align="char"><p><italic>0.598</italic></p></td><td char="." align="char"><p><italic>0.377</italic></p></td></tr><tr><td align="left" rowspan="2"><p>1K</p></td><td align="left"><p>Canonical</p></td><td align="left"><p>0:01</p></td><td char="." align="char"><p>14.5</p></td><td char="." align="char"><p>0.504</p></td><td char="." align="char"><p>0.611</p></td><td char="." align="char"><p>0.167</p></td><td char="." align="char"><p>0.133</p></td><td char="." align="char"><p>0.014</p></td></tr><tr><td align="left"><p><italic>Rand. rest.</italic></p></td><td align="left"><p><italic>0:04</italic></p></td><td char="." align="char"><p><italic>34.1</italic></p></td><td char="." align="char"><p><italic>0.812</italic></p></td><td char="." align="char"><p><italic>0.790</italic></p></td><td char="." align="char"><p><italic>0.392</italic></p></td><td char="." align="char"><p><italic>0.276</italic></p></td><td char="." align="char"><p><italic>0.085</italic></p></td></tr></tbody></table><table-wrap-foot><p>See “<xref rid="Sec2" ref-type="sec">Methods</xref>” section for a description of the ratios</p><p>Best result for each training set size are indicated in italics</p><p><italic>Set</italic> Benchmark training set size, <italic>SMILES</italic> SMILES variant, including randomized variants with and without data augmentation (DA), <italic>Time</italic> training time up in hh:mm, <italic>% GDB-13</italic> Percent of unique molecules from GDB-13 generated in a 2 billion sample with replacement, <italic>Valid</italic> valid SMILES, <italic>Unif</italic> uniformity ratio, <italic>Comp</italic> completeness ratio, <italic>Closed</italic> closedness ratio, <italic>UCC</italic> UCC ratio</p></table-wrap-foot></table-wrap><fig id="Fig3"><label>Fig. 3</label><caption xml:lang="en"><p>Plot illustrating the percent of GDB-13 sampled alongside the sample size of the ideal model (blue) and the best of the canonical (yellow), randomized restricted (green) and randomized unrestricted (orange) models. Notice that the ideal model is always an upper bound and eventually (<inline-formula id="IEq48"><alternatives><mml:math id="IEq48_Math"><mml:mrow><mml:mi>n</mml:mi><mml:mo>∼</mml:mo><mml:mn>21</mml:mn><mml:mi>B</mml:mi></mml:mrow></mml:math><tex-math id="IEq48_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n \sim 21B$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq48.gif" /></alternatives></inline-formula>) would sample the entire GDB-13. The trained models would reach the same point much later</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2019_393_Fig3_HTML.png" id="MO9" /></fig><fig id="Fig4"><label>Fig. 4</label><caption xml:lang="en"><p>Histograms of different statistics from the randomized SMILES models. <bold>a</bold> Kernel Density Estimates (KDEs) of the number of randomized SMILES per molecule from a sample of 1 million molecules from GDB-13. The plot has the x axis cut at 5000, but the unrestricted randomized variant plot has outliers until 15,000. <bold>b</bold> KDEs of the molecule negative log-likelihood (NLL) for each molecule (summing the probabilities for each randomized SMILES) for the same sample of 1 million molecules from GDB-13. The plot is also cropped between range <inline-formula id="IEq49"><alternatives><mml:math id="IEq49_Math"><mml:mfenced close=")" open="(" separators=""><mml:mrow><mml:mn>19</mml:mn><mml:mo>,</mml:mo><mml:mn>25</mml:mn></mml:mrow></mml:mfenced></mml:math><tex-math id="IEq49_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( {19,25} \right)$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq49.gif" /></alternatives></inline-formula>. <bold>c</bold> Histograms between the NLL of all the restricted randomized SMILES of two molecules from GDB-13</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2019_393_Fig4_HTML.png" id="MO10" /></fig></p>
              <p id="Par38">Comparing the two variants of randomized SMILES, models trained with both variants have a similarly uniform output domain (Fig. <xref rid="Fig4" ref-type="fig">4</xref>b), but models trained with restricted randomized variant have a more complete and more closed domain than those trained with the unrestricted variant. The output domain of the ideal randomized SMILES models would comprise all possible SMILES strings of any given variant possible to be generated from all molecules in GDB-13. This contrasts with the canonical model, in which the output domain is one SMILES per molecule. Each molecule has a different number of SMILES strings, depending on its topology, although only a few (generally highly cyclic or branched molecules) have numbers above 1000 (Fig. <xref rid="Fig4" ref-type="fig">4</xref>a). Knowing that the training objective is to obtain a uniform posterior distribution, it would be expected that molecules with more randomized SMILES should have a higher probability of being sampled than those that have fewer. However, this is never the case as models trained with randomized SMILES have a much more uniform posterior probability distribution than those trained with canonical SMILES (Fig. <xref rid="Fig4" ref-type="fig">4</xref>b). The model naturally learns to prioritize some SMILES in molecules with a large number of possible SMILES, and to have a more uniform distribution among all possible SMILES on molecules that have less. This can be seen in Fig. <xref rid="Fig4" ref-type="fig">4</xref>c, where two molecules have the same NLL, but one (blue) has six times the number of possible SMILES than the other (orange).</p>
              <p id="Par39">Models trained with randomized SMILES without data augmentation (the same SMILES strings each epoch) were also benchmarked. Results show (Table <xref rid="Tab3" ref-type="table">3</xref>, Additional file <xref ref-type="supplementary-material" rid="MOESM2">2</xref>: Figure S4 for the best hyperparameter combinations for each SMILES type and Additional file <xref ref-type="supplementary-material" rid="MOESM3">3</xref>: Table S1 for all results) that they perform better than the models trained with canonical SMILES but worse than those with data augmentation. This indicates that not using the canonical representation constraint makes better models, but also that data augmentation has a positive impact on the training process.</p>
              <p id="Par40">DeepSMILES is a SMILES syntax variant that alters the syntax and changes how rings and branching are represented [<xref ref-type="bibr" rid="CR22">22</xref>]. Three different forms of DeepSMILES were explored: one with the new ring syntax, another with the new branching syntax and a last one with both changes. Results show (Table <xref rid="Tab3" ref-type="table">3</xref>, Additional file <xref ref-type="supplementary-material" rid="MOESM3">3</xref>: Table S1 complete) that the performance is consistently lower than using normal canonical SMILES. The validity is generally 1–3% lower than in canonical SMILES, possibly indicating that the model has difficulties in learning the basics of the syntax.</p>
              <p id="Par41">The hyperparameter optimization also gives some hints on how dropout, batch size and cell type affect the training process, although it varies for each SMILES variant. Plots for each hyperparameter compared to the four ratios and the training time were drawn (Additional file <xref ref-type="supplementary-material" rid="MOESM2">2</xref>: Figure S1) and show that adding dropout only makes canonical SMILES models better. The model improves its completeness, but at the expense of closedness, meaning that it generates more molecules from GDB-13 at the expense of making more mistakes. On the other hand, larger batch sizes have generally a positive impact in models of all SMILES variants and at the same time make training processes much faster. But the most interesting result is that the best models for all SMILES variants use LSTM cells. Moreover, even though the training time per epoch of the GRU cells is lower, LSTM models are able to converge in fewer epochs.</p>
              <p id="Par42">Similarity maps for the randomized SMILES were also plotted (Additional file <xref ref-type="supplementary-material" rid="MOESM2">2</xref>: Figure S2) and confirm that models trained with randomized SMILES are able to generate mostly all molecules from GDB-13 with uniform probability. Only molecules on the left tip of the half-moon (highly cyclic) are slightly more difficult to generate, but this is because they have extremely complicated SMILES with uncommon tokens and ring closures. Additionally, maps colored by the number of SMILES per molecule were created and show that most of the molecules that have more randomized SMILES are the same as those that are difficult to sample in the canonical models.</p>
            </sec>
            <sec id="Sec15">
              <title>UC-JSD can be used to predict the best models</title>
              <p id="Par43">The previous benchmark employed an adaptive learning rate strategy (see Additional file <xref ref-type="supplementary-material" rid="MOESM1">1</xref>: Methods S2) that uses the UC-JSD metric to evaluate the quality of the models and trigger a learning rate change. Moreover, the same metric was used to select the best epochs to perform a sample for each model. Plotting the UC-JSD against UCC shows a strong correlation in all three SMILES variants (Fig. <xref rid="Fig5" ref-type="fig">5</xref>). It is important to notice that the UC-JSD values should not be compared between models, as the output domain is different. This result shows that it is not necessary anymore to sample all models, but only the one that has the best UC-JSD. That is why for all future benchmarks only the model with the lowest UC-JSD is sampled. Moreover, the GRU cells have not shown any improvement whatsoever compared to the LSTM cells (Additional file <xref ref-type="supplementary-material" rid="MOESM2">2</xref>: Figure S1) and the unrestricted randomized SMILES variant performs worse than the restricted variant. Henceforth, only the restricted variant of randomized SMILES and LSTM cells will be used for the next benchmarks.<fig id="Fig5"><label>Fig. 5</label><caption xml:lang="en"><p>Linear regression plots between the UC-JSD and the UCC ratio. <bold>a</bold> Canonical SMILES <inline-formula id="IEq50"><alternatives><mml:math id="IEq50_Math"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.931</mml:mn></mml:mrow></mml:math><tex-math id="IEq50_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{2} = 0.931$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq50.gif" /></alternatives></inline-formula>. <bold>b</bold> Restricted randomized SMILES <inline-formula id="IEq51"><alternatives><mml:math id="IEq51_Math"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.856</mml:mn></mml:mrow></mml:math><tex-math id="IEq51_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{2} = 0.856$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq51.gif" /></alternatives></inline-formula>. <bold>c</bold> Unrestricted randomized SMILES <inline-formula id="IEq52"><alternatives><mml:math id="IEq52_Math"><mml:mrow><mml:msup><mml:mi>R</mml:mi><mml:mn>2</mml:mn></mml:msup><mml:mo>=</mml:mo><mml:mn>0.885</mml:mn></mml:mrow></mml:math><tex-math id="IEq52_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{2} = 0.885$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq52.gif" /></alternatives></inline-formula></p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2019_393_Fig5_HTML.png" id="MO11" /></fig></p>
            </sec>
          </sec>
          <sec id="Sec16">
            <title>Training generative models with smaller training sets</title>
            <p id="Par44">To further show the data augmentation capabilities of randomized SMILES, two models were trained with 1000 and 10,000 molecules respectively, randomly obtained from GDB-13. Hyperparameter optimization was modified to accommodate smaller training sets and, as models were faster to train, different network topologies were tested (Table <xref rid="Tab2" ref-type="table">2</xref>). When the training sets are so small, models are often unable to learn the syntax properly and thus generate more invalid structures. The model using 1000 molecules was the most affected by this problem, with some models not even reaching 50% validity. This impacts the accuracy of the UC-JSD, because all molecules tend to have a sampling probability <inline-formula id="IEq38"><alternatives><mml:math id="IEq38_Math"><mml:mrow><mml:mi>p</mml:mi><mml:mo stretchy="false">→</mml:mo><mml:mn>0</mml:mn></mml:mrow></mml:math><tex-math id="IEq38_TeX">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym}
				\usepackage{amsfonts}
				\usepackage{amssymb}
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p \to 0$$\end{document}</tex-math><inline-graphic specific-use="web" mime-subtype="GIF" xlink:href="13321_2019_393_Article_IEq38.gif" /></alternatives></inline-formula>. This makes the UC-JSD have low values because all molecules have very similar probability. For this reason, only models that had more than 50% valid SMILES were considered.</p>
            <p id="Par45">Results show (Table <xref rid="Tab3" ref-type="table">3</xref>, Additional file <xref ref-type="supplementary-material" rid="MOESM3">3</xref>: Table S1 complete) that models trained with randomized SMILES have better performance than those trained with canonical SMILES. In the models trained with 1000 molecules, those with canonical SMILES are at most able to generate up to 70% valid SMILES, although the best model was only able to generate 50% valid SMILES. Moreover, the completeness ratio of the best model is only 0.1325, meaning that most of the SMILES generated are not part of GDB-13: they correspond to molecules containing features excluded from GDB-13 (e.g. strained rings, unstable functional groups, wrong tautomer). Alternatively, the models trained with randomized SMILES show a much better behavior. Most models learn how to generate SMILES strings correctly (validity over 80%), completeness is much higher (0.2757) and their posterior distribution is more uniform. This is further illustrated with the fact that randomized SMILES models generate up to 34.11% of unique GDB-13 molecules and canonical models only 14.54%.</p>
            <p id="Par46">Models trained with a bigger sample of 10,000 molecules show similar trends but have much better performance in both cases. In this case, a model trained with randomized SMILES is able to uniquely generate 62.29% of GDB-13 while only training with less than 0.001% of the database, whereas a canonical SMILES model is only able to generate 38.77%. Closedness is much better in both models: canonical SMILES models have at most 0.4262, whereas randomized SMILES models up to 0.5978. Lastly, a large number of SMILES generated are not included in GDB-13, meaning that the model, even though generating valid molecules, does not fully learn the specific idiosyncrasies of GDB-13 molecules and generates valid molecules that break some condition.</p>
          </sec>
          <sec id="Sec17">
            <title>Improving the existing ChEMBL priors with randomized SMILES</title>
            <p id="Par47">The same benchmark study was also performed on models with a drug-like training set from ChEMBL (see Additional file <xref ref-type="supplementary-material" rid="MOESM1">1</xref>: Methods S1 for more information on how the training set was obtained). A different and reduced set of hyperparameter values were used due to long training times (Table <xref rid="Tab2" ref-type="table">2</xref>). The best models for both the canonical and restricted randomized SMILES benchmarks were obtained using the same procedure as before and a 2 billion sample was performed. Results show (Table <xref rid="Tab4" ref-type="table">4</xref>, extended results Additional file <xref ref-type="supplementary-material" rid="MOESM3">3</xref>: Table S2) that the output domain of the canonical model is much smaller than that of the randomized SMILES model. Specifically, the randomized SMILES model can generate at least twice the number of different molecules than the canonical. Nevertheless, the Fréchet ChemNet Distance (FCD) [<xref ref-type="bibr" rid="CR36">36</xref>] between the validation set and a sampled set of 75,000 SMILES is lower on the canonical SMILES model. This could mean that the molecules generated by the canonical model have more similar properties than ChEMBL molecules, but it could also mean that the canonical model overfits and generates molecules that are similar to the training set given that the validation set and the training set are biased the same way (i.e., they are both obtained from a biased sample of the entire drug-like chemical space).<table-wrap id="Tab4"><label>Table 4</label><caption xml:lang="en"><p>Best models from the ChEMBL benchmark for both SMILES variants</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left"><p>SMILES</p></th><th align="left"><p>Time</p></th><th align="left"><p>% Valid</p></th><th align="left"><p>% Unique</p></th><th align="left"><p>FCD</p></th></tr></thead><tbody><tr><td align="left"><p>Canonical</p></td><td align="left"><p>131:32</p></td><td char="." align="char"><p>98.26</p></td><td char="." align="char"><p>34.67</p></td><td char="." align="char"><p>0.0712</p></td></tr><tr><td align="left"><p>Rest. Random.</p></td><td align="left"><p>84:22</p></td><td char="." align="char"><p>98.33</p></td><td char="." align="char"><p>64.09</p></td><td char="." align="char"><p>0.1265</p></td></tr></tbody></table><table-wrap-foot><p><italic>SMILES</italic> SMILES variant, <italic>Time</italic> time used to train the model hhh:mm, <italic>% Valid</italic> Percent of valid molecules, <italic>% Unique</italic> Percent of unique molecules in a 2 billion SMILES sample, <italic>Fréchet</italic> ChemNet Distance (FCD) between the validation and a sample of 75,000 molecules (FCD)</p></table-wrap-foot></table-wrap></p>
            <p id="Par48">To prove that the molecules sampled from the randomized SMILES model are at least as diverse as those in the canonical, several physicochemical properties and metrics (as used in the MOSES benchmark [<xref ref-type="bibr" rid="CR37">37</xref>]), such as molecular weight, <italic>logP</italic>, Synthetic Accessibility Score (SA) [<xref ref-type="bibr" rid="CR38">38</xref>], Quantitative Estimate of Drug-likeness Score (QED) [<xref ref-type="bibr" rid="CR39">39</xref>], Natural-Product likeness score (NP) [<xref ref-type="bibr" rid="CR40">40</xref>] and Internal Diversity (cross-molecule Tanimoto similarity on ECFP4) were calculated for a sample of the training, validation, randomized SMILES model and canonical SMILES model (Additional file <xref ref-type="supplementary-material" rid="MOESM2">2</xref>: Figure S3). All of the plots are nearly identical, showing that there is no clear difference between molecules in any of the four sets. Additionally, molecule NLL plots for the same four samples were calculated for both models (Fig. <xref rid="Fig6" ref-type="fig">6</xref>) and show that the canonical model greatly overfits the training and validation sets compared to the randomized SMILES model, which has mostly the same distribution for both sets. When comparing the two samples, the canonical model has much lower probabilities of generating most of the molecules generated by the randomized SMILES model, but not the opposite. The randomized SMILES model is able to generate the canonical SMILES model molecules with higher likelihood than average, implying that the output domain of the canonical SMILES model is a subset of the randomized SMILES model output domain.<fig id="Fig6"><label>Fig. 6</label><caption xml:lang="en"><p>Kernel Density Estimates (KDEs) of the Molecule negative log-likelihoods (NLLs) of the ChEMBL models for the canonical SMILES variant (left) and the randomized SMILES variant (right). Each line symbolizes a different subset of 50,000 molecules from: Training set (green), validation set (orange), randomized SMILES model (blue) and canonical SMILES model (yellow). Notice that the Molecule NLLs for the randomized SMILES model (right) are obtained from the sum of all the probabilities of the randomized SMILES for each of the 50,000 molecules (adding up to 320 million randomized SMILES), whereas those from the canonical model are the canonical SMILES of the 50,000 molecules</p></caption><graphic specific-use="web" mime-subtype="PNG" xlink:href="MediaObjects/13321_2019_393_Fig6_HTML.png" id="MO12" /></fig></p>
          </sec>
        </sec>
        <sec id="Sec18" sec-type="discussion">
          <title>Discussion</title>
          <sec id="Sec19">
            <title>Why are randomized SMILES better?</title>
            <p id="Par49">A SMILES molecular generative model learns by finding patterns in the SMILES strings from the training set with the goal of generalizing a model that is able to obtain all the SMILES in the training set with the highest possible probability. The procedure is exactly the same with any SMILES variant, the only thing that changes is the string representation of each molecule and, in the case of randomized SMILES, the number of different representations each molecule has. When the canonical representation is used, the model learns to generate one linear representation of each molecule obtained through a canonicalization algorithm. This means that the model must learn not only to generate valid SMILES strings, but also to generate those in the canonical form. As shown in “<xref rid="Sec2" ref-type="sec">Methods</xref>” section (Fig. <xref rid="Fig1" ref-type="fig">1</xref>), the canonicalization algorithm in RDKit does not only traverse the molecule using a fixed ordering, but also adds some restrictions on how to traverse rings. Moreover, models tend to see the same patterns repeatedly, leading to premature overfitting (Fig. <xref rid="Fig6" ref-type="fig">6</xref>). Alternatively, randomized SMILES models do not have the canonical form limitation and can learn the SMILES syntax without restriction. When no data augmentation is used, randomized SMILES still perform substantially better than canonical SMILES. Additionally, heavy regularization with dropout in canonical models gave a better overall performance, but opposite results were obtained with randomized SMILES, showing that using different randomized SMILES on each epoch also serves as a regularization technique.</p>
            <p id="Par50">Another way of understanding why randomized variants are better is to draw a parallel with image classification models. For example, when an image classification model is trained to predict whether an image depicts a cat, the model performance can be improved with a training set that has examples of cats from all the possible angles and not always a front picture. This is not always easy to obtain in image predictive models, but in the case of molecular generative models it is extremely easy to generate <italic>snapshots</italic> of the same molecule from different angles (i.e., different ways of writing the SMILES string). This allows models to better learn the constraints of the training set chemical space (i.e., in the case of GDB-13: heteroatom ratios, allowed functional groups, etc.). Nevertheless, for each molecule there is a different number of randomized SMILES (Fig. <xref rid="Fig4" ref-type="fig">4</xref>), thus possibly generating a bias towards the molecules that have more representations. None was detected in this study possibly because larger and highly branched molecules, which tend to have more combinations, are also generally more difficult to sample and can, in effect, counteract the bias (Fig. <xref rid="Fig4" ref-type="fig">4</xref>c). Lastly, the restricted variant of randomized SMILES performed best, indicating that restricting the randomized SMILES algorithm makes the model generalize better. For example, the unrestricted randomized SMILES can represent the phenyl ring of aspirin (Fig. <xref rid="Fig1" ref-type="fig">1</xref>) in a much more convoluted way “<monospace>c1cc(c(cc1)</monospace>”, something that would be impossible in the restricted variant. Finding variants that perform even better should be a future research goal in this field.</p>
          </sec>
          <sec id="Sec20">
            <title>Understanding diversity in molecular generative models</title>
            <p id="Par51">A challenge in Computer-Assisted Drug Design (CADD) is to computationally generate or evaluate molecules that fit a given set of constraints. This process is not devoid of error: for instance, an inactive molecule can be predicted as active (false positive) or an active one can be predicted as inactive (false negative). From a drug design perspective, false positives are more damaging due to the economic impact a wrong prediction can have. False negatives do not impact as directly but are important nonetheless: the next <italic>blockbuster</italic> could be any molecule wrongly skipped by computational solutions.</p>
            <p id="Par52">Analogously, the same problem can be brought to generative models. A model can generate molecules that are outside of the target chemical space (false positives) or the output domain can collapse [<xref ref-type="bibr" rid="CR41">41</xref>] not being able to generate a chunk of the expected chemical space (false negatives). This is very easy to assess when training models that generate the GDB-13 chemical space. First, any molecule sampled not included in GDB-13 is a false positive (closedness). It was previously shown [<xref ref-type="bibr" rid="CR18">18</xref>] that the vast majority of these clearly do not comply to one or more conditions of GDB-13, such as having invalid functional groups, molecular graph or not being the most stable tautomer. Alternatively, any molecule comprised in GDB-13 not possible to being sampled (i.e. very high NLL) becomes a false negative (completeness). In both cases this means that the model is not able to learn correctly the rules used in the enumeration process. When canonical and randomized SMILES models are compared, the results show that randomized SMILES models perform substantially better in both properties (Table <xref rid="Tab3" ref-type="table">3</xref>). They are able to learn better the filters used in enumerating GDB-13 and thus prevent the generation of incorrect molecules and at the same time generate more difficult outliers that comply with GDB-13 (Additional file <xref ref-type="supplementary-material" rid="MOESM2">2</xref>: Figure S1, left tip of the NLL similarity maps).</p>
            <p id="Par53">Training molecules on unknown target chemical spaces is a much more difficult task. Compared to GDB-13, where the generated molecules can be checked whether or not they form part of it, there is no way of bounding the limits (if there are any) of a drug-like space. This makes benchmarking models much more complex. For instance, a model could generate an extremely diverse set of molecules, most of which are completely unrelated to the training set chemical space, compared to a model that generates less diverse and fewer molecules that are more akin to the training set chemical space. As it is unknown which is the target chemical space, assessing which is the best model is impossible. For this reason, some methods were published [<xref ref-type="bibr" rid="CR37">37</xref>, <xref ref-type="bibr" rid="CR42">42</xref>] that aggregate a set of metrics to obtain a better overview of the output domain of the model. Unfortunately, they compare the models with a test set split from the training set and this tends to benefit models that overfit. Additionally, they are not able to measure mode collapse the same way as with the GDB-13 benchmark, as can be seen in [<xref ref-type="bibr" rid="CR43">43</xref>]. This means that models may seem extremely diverse when being sampled a few thousand times, but when being sampled more times the same molecules start appearing repeatedly. This is the case with the ChEMBL models trained here. We know that the drug-like chemical space is huge [<xref ref-type="bibr" rid="CR44">44</xref>], so we would not expect the model to collapse early. Results show that those trained with randomized SMILES have a much larger output domain (at least double) than those trained with canonical SMILES. Moreover, sets of molecules generated are physicochemically almost indistinguishable (Additional file <xref ref-type="supplementary-material" rid="MOESM2">2</xref>: Figure S3) from sets generated from the canonical SMILES model, meaning that they are from the same chemical space. This showcases how models trained with randomized SMILES are able to represent chemical spaces that are more complete and at least as closed as those generated by models using canonical SMILES.</p>
          </sec>
          <sec id="Sec21">
            <title>SMILES generative models as action-based generative models</title>
            <p id="Par54">The most common way of understanding SMILES generative models is as grammar-based models that generate SMILES strings that are similar to the training set [<xref ref-type="bibr" rid="CR7">7</xref>, <xref ref-type="bibr" rid="CR8">8</xref>], akin to language generative models [<xref ref-type="bibr" rid="CR45">45</xref>]. Alternatively, SMILES generative models can be also understood as action (or policy)-based graph generative models [<xref ref-type="bibr" rid="CR16">16</xref>, <xref ref-type="bibr" rid="CR46">46</xref>] in which a molecular graph is built stepwise. In these models, each step an action is chosen (“<italic>add atom</italic>”, “<italic>add bond</italic>”, etc.) and is sampled from a fixed or varying size action space (or policy) that has all possible actions (even invalid ones) alongside the probability of each happening. A parallelism can be partially drawn for SMILES generative models: the vocabulary is the action space in which atom tokens (“<monospace>C</monospace>”, “<monospace>N</monospace>”, “<monospace>[O-]</monospace>”, etc.) are “<italic>add atom”</italic> actions, the bond tokens (“<monospace>=</monospace>”, “<monospace>#</monospace>”, etc.) are “<italic>add bond”</italic> actions as are also the ring and branching tokens. The main difference is that “<italic>add atom”</italic> actions are always adding the new atom to the last atom added, the bond tokens add a bond to an unknown atom, which is specified just after, and the ring and branching tokens add also bonds and enable the model to jump from one place to another. Moreover, a single bond is by default added if no bond is specified between atoms when at least one is aliphatic, and an aromatic bond is added otherwise.</p>
            <p id="Par55">One of the main issues with graph generative models is that the action space can grow dangerously large, making it very challenging to train models that generate big molecules [<xref ref-type="bibr" rid="CR46">46</xref>]. This is not the case of SMILES generative models, as they only have to choose every epoch among a limited number of options (i.e., the vocabulary). On the other hand, SMILES models traverse the graph in a very specific way, they do not allow as many options as graph models. This is specially the case with canonical SMILES: Morgan numbering greatly reduces the possible paths, as it tends to prioritize starting in sidechains rather than in the rings of the molecule [<xref ref-type="bibr" rid="CR28">28</xref>]. This makes sense when grammatically simpler SMILES strings are desired. We think that when using randomized SMILES, models become more action-based rather than grammar-based. Additionally, this may also indicate why the syntax changes added in DeepSMILES have a detrimental effect on the learning capability of SMILES generative models, as they give the model a more complex action space. For instance, the ring token altered behavior makes the ring closures extremely grammar sensitive and the new branching token behavior makes the SMILES strings unnecessarily longer without any appreciable improvement. We think that the SMILES syntax is, with all its peculiarities, an excellent hybrid between action-based and grammar-based generative models and is, to our knowledge, the most successful molecular descriptor for deep learning based molecular generation available so far.</p>
          </sec>
        </sec>
        <sec id="Sec22" sec-type="conclusions">
          <title>Conclusions</title>
          <p id="Par56">In this research we have performed an extensive benchmark of SMILES-based generative models with a wide range of hyperparameters and with different variants of the SMILES syntax. To guide the benchmark a new metric, the UC-JSD, based on the NLL of the training, validation and sampled sets was designed. Our study shows that training LSTM cell-based RNN models using randomized SMILES substantially improves the quality of the generated chemical space without having to change anything in the generative model architecture. In the case of models trained with a sample of 1 million GDB-13 molecules, the best models are able to generate almost all molecules from the database with uniform probability and generating very few molecules outside of it. Using smaller training set sizes (10,000 and 1000) further highlights the data augmentation effect of randomized SMILES and enables training models that are able to generate 62% of GDB-13 with only a sample comprising 0.001% of the database. When training models on a ChEMBL training set, randomized SMILES models have a much bigger output domain of molecules in the same range of physicochemical properties as the canonical SMILES models. Moreover, randomized SMILES models can easily generate all molecules of the canonical SMILES output domain. The randomized SMILES variant that gave the best results is the one that has restrictions, compared to the one that is able to generate all possible randomized SMILES for each molecule. Regarding different RNN hyperparameters and architectures, we wholeheartedly recommend using LSTM cells instead of GRU, due to their improved learning capability. Nevertheless, dropout and batch size have varying behavior on each training set, thus we would recommend performing a hyperparameter optimization to obtain the best values. We envision that randomized SMILES will play a significant role in generative models in the future and we encourage researchers to use them in different model architectures and problems, such as classification and prediction models.</p>
        </sec>
      </body>
      <back>
        <ack>
          <title>Acknowledgements</title>
          <p>We would like to acknowledge Michael Withnall, Rocío Mercado, Atanas Patronov, Panagiotis-Christos Kotsias and Owen Liu for their scientific insight. Also, we would like to thank Michael Withnall for his help in reviewing the manuscript. Lastly, we would like to thank the unknown reviewer #2 in [<xref ref-type="bibr" rid="CR18">18</xref>] for some of the ideas that led to this research.</p>
        </ack>
        <sec sec-type="author-contribution">
          <title>Authors’ contributions</title>
          <p>JAP planned, designed and performed the research. He also wrote the software and the manuscript. SJ provided the mathematical analysis. SJ and OP contributed in the software development and the benchmarking process. EB, CT, JLR, HC and OE cosupervised the project. All authors read and approved the final manuscript.</p>
        </sec>
        <sec>
          <title>Funding</title>
          <p>Josep Arús-Pous is supported financially by the European Union’s Horizon 2020 research and innovation program under the Marie Skłodowska-Curie grant agreement no. 676434, “Big Data in Chemistry” (“BIGCHEM,” <ext-link xlink:href="http://bigchem.eu" ext-link-type="uri">http://bigchem.eu</ext-link>).</p>
        </sec>
        <sec sec-type="data-availability">
          <title>Availability of data and materials</title>
          <p>The code used to train and benchmark all SMILES generative models is available in the (<ext-link xlink:href="https://github.com/undeadpixel/reinvent-randomized" ext-link-type="uri">https://github.com/undeadpixel/reinvent-randomized</ext-link>) repository. The GDB-13 database is available through the Reymond group website (<ext-link xlink:href="http://gdb.unibe.ch/downloads" ext-link-type="uri">http://gdb.unibe.ch/downloads</ext-link>).</p>
        </sec>
        <sec sec-type="ethics-statement">
          <sec id="FPar1" sec-type="COI-statement">
            <title>Competing interests</title>
            <p id="Par57">The authors declare that they have no competing interests.</p>
          </sec>
        </sec>
        <ref-list id="Bib1">
          <title>References</title>
          <ref-list>
            <ref id="CR1">
              <label>1.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Bohacek</surname>
                    <given-names>RS</given-names>
                  </name>
                  <name>
                    <surname>McMartin</surname>
                    <given-names>C</given-names>
                  </name>
                  <name>
                    <surname>Guida</surname>
                    <given-names>WC</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">ChemInform abstract: the art and practice of structure-based drug design: a molecular modeling perspective</article-title>
                <source>ChemInform</source>
                <year>2010</year>
                <pub-id pub-id-type="doi">10.1002/chin.199617316</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR2">
              <label>2.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Reymond</surname>
                    <given-names>JL</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">The chemical space project</article-title>
                <source>Acc Chem Res</source>
                <year>2015</year>
                <volume>48</volume>
                <fpage>722</fpage>
                <lpage>730</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2MXivFWisb4%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/ar500432k</pub-id>
                <pub-id pub-id-type="pmid">25687211</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR3">
              <label>3.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Blum</surname>
                    <given-names>LC</given-names>
                  </name>
                  <name>
                    <surname>Reymond</surname>
                    <given-names>JL</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">970 Million druglike small molecules for virtual screening in the chemical universe database GDB-13</article-title>
                <source>J Am Chem Soc</source>
                <year>2009</year>
                <volume>131</volume>
                <fpage>8732</fpage>
                <lpage>8733</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1MXmvFWru7k%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/ja902302h</pub-id>
                <pub-id pub-id-type="pmid">19505099</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR4">
              <label>4.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Ruddigkeit</surname>
                    <given-names>L</given-names>
                  </name>
                  <name>
                    <surname>Van Deursen</surname>
                    <given-names>R</given-names>
                  </name>
                  <name>
                    <surname>Blum</surname>
                    <given-names>LC</given-names>
                  </name>
                  <name>
                    <surname>Reymond</surname>
                    <given-names>JL</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Enumeration of 166 billion organic small molecules in the chemical universe database GDB-17</article-title>
                <source>J Chem Inf Model</source>
                <year>2012</year>
                <volume>52</volume>
                <fpage>2864</fpage>
                <lpage>2875</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XhsFClsL3J</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci300415d</pub-id>
                <pub-id pub-id-type="pmid">23088335</pub-id>
                <pub-id pub-id-type="pmcid">23088335</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR5">
              <label>5.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Visini</surname>
                    <given-names>R</given-names>
                  </name>
                  <name>
                    <surname>Arús-Pous</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Awale</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Reymond</surname>
                    <given-names>JL</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Virtual exploration of the ring systems chemical universe</article-title>
                <source>J Chem Inf Model</source>
                <year>2017</year>
                <volume>57</volume>
                <fpage>2707</fpage>
                <lpage>2718</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXhs1ehtbnL</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.7b00457</pub-id>
                <pub-id pub-id-type="pmid">29019686</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR6">
              <label>6.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Ruddigkeit</surname>
                    <given-names>L</given-names>
                  </name>
                  <name>
                    <surname>Blum</surname>
                    <given-names>LC</given-names>
                  </name>
                  <name>
                    <surname>Reymond</surname>
                    <given-names>JL</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Visualization and virtual screening of the chemical universe database GDB-17</article-title>
                <source>J Chem Inf Model</source>
                <year>2013</year>
                <volume>53</volume>
                <fpage>56</fpage>
                <lpage>65</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38XhvVygu77J</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci300535x</pub-id>
                <pub-id pub-id-type="pmid">23259841</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR7">
              <label>7.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Segler</surname>
                    <given-names>MHS</given-names>
                  </name>
                  <name>
                    <surname>Kogej</surname>
                    <given-names>T</given-names>
                  </name>
                  <name>
                    <surname>Tyrchan</surname>
                    <given-names>C</given-names>
                  </name>
                  <name>
                    <surname>Waller</surname>
                    <given-names>MP</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Generating focused molecule libraries for drug discovery with recurrent neural networks</article-title>
                <source>ACS Cent Sci</source>
                <year>2018</year>
                <volume>4</volume>
                <fpage>120</fpage>
                <lpage>131</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC2sXitVCjsLfP</pub-id>
                <pub-id pub-id-type="doi">10.1021/acscentsci.7b00512</pub-id>
                <pub-id pub-id-type="pmid">29392184</pub-id>
                <pub-id pub-id-type="pmcid">29392184</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR8">
              <label>8.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Olivecrona</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Blaschke</surname>
                    <given-names>T</given-names>
                  </name>
                  <name>
                    <surname>Engkvist</surname>
                    <given-names>O</given-names>
                  </name>
                  <name>
                    <surname>Chen</surname>
                    <given-names>H</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Molecular de-novo design through deep reinforcement learning</article-title>
                <source>J Cheminform</source>
                <year>2017</year>
                <pub-id pub-id-type="doi">10.1186/s13321-017-0235-x</pub-id>
                <pub-id pub-id-type="pmid">29086083</pub-id>
                <pub-id pub-id-type="pmcid">5583141</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR9">
              <label>9.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Gaulton</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Hersey</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Nowotka</surname>
                    <given-names>ML</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">The ChEMBL database in 2017</article-title>
                <source>Nucleic Acids Res</source>
                <year>2017</year>
                <volume>45</volume>
                <fpage>D945</fpage>
                <lpage>D954</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhslWhurs%3D</pub-id>
                <pub-id pub-id-type="doi">10.1093/nar/gkw1074</pub-id>
                <pub-id pub-id-type="pmid">27899562</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR10">
              <label>10.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Awale</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Sirockin</surname>
                    <given-names>F</given-names>
                  </name>
                  <name>
                    <surname>Stiefl</surname>
                    <given-names>N</given-names>
                  </name>
                  <name>
                    <surname>Reymond</surname>
                    <given-names>JL</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Drug analogs from fragment-based long short-term memory generative neural networks</article-title>
                <source>J Chem Inf Model</source>
                <year>2019</year>
                <volume>59</volume>
                <fpage>1347</fpage>
                <lpage>1356</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXlslejtrw%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00902</pub-id>
                <pub-id pub-id-type="pmid">30908913</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR11">
              <label>11.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Blaschke</surname>
                    <given-names>T</given-names>
                  </name>
                  <name>
                    <surname>Olivecrona</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Engkvist</surname>
                    <given-names>O</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Application of generative autoencoder in de novo molecular design</article-title>
                <source>Mol Inform</source>
                <year>2018</year>
                <pub-id pub-id-type="doi">10.1002/minf.201700123</pub-id>
                <pub-id pub-id-type="pmid">29235269</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR12">
              <label>12.</label>
              <mixed-citation publication-type="other">Guimaraes GL, Sanchez-Lengeling B, Outeiral C, et al (2017) Objective-reinforced generative adversarial networks (organ) for sequence generation models. <ext-link xlink:href="https://arxiv.org/abs/1705.10843" ext-link-type="uri">https://arxiv.org/abs/1705.10843</ext-link></mixed-citation>
            </ref>
            <ref id="CR13">
              <label>13.</label>
              <mixed-citation publication-type="other">Prykhodko O, Johansson S, Kotsias P-C, et al (2019) A de novo molecular generation method using latent vector based generative adversarial network. <ext-link xlink:href="10.26434/chemrxiv.8299544.v1" ext-link-type="doi">https://doi.org/10.26434/chemrxiv.8299544.v1</ext-link></mixed-citation>
            </ref>
            <ref id="CR14">
              <label>14.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Chen</surname>
                    <given-names>H</given-names>
                  </name>
                  <name>
                    <surname>Engkvist</surname>
                    <given-names>O</given-names>
                  </name>
                  <name>
                    <surname>Wang</surname>
                    <given-names>Y</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">The rise of deep learning in drug discovery</article-title>
                <source>Drug Discov Today</source>
                <year>2018</year>
                <volume>23</volume>
                <fpage>1241</fpage>
                <lpage>1250</lpage>
                <pub-id pub-id-type="doi">10.1016/j.drudis.2018.01.039</pub-id>
                <pub-id pub-id-type="pmid">29366762</pub-id>
                <pub-id pub-id-type="pmcid">29366762</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR15">
              <label>15.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Weininger</surname>
                    <given-names>D</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">SMILES, a chemical language and information system: 1: introduction to methodology and encoding rules</article-title>
                <source>J Chem Inf Comput Sci</source>
                <year>1988</year>
                <volume>28</volume>
                <fpage>31</fpage>
                <lpage>36</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaL1cXnsVeqsA%3D%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci00057a005</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR16">
              <label>16.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Li</surname>
                    <given-names>Y</given-names>
                  </name>
                  <name>
                    <surname>Zhang</surname>
                    <given-names>L</given-names>
                  </name>
                  <name>
                    <surname>Liu</surname>
                    <given-names>Z</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Multi-objective de novo drug design with conditional graph generative model</article-title>
                <source>J Cheminform</source>
                <year>2018</year>
                <volume>10</volume>
                <fpage>1</fpage>
                <lpage>24</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1MXmtFWiu7s%3D</pub-id>
                <pub-id pub-id-type="doi">10.1186/s13321-018-0287-6</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR17">
              <label>17.</label>
              <mixed-citation publication-type="other">Jin W, Barzilay R, Jaakkola T (2018) Junction tree variational autoencoder for molecular graph generation. <ext-link xlink:href="https://arxiv.org/abs/1802.04364" ext-link-type="uri">https://arxiv.org/abs/1802.04364</ext-link></mixed-citation>
            </ref>
            <ref id="CR18">
              <label>18.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Arús-Pous</surname>
                    <given-names>J</given-names>
                  </name>
                  <name>
                    <surname>Blaschke</surname>
                    <given-names>T</given-names>
                  </name>
                  <name>
                    <surname>Ulander</surname>
                    <given-names>S</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Exploring the GDB-13 chemical space using deep generative models</article-title>
                <source>J Cheminform</source>
                <year>2019</year>
                <volume>11</volume>
                <fpage>20</fpage>
                <pub-id pub-id-type="doi">10.1186/s13321-019-0341-z</pub-id>
                <pub-id pub-id-type="pmid">30868314</pub-id>
                <pub-id pub-id-type="pmcid">6419837</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR19">
              <label>19.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Bjerrum</surname>
                    <given-names>EJ</given-names>
                  </name>
                  <name>
                    <surname>Sattarov</surname>
                    <given-names>B</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Improving chemical autoencoder latent space and molecular de novo generation diversity with heteroencoders</article-title>
                <source>Biomolecules</source>
                <year>2018</year>
                <volume>8</volume>
                <fpage>1</fpage>
                <lpage>17</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXit12hurjL</pub-id>
                <pub-id pub-id-type="doi">10.3390/biom8040131</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR20">
              <label>20.</label>
              <mixed-citation publication-type="other">Bjerrum EJ (2017) SMILES enumeration as data augmentation for neural network modeling of molecules. <ext-link xlink:href="https://arxiv.org/abs/1703.07076" ext-link-type="uri">https://arxiv.org/abs/1703.07076</ext-link></mixed-citation>
            </ref>
            <ref id="CR21">
              <label>21.</label>
              <mixed-citation publication-type="other">Kimber TB, Engelke S, Tetko I V, et al (2018) Synergy effect between convolutional neural networks and the multiplicity of smiles for improvement of molecular prediction. <ext-link xlink:href="https://arxiv.org/abs/1812.04439" ext-link-type="uri">https://arxiv.org/abs/1812.04439</ext-link></mixed-citation>
            </ref>
            <ref id="CR22">
              <label>22.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>O’Boyle</surname>
                    <given-names>N</given-names>
                  </name>
                  <name>
                    <surname>Dalke</surname>
                    <given-names>A</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">DeepSMILES: an adaptation of SMILES for use in machine-learning of chemical structures</article-title>
                <source>ChemRxiv</source>
                <year>2018</year>
                <pub-id pub-id-type="doi">10.26434/chemrxiv.7097960.v1</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR23">
              <label>23.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Hochreiter</surname>
                    <given-names>S</given-names>
                  </name>
                  <name>
                    <surname>Schmidhuber</surname>
                    <given-names>J</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Long short-term memory</article-title>
                <source>Neural Comput</source>
                <year>1997</year>
                <volume>9</volume>
                <fpage>1735</fpage>
                <lpage>1780</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:STN:280:DyaK1c%2FhvVahsQ%3D%3D</pub-id>
                <pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id>
                <pub-id pub-id-type="pmid">9377276</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR24">
              <label>24.</label>
              <mixed-citation publication-type="other">Cho K, van Merrienboer B, Gulcehre C, et al (2014) Learning phrase representations using RNN encoder-decoder for statistical machine translation. <ext-link xlink:href="10.3115/v1/D14-1179" ext-link-type="doi">https://doi.org/10.3115/v1/D14-1179</ext-link></mixed-citation>
            </ref>
            <ref id="CR25">
              <label>25.</label>
              <mixed-citation publication-type="other">Weiss G, Goldberg Y, Yahav E (2018) On the practical computational power of finite precision RNNs for language recognition. <ext-link xlink:href="https://arxiv.org/abs/1805.04908" ext-link-type="uri">https://arxiv.org/abs/1805.04908</ext-link></mixed-citation>
            </ref>
            <ref id="CR26">
              <label>26.</label>
              <mixed-citation publication-type="other">Hinton GE, Srivastava N, Krizhevsky A, et al (2012) Improving neural networks by preventing co-adaptation of feature detectors. <ext-link xlink:href="https://arxiv.org/abs/1207.0580v1" ext-link-type="uri">https://arxiv.org/abs/1207.0580v1</ext-link></mixed-citation>
            </ref>
            <ref id="CR27">
              <label>27.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Morgan</surname>
                    <given-names>HL</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">The generation of a unique machine description for chemical structures—a technique developed at chemical abstracts service</article-title>
                <source>J Chem Doc</source>
                <year>1965</year>
                <volume>5</volume>
                <fpage>107</fpage>
                <lpage>113</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaF2MXkt1Omtr0%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/c160017a018</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR28">
              <label>28.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Weininger</surname>
                    <given-names>D</given-names>
                  </name>
                  <name>
                    <surname>Weininger</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Weininger</surname>
                    <given-names>JL</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">SMILES. 2. Algorithm for generation of unique SMILES notation</article-title>
                <source>J Chem Inf Comput Sci</source>
                <year>1989</year>
                <volume>29</volume>
                <fpage>97</fpage>
                <lpage>101</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DyaL1MXitFWlt7s%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci00062a008</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR29">
              <label>29.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Williams</surname>
                    <given-names>RJ</given-names>
                  </name>
                  <name>
                    <surname>Zipser</surname>
                    <given-names>D</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">A learning algorithm for continually running fully recurrent neural networks</article-title>
                <source>Neural Comput</source>
                <year>2008</year>
                <volume>1</volume>
                <fpage>270</fpage>
                <lpage>280</lpage>
                <pub-id pub-id-type="doi">10.1162/neco.1989.1.2.270</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR30">
              <label>30.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Lin</surname>
                    <given-names>J</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Divergence measures based on the Shannon entropy</article-title>
                <source>IEEE Trans Inf Theory</source>
                <year>1991</year>
                <volume>37</volume>
                <fpage>47</fpage>
                <lpage>51</lpage>
                <pub-id pub-id-type="doi">10.1109/18.61115</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR31">
              <label>31.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Paszke</surname>
                    <given-names>A</given-names>
                  </name>
                  <name>
                    <surname>Chanan</surname>
                    <given-names>G</given-names>
                  </name>
                  <name>
                    <surname>Lin</surname>
                    <given-names>Z</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Automatic differentiation in PyTorch</article-title>
                <source>Adv Neural Inf Process Syst</source>
                <year>2017</year>
                <volume>30</volume>
                <fpage>1</fpage>
                <lpage>4</lpage>
              </mixed-citation>
            </ref>
            <ref id="CR32">
              <label>32.</label>
              <mixed-citation publication-type="other">Landrum G (2006) RDKit: Open-source cheminformatics. <ext-link xlink:href="http://www.rdkit.org" ext-link-type="uri">http://www.rdkit.org</ext-link></mixed-citation>
            </ref>
            <ref id="CR33">
              <label>33.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Zaharia</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Franklin</surname>
                    <given-names>MJ</given-names>
                  </name>
                  <name>
                    <surname>Ghodsi</surname>
                    <given-names>A</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Apache Spark</article-title>
                <source>Commun ACM</source>
                <year>2016</year>
                <volume>59</volume>
                <fpage>56</fpage>
                <lpage>65</lpage>
                <pub-id pub-id-type="doi">10.1145/2934664</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR34">
              <label>34.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Hunter</surname>
                    <given-names>JD</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Matplotlib: a 2D graphics environment</article-title>
                <source>Comput Sci Eng</source>
                <year>2007</year>
                <volume>9</volume>
                <fpage>99</fpage>
                <lpage>104</lpage>
                <pub-id pub-id-type="doi">10.1109/MCSE.2007.55</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR35">
              <label>35.</label>
              <mixed-citation publication-type="other">Waskom M, Botvinnik O, O’Kane D, et al (2018) seaborn: v0.9.0 (July 2018). <ext-link xlink:href="10.5281/ZENODO.1313201" ext-link-type="doi">https://doi.org/10.5281/ZENODO.1313201</ext-link></mixed-citation>
            </ref>
            <ref id="CR36">
              <label>36.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Preuer</surname>
                    <given-names>K</given-names>
                  </name>
                  <name>
                    <surname>Renz</surname>
                    <given-names>P</given-names>
                  </name>
                  <name>
                    <surname>Unterthiner</surname>
                    <given-names>T</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Fréchet ChemNet distance: a metric for generative models for molecules in drug discovery</article-title>
                <source>J Chem Inf Model</source>
                <year>2018</year>
                <volume>58</volume>
                <fpage>1736</fpage>
                <lpage>1741</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC1cXhsFejsLrL</pub-id>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00234</pub-id>
                <pub-id pub-id-type="pmid">30118593</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR37">
              <label>37.</label>
              <mixed-citation publication-type="other">Polykovskiy D, Zhebrak A, Sanchez-Lengeling B, et al (2018) Molecular sets (MOSES): a benchmarking platform for molecular generation models. <ext-link xlink:href="https://arxiv.org/abs/1811.12823" ext-link-type="uri">https://arxiv.org/abs/1811.12823</ext-link></mixed-citation>
            </ref>
            <ref id="CR38">
              <label>38.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Ertl</surname>
                    <given-names>P</given-names>
                  </name>
                  <name>
                    <surname>Schuffenhauer</surname>
                    <given-names>A</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Estimation of synthetic accessibility score of drug-like molecules based on molecular complexity and fragment contributions</article-title>
                <source>J Cheminform</source>
                <year>2009</year>
                <volume>1</volume>
                <fpage>8</fpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD1MXhsVGhs73J</pub-id>
                <pub-id pub-id-type="doi">10.1186/1758-2946-1-8</pub-id>
                <pub-id pub-id-type="pmid">20298526</pub-id>
                <pub-id pub-id-type="pmcid">3225829</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR39">
              <label>39.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Bickerton</surname>
                    <given-names>GR</given-names>
                  </name>
                  <name>
                    <surname>Paolini</surname>
                    <given-names>GV</given-names>
                  </name>
                  <name>
                    <surname>Besnard</surname>
                    <given-names>J</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Quantifying the chemical beauty of drugs</article-title>
                <source>Nat Chem</source>
                <year>2012</year>
                <volume>4</volume>
                <fpage>90</fpage>
                <lpage>98</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BC38Xht1aktLk%3D</pub-id>
                <pub-id pub-id-type="doi">10.1038/nchem.1243</pub-id>
                <pub-id pub-id-type="pmid">22270643</pub-id>
                <pub-id pub-id-type="pmcid">3524573</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR40">
              <label>40.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Ertl</surname>
                    <given-names>P</given-names>
                  </name>
                  <name>
                    <surname>Roggo</surname>
                    <given-names>S</given-names>
                  </name>
                  <name>
                    <surname>Schuffenhauer</surname>
                    <given-names>A</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Natural product-likeness score and its application for prioritization of compound libraries</article-title>
                <source>J Chem Inf Model</source>
                <year>2008</year>
                <volume>48</volume>
                <fpage>68</fpage>
                <lpage>74</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD2sXhtlCqtLfP</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci700286x</pub-id>
                <pub-id pub-id-type="pmid">18034468</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR41">
              <label>41.</label>
              <mixed-citation publication-type="other">Metz L, Poole B, Pfau D, Sohl-Dickstein J (2016) Unrolled generative adversarial networks. <ext-link xlink:href="https://arxiv.org/abs/1611.02163" ext-link-type="uri">https://arxiv.org/abs/1611.02163</ext-link></mixed-citation>
            </ref>
            <ref id="CR42">
              <label>42.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Brown</surname>
                    <given-names>N</given-names>
                  </name>
                  <name>
                    <surname>Fiscato</surname>
                    <given-names>M</given-names>
                  </name>
                  <name>
                    <surname>Segler</surname>
                    <given-names>MHS</given-names>
                  </name>
                  <name>
                    <surname>Vaucher</surname>
                    <given-names>AC</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">GuacaMol: benchmarking models for de novo molecular design</article-title>
                <source>J Chem Inf Model</source>
                <year>2019</year>
                <pub-id pub-id-type="doi">10.1021/acs.jcim.8b00839</pub-id>
                <pub-id pub-id-type="pmid">30887799</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR43">
              <label>43.</label>
              <mixed-citation publication-type="other">Johansson S, Ptykhodko O, Arús-Pous J, et al (2019) Comparison between SMILES-based differential neural computer and recurrent neural network architectures for de novo molecule design. <ext-link xlink:href="10.26434/chemrxiv.9758600.v1" ext-link-type="doi">https://doi.org/10.26434/chemrxiv.9758600.v1</ext-link></mixed-citation>
            </ref>
            <ref id="CR44">
              <label>44.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Ertl</surname>
                    <given-names>P</given-names>
                  </name>
                </person-group>
                <article-title xml:lang="en">Cheminformatics analysis of organic substituents: identification of the most common substituents, calculation of substituent properties, and automatic identification of drug-like bioisosteric groups</article-title>
                <source>J Chem Inf Comput Sci</source>
                <year>2003</year>
                <volume>43</volume>
                <fpage>374</fpage>
                <lpage>380</lpage>
                <pub-id pub-id-type="other" assigning-authority="ChemPort ( Chemical Abstract Service )">1:CAS:528:DC%2BD38Xpt1Cmtbs%3D</pub-id>
                <pub-id pub-id-type="doi">10.1021/ci0255782</pub-id>
                <pub-id pub-id-type="pmid">12653499</pub-id>
              </mixed-citation>
            </ref>
            <ref id="CR45">
              <label>45.</label>
              <mixed-citation publication-type="other">Jozefowicz R, Vinyals O, Schuster M, et al (2016) Exploring the limits of language modeling. <ext-link xlink:href="https://arxiv.org/abs/1602.02410" ext-link-type="uri">https://arxiv.org/abs/1602.02410</ext-link></mixed-citation>
            </ref>
            <ref id="CR46">
              <label>46.</label>
              <mixed-citation publication-type="journal">
                <person-group person-group-type="author">
                  <name>
                    <surname>Li</surname>
                    <given-names>Y</given-names>
                  </name>
                  <name>
                    <surname>Vinyals</surname>
                    <given-names>O</given-names>
                  </name>
                  <name>
                    <surname>Dyer</surname>
                    <given-names>C</given-names>
                  </name>
                  <etal />
                </person-group>
                <article-title xml:lang="en">Learning deep generative models of graphs</article-title>
                <source>Iclr</source>
                <year>2018</year>
                <pub-id pub-id-type="doi">10.1146/annurev-statistics-010814-020120</pub-id>
              </mixed-citation>
            </ref>
          </ref-list>
        </ref-list>
        <app-group>
          <app id="App1">
            <sec id="Sec23">
              <title>Supplementary information</title>
              <p>
                <supplementary-material content-type="local-data" id="MOESM1" xlink:title="Supplementary information">
                  <media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/13321_2019_393_MOESM1_ESM.pdf" position="anchor">
                    <caption xml:lang="en">
                      <p><bold>Additional file 1.</bold> Supplementary methods.</p>
                    </caption>
                  </media>
                </supplementary-material>
                <supplementary-material content-type="local-data" id="MOESM2" xlink:title="Supplementary information">
                  <media mimetype="application" mime-subtype="pdf" xlink:href="MediaObjects/13321_2019_393_MOESM2_ESM.pdf" position="anchor">
                    <caption xml:lang="en">
                      <p><bold>Additional file 2.</bold> Supplementary figures.</p>
                    </caption>
                  </media>
                </supplementary-material>
                <supplementary-material content-type="local-data" id="MOESM3" xlink:title="Supplementary information">
                  <media mimetype="application" mime-subtype="vnd.ms-excel" xlink:href="MediaObjects/13321_2019_393_MOESM3_ESM.xlsx" position="anchor">
                    <caption xml:lang="en">
                      <p><bold>Additional file 3.</bold> Supplementary tables.</p>
                    </caption>
                  </media>
                </supplementary-material>
              </p>
            </sec>
          </app>
        </app-group>
        <glossary>
          <title>Abbreviations</title>
          <def-list>
            <def-item>
              <term>ADAM</term>
              <def>
                <p id="Par2">Adaptive Moment Estimation</p>
              </def>
            </def-item>
            <def-item>
              <term>CADD</term>
              <def>
                <p id="Par3">Computer-Assisted Drug Design</p>
              </def>
            </def-item>
            <def-item>
              <term>FCD</term>
              <def>
                <p id="Par4">Fréchet ChemNet Distance</p>
              </def>
            </def-item>
            <def-item>
              <term>GAN</term>
              <def>
                <p id="Par5">Generative Adversarial Network</p>
              </def>
            </def-item>
            <def-item>
              <term>GDB</term>
              <def>
                <p id="Par6">Generated Database</p>
              </def>
            </def-item>
            <def-item>
              <term>GRU</term>
              <def>
                <p id="Par7">Gated Recurrent Unit</p>
              </def>
            </def-item>
            <def-item>
              <term>HSV</term>
              <def>
                <p id="Par8">Hue–Saturation–Value</p>
              </def>
            </def-item>
            <def-item>
              <term>JSD</term>
              <def>
                <p id="Par9">Jensen–Shannon Divergence</p>
              </def>
            </def-item>
            <def-item>
              <term>LSTM</term>
              <def>
                <p id="Par10">long short-term memory</p>
              </def>
            </def-item>
            <def-item>
              <term>NLL</term>
              <def>
                <p id="Par11">negative log-likelihood</p>
              </def>
            </def-item>
            <def-item>
              <term>PCA</term>
              <def>
                <p id="Par12">principal component analysis</p>
              </def>
            </def-item>
            <def-item>
              <term>RNN</term>
              <def>
                <p id="Par13">Recurrent Neural Network</p>
              </def>
            </def-item>
            <def-item>
              <term>SMILES</term>
              <def>
                <p id="Par14">Simple Molecular Input Line Entry System</p>
              </def>
            </def-item>
            <def-item>
              <term>UCC</term>
              <def>
                <p id="Par15">Uniformity–Completeness–Closedness Ratio</p>
              </def>
            </def-item>
            <def-item>
              <term>UC-JSD</term>
              <def>
                <p id="Par16">Uniformity–Completeness JSD</p>
              </def>
            </def-item>
            <def-item>
              <term>VAE</term>
              <def>
                <p id="Par17">Variational Autoencoder</p>
              </def>
            </def-item>
          </def-list>
        </glossary>
        <notes notes-type="ESMHint">
          <title>Supplementary information</title>
          <p><bold>Supplementary information</bold> accompanies this paper at <ext-link xlink:href="10.1186/s13321-019-0393-0" ext-link-type="doi">https://doi.org/10.1186/s13321-019-0393-0</ext-link>.</p>
        </notes>
        <notes notes-type="Misc">
          <title>Publisher's Note</title>
          <p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p>
        </notes>
      </back>
    </article>
